---
title: "Newport_SSINP"
output: html_document
date: '2022-08-03'
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#libraries
```{r loadLibraries, echo=FALSE, message=FALSE}
library(tidyverse)
library(scales)
library(readr)
library(dplyr)
```

#import files
```{r fileLocations, echo=FALSE}
# Directory path on Lauren's machine
setwd("~/Documents/Oyster/Rcode")

# Comment out one of the two localpath lines below depending on what computer you're on
#localpath = '../../data/'  # Luke's path
localpath = '../data/'  # Lauren's path

NewportDir = paste0(localpath,'Newport_SSINP/')
WestcliffEelgrassDir = paste0(localpath,'Newport_SSINP/Westcliff/Eelgrass/')

# Collated calibration data from May 2019, used for Nov 2019 + 2020 oyster deployments
gapecalibfile = paste0(localpath,'Newport_SSINP/calibration/201905_calibrations.csv')
combogapefile= paste0(localpath,'Newport_SSINP/combinedgape/')

#meta file with serial numbers
metafilePath = paste0(localpath,'Newport_SSINP/metadata/')
```

#Functions: gapBounds, gapBounds2, hallFilter, DistFromCounts, percentileRange, calcPercentGapeNLME, calcPercentGapeNLMEsubset, CRegressEstimate
```{r gapeFunctions-, echo=showcode}

# A set of functions originally taken from MussleTracker_data_proc4.R to 
# handle the gape data

###############################################################################	
# A function to determine where there are gaps of missing data, so that we can 
# apply a filter to the good chunks of data. Most filters fail if there are NAs
# present, and our remaining gaps in the data are big enough that they probably
# shouldn't be smoothed-over anyhow. 
#' A function to find the start and end indices of numeric data in a vector. 
#' Useful for dealing with timeseries that have gaps, and operations that need
#' to operate the separate chunks of contiguous numeric values
#' @param value A vector of numeric values, with NAs 
#' @return A data frame with 2 columns, Start and End, which contain the row
#' indices for the first good and last good values in each contiguous run of
#' numeric values. 	
gapBounds <- function (values){
	# This function returns a data frame of 2 columns, start + end, that give
	# the row indices of the start and end of each run of good data in the 
	# input vector. It should handle data streams that start with NAs and
	# should handle streams that end with NAs. 
	
	# Use run length encoding function to find NA gaps
	gaps = rle(is.na(values))
	# If the first entry in gaps is TRUE, this indicates that the data started
	# with NA values.
	if (gaps$values[1] == TRUE) {
		startNA = TRUE	# Data started with NAs
	} else {	
		startNA = FALSE	 # Data started with real values
	}
	# If the last entry in gaps is TRUE, the data end on NAs
	if (gaps$values[length(gaps$values)] == TRUE){
		endNA = TRUE	# Data ends with NAs
	} else {
		endNA = FALSE	# Data ends with real values
	}
	
	# The number of gaps with value == TRUE is the number of good runs of data
	# and gaps with value == FALSE are the starts of runs of NAs. This will
	# get the number of FALSE (negated to TRUE) values in gaps. A dataset
	# ending in good data will have a last entry in gaps of FALSE. 
	numgaps = sum(!gaps$values)
	
	# Create the output data frame
	results = data.frame(Start = integer(numgaps), End = integer(numgaps))
	
	if (!startNA) {
		# If startNA == FALSE (started on good data, the first entry should 
		# always be 1 (1st row) 
		# If there are no gaps, the
		# contents of gaps$lengths will just be the length of the values vector
		results$Start[1] = 1
		results$End[1] = gaps$lengths[1]
	} else if (startNA){
		# However, if the dataset starts with NAs, the first entry in gaps will
		# be the index of the first good data, while the 2nd entry will be the 
		# start of the next stretch of NAs
		results$Start[1] = gaps$lengths[1]+1
		results$End[1] = sum(gaps$lengths[1:2])
	}
	
	# If there is more than 1 entry in gaps$lengths, process the other gaps
	j = 2; # counter
	if (numgaps > 1){
		if (!startNA & endNA){
			# If the dataset ends on NAs (TRUE last), truncate gapind by 1
			gapind = seq(2,length(gaps$lengths)-1, by = 2)
		} else if (!startNA & !endNA) {
			# If the dataset ends on good values (FALSE last)
			gapind = seq(2,length(gaps$lengths), by = 2)
		} else if (startNA & endNA) {
			# If dataset starts on NAs (TRUE 1st) and ends on NAs (TRUE last)
			gapind = seq(3, length(gaps$lengths)-1, by = 2)
		} else if (startNA & !endNA) {
			# If dataset starts on NAs (TRUE 1st) and ends on good data 
			# (FALSE last)
			gapind = seq(3, length(gaps$lengths), by = 2)
		}	
		# Step through the rest of the gaps object to find the start and end
		# points of each stretch of good data. 
		for (i in gapind){
			nextstart = sum(gaps$lengths[1:i]) + 1
			nextend = sum(gaps$lengths[1:(i+1)])
			results$Start[j] = nextstart
			results$End[j] = nextend
			j = j + 1
		}
	} # end of if (numgaps > 1)
	results	# return the results dataframe, 2 columns Start and End
}

################################################################################
### gapBounds2 function
#' Function that sets bounds for upper and lower limit of hall counts to calculate gape opening % while dealing with growth rates. Change chunk length to certain number of days (hall count rows) (arbitrary 5 days). Use gapbounds function to set new ranges. 
#' @param mygaps A datafrom from gapbounds function
#' @param maxrow Max length of rows (time) to subset out 

gapBounds2=function(mygaps, maxrow){
  mydiff=mygaps$End[1]-mygaps$Start[1]
  if (mydiff>maxrow){
    quotient=mydiff/maxrow #quotient=number of subsets (chunks) to make based on max row
    nloops = floor(quotient)
    
    for (i in 1:nloops){
      if (i == 1 & nloops>=2){
        mygaps2 = data.frame(Start = mygaps$Start[1], End = mygaps$Start[1]+maxrow)
      } else if (i==1 & nloops<2){
        mygaps2 = data.frame(Start = mygaps$Start[1], End = mygaps$End[1])
      } else if (i > 1 & i < nloops){
        temp = data.frame(Start = mygaps2$End[i-1]+1, End = mygaps2$End[i-1]+maxrow )
        mygaps2 = rbind(mygaps2, temp)
      } else if (i > 1 & i == nloops){
        temp = data.frame(Start = mygaps2$End[i-1]+1, End = mygaps$End[1])
        mygaps2 = rbind(mygaps2, temp)
      }
    }
  } else if (mydiff <= maxrow){
    # Handle the case where the 1st entry in mygaps is shorter than maxrow, just copy the values
    # over to a dataframe we're calling mygaps2
    mygaps2 = data.frame(Start = mygaps$Start[1], End = mygaps$End[1])
  }
  # Handle cases where there is more than a single row in mygaps
  if (nrow(mygaps) > 1) {
    for (j in 2:nrow(mygaps)){
      mydiff=mygaps$End[j]-mygaps$Start[j]
      if (mydiff>maxrow){
        quotient=mydiff/maxrow #quotient=number of subsets (chunks) to make based on max row
        nloops = floor(quotient)
        
        for (i in nloops){
          if (i == 1){
            temp = data.frame(Start = mygaps$Start[j], End = maxrow)
            mygaps2 = rbind(mygaps2, temp)
          } else if (i > 1 & i < nloops){
            temp = data.frame(Start = mygaps2$End[nrow(mygaps2)]+1, End = mygaps2$End[nrow(mygaps2)]+maxrow )
            mygaps2 = rbind(mygaps2, temp)
          } else if (i == nloops){
            temp = data.frame(Start = mygaps2$End[nrow(mygaps2)]+1, End = mygaps$End[j])
            mygaps2 = rbind(mygaps2, temp)
          }
        }
      } else {
        # handle the case where the next entry in mygaps is shorter than maxrow, just copy the start/end from 
        # mygaps into mygaps2
        mygaps2 = rbind(mygaps2, mygaps[j,])
      }
    }
  }
  mygaps2 # return this data frame at the end of the function
  
}




################################################################################
### hallFilter function
#' A function to apply a Butterworth 1st order low-pass filter to a vector
#'  of data. If there are NA values in the original data, they will be 
#'  preserved in the output vector. The filter is set to filter at 1/10 the
#' original sampling rate (which is 5 sec interval (0.2Hz), producing an approximate
#' 50-second window of smoothing. Raw input data are initially centered on 0
#' to avoid artifacts at the start of the filter, and the output is re-centered
#' at the original starting value after the filtering is applied. 
#' 
#' @param hallData A vector of hall effect data.
#' @return A vector of filtered hall effect sensor data.
#'  
hallFilter = function(hallData){	
	# Define a butterworth low-pass filter, 1st order, to filter at 1/10 the 
	# sampling rate (which was 5 secs (0.2Hz))
	#myfilter = signal:::butter(1,0.1,type='low', plane = 'z')
	# Find the start and end of any gaps in the data set using the gapBounds
	# function defined earlier
	mygaps = gapBounds(hallData)
	# Apply the filter to the good runs of data
	for (i in 1:nrow(mygaps)){
		# Extract the run of good data
		dats = hallData[mygaps$Start[i]:mygaps$End[i]]
		# Find the offset of the data from zero
		offset = dats[1]
		# Subtract the offset off of all of the data in this run
		# (The butterworth filter returns large transient values at the start
		# of the run if the value is much different from zero)
		dats = dats - offset
		# Call the filter routine to apply the filter
		yfiltered = signal:::filter(myfilter,dats)
		# Add the offset back on so the data are back on their original scale
		yfiltered = yfiltered + offset
		# Write the filtered data back into the data vector
		hallData[mygaps$Start[i]:mygaps$End[i]] = yfiltered
	}
	# Round the filtered data back to the nearest whole number, since these
	# represent ADC count data.
	hallData = round(hallData)
	hallData	# return vector of filtered data
}

################################################################################
# Define a non-linear function that allows you to plug in a Hall effect sensor
# count value and generate an approximate distance in mm. Supply the 
# coefficients a, b, c, based on a nls curve fit for counts as a function of
# distance. 
# In the above equation, 'a' is the effective asymptotic value
# 'b' is the range of values between the minimum observed count and the 
# asymptotic count (so a min count of 400 and asymptote of 500 gives b = 100)
# 'c' is effectively a curvature parameter. Larger values of 'c' yield a 
# smaller change in Distance as Counts increases (when back-calculating 
# Distance), particularly for low Counts values. 
#' A function to back-calculate an asymptotic function to generate a distance
#' value based on a hall effect sensor count value input.
#' 
#' @param Counts A vector of hall effect sensor values, usually between 0 and 
#' 512 (from a 10-bit analog-to-digital converter). 
#' @param a A parameter derived from an asymptotic curve fit, representing the
#' asymptotic count value
#' @param b A parameter derived from an asymptotic curve fit, representing the 
#' range from the minimum count up to the asymptotic value. 
#' @param c A parameter derived from an asymptotic curve fit, representing the
#' curvature of the fit. 
#' @returns A vector of distance values of the same length as Counts. Any input 
#' values equal or larger than the asymptotic parameter a will return Inf.

DistFromCounts <- function(Counts, a,b,c){
	dist = (log((Counts - a)/(-1*b))) / (-1*c)
	dist
}


################################################################################
### percentileRange function
# Define a function to calculate the average reading for the upper and lower
# x percentile of the hall effect data. 
#' Calculate the average of the specified lower percentile and upper percentile
#' and return the values that correspond to those means (rounded to integers).
#' 
#' @param hallData A vector of analog-to-digital convertor counts from a 
#' Hall effect sensor. The values are assumed to start low (<512) when the 
#' magnetic signal is strong (close to sensor) and increase towards 512 when
#' the magnetic signal is weak (far from sensor).
#' @param percentileLim A set of 2 numeric values between 0 and 1, usually close
#'  to 0.01 and 0.99
#' used to define the lower and upper percentile limits. A value of 
#' 0.01 would cause the 1st percentile sensor values to be used as the
#' lower limits, and similar for the upper limit. 
#' @return A two element vector for the lower and upper values in hallData that
#' represent the lower and upper percentiles

percentileRange <- function (Hallvec, percentileLim = c(0.01,0.99)){
	# Remove any NA's
	temp = Hallvec[!is.na(Hallvec)]
	# Reorder the values smallest to largest
	temp = temp[order(temp)]
	# Get the index of the entry closest to percentileLim
	indx = round(percentileLim[1] * length(temp)) 
	# Calculate the mean value for the lower % of closed valve values and
	# round up to the next integer value
	closedVal = ceiling(mean(temp[1:indx]))
	
	# Now do the same for the other end of the range of hall effect values
	# These would normally represent "fully open" readings near 512 if the 
	# magnet and sensor are situated so that the magnet drives the signal
	# below 512 when it approaches the sensor as the shell valves close. 
	indx = round(percentileLim[2] * length(temp))
	# Calculate the mean value for the upper % of open valve values and round
	# down to the next integer value
	openVal = floor(mean(temp[indx:length(temp)]))
	result = c(closedVal, openVal)
	result # return the two values, always smallest then largest
}

################################################################################
### calcPercentGapeNLME function
# Use this function to calculate the percentage gape (0-100%)
# This calculates the baseline fully-closed and fully-open values based on the upper and lower 1% of hall effect sensor data for the given range of rows in the input dataset. Supply a vector of Hall effect sensor data. This function will find any gaps in the vector (NA's) and calculate a new baseline for each section of good data. 
#' Convert hall effect count data (integer values from analog-digital converter) into percent gape values between 0 and 100\%. ' 
#' @param hallData A vector of analog-to-digital convertor counts from a  Hall effect sensor. The values are assumed to start low (<512) when the magnetic signal is strong (close to sensor) and increase towards 512 when the magnetic signal is weak (far from sensor).
#' @param cRegress A linear model object (lm) for the relationship of 'c' versus the log-transform of 'b'. 
#' @param percentileLim A numeric value between 0 and 1, usually close to 0.01, used to define the lower and upper percentile limits (the upper limit is mirrored from the lower limit). A value of 0.01 would cause the 1st and 99th percentile sensor values to be used as the lower and upper limits.
#' @return A vector of the same length as hallData containing percentage values 0 to 100, representing the shell valve gape opening. A value of 0 is a fully closed shell (strong magnetic signal) and 100 is a fully open shell (weak magnetic signal).

calcPercentGapeNLME <- function (hallData, cRegress, percentileLim = 0.01){
	require(nlme)
	# Get the row indices for the major gaps that exist now
	mygaps = gapBounds(hallData)
	# Create an empty output vector of the same length as the input data. 
	outputData = vector(mode = 'numeric', length = length(hallData))
	outputData[] = NA # Convert all the values to NA to begin with
	
	# Now for each section of the input data, calculate the percent gape. This involves using the entries in mygaps to subset the input data and calculate individual percentile values for each contiguous section of data. If there is a significant gap in the data (usually > 30 sec), the percentages will be re-calculated for each contiguous section of data. This should accommodate any sensor/magnet re-glueing issues. 
	for (i in 1:nrow(mygaps)){
		st = mygaps$Start[i]
		end = mygaps$End[i]
		# First calculate the upper and lower 1% values of the Hall readings. These will be count values representing the fully closed (lowest) and fully open (highest, closest to 512) values.
		myrange = percentileRange(hallData[st:end], 
				percentileLim = percentileLim)
		
		# Now truncate count values that are outside myrange to the limits of myrange.
		hallData[which(hallData[st:end] < myrange[1])] = myrange[1]
		hallData[which(hallData[st:end] > myrange[2])] = myrange[2]
		
		# The count values are now constrained within a range that should encompass fully closed (0mm) to fully open (~5mm gape). Now calculate the approximate gape opening based on the count values and a curve fit. 
		
		# Define the three asymptotic curve parameters a,b,c using the data for the current chunk of data.
		aVal = max(hallData[st:end],na.rm=TRUE)+1 # add 1 count to avoid Inf output
		bVal = max(hallData[st:end],na.rm=TRUE) - 
				min(hallData[st:end],na.rm=TRUE)
		# Calculating c requires the linear model object supplied as cRegress, which will use the log-transformed 'b' value to estimate the 'c' parameter
		cVal = predict(cRegress, newdata = list(b = bVal))
		
		# Use DistFromCounts function to estimate valve gape distance using the input hall effect count data at each time point and the parameters a,b,c
		temp2 = DistFromCounts( hallData[st:end], 
				a = aVal,
				b = bVal,
				c = cVal)
		
		# Replace any fully-open Inf values with the largest good max distance
		temp2[is.infinite(temp2)] = max(temp2[is.finite(temp2)])
		# Replace any negative distance values with the 0 distance value
		temp2[which(temp2 < 0)] = 0
		# Convert to percentage by dividing all distances by the maximum value
		outputData[st:end] = (temp2 / max(temp2,na.rm=TRUE)) * 100
		
		# Round the values off to a reasonable precision
		outputData[st:end] = round(outputData[st:end],1)
		
	}	
	outputData	# return the output data vector
}	# end of calcPercentGapeNLME function




################################################################################
### calcPercentGapeNLMEsubset function
# Use this function to calculate the percentage gape (0-100%) of only a subset of data (days) at a time to account for growth of oyster shell. 
# This calculates the baseline fully-closed and fully-open values based on the upper and lower 1% of hall effect sensor data for the given range of rows in the input dataset. Supply a vector of Hall effect sensor data. This function will find any gaps in the vector (NA's) and calculate a new baseline for each section of good data. 
#' Convert hall effect count data (integer values from analog-digital converter) into percent gape values between 0 and 100\%. ' 
#' @param hallData A vector of analog-to-digital converter counts from a  Hall effect sensor. The values are assumed to start low (<512) when the magnetic signal is strong (close to sensor) and increase towards 512 when the magnetic signal is weak (far from sensor).
#' @param cRegress A linear model object (lm) for the relationship of 'c' versus the log-transform of 'b'. 
#' @param percentileLim A numeric value between 0 and 1, usually close to 0.01, used to define the lower and upper percentile limits (the upper limit is mirrored from the lower limit). A value of 0.01 would cause the 1st and 99th percentile sensor values to be used as the lower and upper limits.
#' @return A vector of the same length as hallData containing percentage values 0 to 100, representing the shell valve gape opening. A value of 0 is a fully closed shell (strong magnetic signal) and 100 is a fully open shell (weak magnetic signal).
#' @param maxrow A number of rows used to define the upper and lower limits of hall sensor calculations. Chose arbitrary 7 day chunk. may want to be 5 days bc if have little bit of leftover chunk then can  add on to previous so then last chunk is 7 days. 

calcPercentGapeNLMEsubset <- function (hallData, cRegress, percentileLim = 0.01, maxrow){
	require(nlme)
	# Get the row indices for the major gaps that exist now
	mygaps = gapBounds(hallData) #figure out if length of mygaps exceeds max length value (7 days). If so then split it up 
	
	# Use gapBounds2 to subdivide the continuous data into chunks of time that are defined by maxrow
	mygaps = gapBounds2(mygaps, maxrow)
	
	# Create an empty output vector of the same length as the input data. 
	outputData = vector(mode = 'numeric', length = length(hallData))
	outputData[] = NA # Convert all the values to NA to begin with
	
	# Now for each section of the input data, calculate the percent gape. This involves using the entries in mygaps to subset the input data and calculate individual percentile values for each contiguous section of data. If there is a significant gap in the data (usually > 30 sec), the percentages will be re-calculated for each contiguous section of data. This should accommodate any sensor/magnet re-glueing issues. 
	for (i in 1:nrow(mygaps)){
		st = mygaps$Start[i]
		end = mygaps$End[i]
		# First calculate the upper and lower 1% values of the Hall readings. These will be count values representing the fully closed (lowest) and fully open (highest, closest to 512) values.
		myrange = percentileRange(hallData[st:end], 
				percentileLim = percentileLim)
		
		# Now truncate count values that are outside myrange to the limits of myrange.
		rowIndices = which(hallData[st:end] < myrange[1])
		hallData[(st+rowIndices)] = myrange[1]
		rowIndices = which(hallData[st:end] > myrange[2])
		hallData[(st+rowIndices)] = myrange[2]
		
		# The count values are now constrained within a range that should encompass fully closed (0mm) to fully open (~5mm gape). Now calculate the approximate gape opening based on the count values and a curve fit. 
		
		# Define the three asymptotic curve parameters a,b,c using the data for the current chunk of data.
		aVal = max(hallData[st:end],na.rm=TRUE)+1 # add 1 count to avoid Inf output
		bVal = max(hallData[st:end],na.rm=TRUE) - 
				min(hallData[st:end],na.rm=TRUE)
		# Calculating c requires the linear model object supplied as cRegress, which will use the log-transformed 'b' value to estimate the 'c' parameter
		cVal = predict(cRegress, newdata = list(b = bVal))
		
		# Use DistFromCounts function to estimate valve gape distance using the input hall effect count data at each time point and the parameters a,b,c
		temp2 = DistFromCounts( hallData[st:end], 
				a = aVal,
				b = bVal,
				c = cVal)
		
		# Replace any fully-open Inf values with the largest good max distance
		temp2[is.infinite(temp2)] = max(temp2[is.finite(temp2)])
		# Replace any negative distance values with the 0 distance value
		temp2[which(temp2 < 0)] = 0
		# Convert to percentage by dividing all distances by the maximum value
		outputData[st:end] = (temp2 / max(temp2,na.rm=TRUE)) * 100
		
		# Round the values off to a reasonable precision
		outputData[st:end] = round(outputData[st:end],1)
		
	}	
	outputData	# return the output data vector
}	# end of calcPercentGapeNLME function

#cRegressEstimate helps calibrate sensors using CalibFileName.csv file 
################################################################################
################################################################################
# Function cRegressEstimate
# Currently this represents the best data showing how the relationship between
# magnet signal and distance is repeatable and consistent among different
# magnets (all from the same batch/part number). As such, a non-linear curve
# can be fit to the various calibration runs to generate a set of 
# coefficients a,b,c for an asymptotic curve. Then the values from a field 
# mussel can be analyzed to determine the asymptotic maximum count value
# (widest opening) and minimum count value (fully closed), which give us the
# parameters for coefficients a and b in the curve fit. Because the curvature
# coefficient 'c' of the curves is very linear with respect to the coefficient
# 'b' (range of readings), we can plug in the max vs. min range for a particular
# mussel and use that to predict the appropriate curve coefficient 'c' to use
# for that mussel. 

#' A function to generate a set of hall effect calibration data and estimate
#' the relationship between parameters 'b' and 'c', for later use in the 
#' function calcPercentGapeNLME(). 
#' @param SN A text string naming the board serial number, in the form 'SN21'
#' @param Channel numeric value naming the Hall effect sensor channel to be 
#' calibrated (expected values 0-15). 
#' @param gapeCalibFile A text path to the file that holds Hall effect
#' calibration data
#' @return cRegress A linear model object representing the best fit regression
#'  of parameter 'c' on the log-transformed parameter 'b'. 

cRegressEstimate <- function(SN, Channel, gapeCalibFile){
	require(nlme)
	# Create an identifier for the board/channel combination 'SN21Ch01'
	if (Channel < 10){
		newChannel = paste0('0',as.character(Channel))
	} else {
		newChannel = as.character(Channel)
	}
	currentID = paste0(SN,'Ch',newChannel)
	 
	# Get list of calibration data
	gapeCalibs = read.csv(gapeCalibFile)

	gapeCalibs$Serial = factor(gapeCalibs$Serial)
	gapeCalibs$Trial = factor(gapeCalibs$Trial)
	# Create a unique identifier for each board/channel combination,
	# ensuring that the Channel number is always 2 digits
	gapeCalibs$ID = factor(paste0(gapeCalibs$Serial,'Ch',
					ifelse(gapeCalibs$HallChannel<10,
							paste0('0',gapeCalibs$HallChannel),
							gapeCalibs$HallChannel)))

# For any instance where magnet was set up so that values climbed above 512
# when close to the sensor, reverse those values so they are all less than 
# 512. 
	for (i in 1:length(levels(gapeCalibs$ID))){
		if (max(gapeCalibs$Reading[gapeCalibs$ID == 
								levels(gapeCalibs$ID)[i]]) > 515){
			gapeCalibs$Reading[gapeCalibs$ID == 
							levels(gapeCalibs$ID)[i]] = 1023 - 
					gapeCalibs$Reading[gapeCalibs$ID == 
									levels(gapeCalibs$ID)[i]]
		}
	}	
	# Begin generating the asymptotic curve fit parameters for the magnetic
# hall effect sensor data. 
	gC2 = groupedData(Reading~Distance.mm|ID/Trial,data=gapeCalibs)
	

# Fit the asymptotic model and generate a set of coefficient values
# This will adjust each set of a,b,c coefs based on a random effect of
# magID and Trial. 
	
	# Fit separate curves to each group (trial) in the grouped data frame
	mod3 = nlsList(Reading~a-(b*exp(-c*Distance.mm)), 
			data = subset(gC2,subset = ID == currentID), 
			start = c(a = 460, b = 60, c = 0.3),
			control = list(minFactor = 1e-9, msMaxIter = 200, maxiter = 200))
# Extract the a,b,c coefficients and put them in a data frame
	abcvals = data.frame(a = coef(mod3)[,1], b = coef(mod3)[,2],
			c = coef(mod3)[,3])
	
# The fit between 'b' and 'c' can be linearized with a log transform of 'b'
	cRegress = lm(c~log(b), data = abcvals)  

#	plot(c~`log(b)`, data = cRegress$model) # Plot the original data
#	abline(cRegress) # fitted regression line
	
	# Return cRegress as the output
	cRegress
}  # end of cRegressEstimate function.
################################################################################

```

#Functions: loadFieldMetaData, Excisehall, concatGape_function
```{r importFunctions, echo=showcode}

########################################
#' Load a csv data file containing deployment metadata
#' 
#' The deployment metadata file should contain entries for each oyster and 
#' its associated hall effect sensor channel for a given period of time denoted 
#' by the timestamps in 2 columns titled StartIncludeUTC and EndIncludeUTC, which
#' are assumed to have Excel-formatted date and time stamps in the UTC time zone
#' marking the start and end of each known-good deployment period (thus ignoring
#' time periods when the sensors were pulled from the mooring for maintenance or
#' other interruptions). 
#' 
#' @param filename The path and filename of the metadata csv file
#' @param timezone The timezone of the timestamp data in the metadata file. 
#' Default = UTC.
#' @return A data frame containing the same original columns as the metadata 
#' file, but with timestamps formatted as POSIXct values in the appropriate
#' time zone. 

loadFieldMetaData <- function(filename, timezone = 'UTC'){
	metadata = read.csv(file= filename)
	metadata$StartIncludeUTC = as.POSIXct(metadata$StartIncludeUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	metadata$EndIncludeUTC = as.POSIXct(metadata$EndIncludeUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	metadata # return data frame
}

# This function deals with the GAPE data files that have an unnecessary comma at the end of each row of data, which messes with the read.csv import function

#' Concatenate multiple daily gape files into one data frame
#' 
#' @param filenames A vector of filenames (including path) to be concatenated
#' @param myTimeZone A text representation of the timezone that the datalogger clock was set to (i.e. 'UTC' or 'PST8PDT' or 'etc/GMT+8')
#' @param verbose Turn on the progress bar if set to TRUE
#' @return A data frame consisting of the input files concatenated and sorted chronologically. 

ConcatGapeFiles <- function(filenames, myTimeZone = 'UTC',verbose=TRUE){
	if(verbose){
		pb = txtProgressBar(min=0,max = length(filenames), style = 3)
	}
# Open the raw data files and concatenate them.
	for (f in 1:length(filenames)){
		if(verbose) setTxtProgressBar(pb,f)
		con = file(filenames[f])
		line1 = scan(con, what='character', nlines = 1, sep = ',', quiet = TRUE)
		line2 = scan(con, what = 'character', skip = 1, nlines = 1, sep = ',', quiet = TRUE)
		close(con)
		if (length(line2) > length(line1)){
		  # There is an extra column in the data section due to a trailing comma in the data rows so we need to handle the column names correctly on import
		  dattemp = read.csv(filenames[f], skip = 1, header = FALSE,
		                    col.names = c(line1,'Extra'))
		  # Remove the extra column at the end
		  dattemp = dattemp[,-(ncol(dattemp))]
		} else {
		  # number of headers is same as number of data columns, so just import as normal
		  dattemp = read.csv(filenames[f])
		}
		
		
		###########################
	# Columns:
	# POSIXt: elapsed seconds since 1970-01-01 00:00:00 (unix epoch) in whatever
	#         timezone the sensor was set to during deployment. Presumably UTC
	# DateTime: human-readable character date and time, in whatever timezone the
	#         sensor was set to during deployment 
	# SN: serial number of the sensor
	# Hall: raw Hall effect gape sensor reading
	# TempC: temperature in Celsius from TMP107 sensor on the heart rate dongle	
	# Battery.V:  Supply battery voltage
		#########################
	# Convert the DateTime column to a POSIXct object.
		dattemp$DateTime = as.POSIXct(dattemp$DateTime, tz=myTimeZone) 	

		
		# Concatenate the data files. 
		if (f == 1){
			dat = dattemp
		} else if (f > 1){
			dat = rbind(dat,dattemp)
		}
	}
	if(verbose) close(pb)
# Reorder the concatenated data frame by the DateTime values in case the files were not fed in in chronological order
	dat = dat[order(dat$DateTime),]
}

##############################################################
#' Excise questionable rows of data from Hall effect sensor data
#' 
#' @param halldata A dataframe containing timestamps and hall effect data from
#' multiple sensors.
#' @param metadf A data frame of start and end times to use in the data set,
#' formatted as POSIXct time stamps in the same time zone as values in the 
#' timestamps argument. The data frame should also have a column with an OysterID
#' and a hall effect sensor channel for that oyster for each time period laid
#' out in the data frame.
#' @param maxGapFill The maximum number of missing values in the input time 
#' series to impute via linear interpolation. Gaps longer than this will remain 
#' as NA values. 
#' 
#' @return A data frame with columns
#' added on for each oyster with available hall effect sensor data. Missing 
#' periods of hall effect values will be denoted with NA values.  


exciseHall = function(halldata, metadf, maxGapFill = 12) {
	# Get the list of unique OysterID values from excised
	oysters = halldata$SN[1]
	#oysters = oysters[order(oysters)] # reorder if needed 
	
	# Generate output data frame based on initial deployment time and final 
	# raw data time
	time1 = min(metadf$StartIncludeUTC, na.rm=T)
	time2 = max(halldata$DateTime, na.rm=T)
	totaltimeseries = seq(time1,time2,by=60) #every 60 seconds
	
#	for (i in 1:length(oysters)){
		# Create vector to hold hall effect data for this oyster
		# temphall = vector(mode='logical',length = length(totaltimeseries))
		temphall = matrix(data = NA, nrow = length(totaltimeseries), ncol = 3)
		colnames(temphall) = c('Hall', 'Temp.C','Battery.V')
		# temphall[] = NA # Set all entries to NA initially
		# Convert outputdf to a zoo object for easier time handling
		tempoutput = zoo(temphall,order.by = totaltimeseries)
		
		# Get the rows in metadf that refer to the current oyster
		oysterRows = metadf[metadf$SN == oysters[1],]
		
		for (j in 1:nrow(oysterRows)){
			# Get the hall effect sensor channel for this entry
			#tempChannel = as.character(oysterRows$Channel[j])
			# Get the start and end time stamps for this entry
			startTime = oysterRows$StartIncludeUTC[j]
			endTime = oysterRows$EndIncludeUTC[j]
			# Extract the relevant set of data from halldata. If no EndTime in metadata then take all halldata after StartTime
    	  if (is.na(endTime)==T){
    		  	temp1 = halldata[halldata$DateTime>=startTime, 
    				c('DateTime',"Hall", "Temp.C", "Battery.V")]
    		} else {
      		  temp1 = halldata[halldata$DateTime>=startTime & 
      						halldata$DateTime<=endTime, 
      				c('DateTime', "Hall", "Temp.C", "Battery.V")]
  			   }
			
			# temp = halldata[halldata$DateTime>=startTime & 
			# 				halldata$DateTime<=endTime, 
			# 		c('DateTime',"Hall", "Temp.C", "Battery.V")]
			# Convert to a zoo timeseries object
			temp = zoo(temp1[,2:4],order.by=temp1[,1],frequency = 1)
			
			# Insert the temp hall data into the zoo object tempoutput
			# The syntax is fun here: On the left side of the equals sign,
			# you want a set of indices in tempoutput where the time value
			# from tempoutput is found %in% the time values of temp. And on the
			# right side of the equals sign, you want to get hall effect values
			# from temp where the time value in temp is found %in% the time
			# values of tempoutput (a much bigger set)
			tempoutput[index(tempoutput) %in% index(temp), ] = 
					temp[index(temp) %in% index(tempoutput),]
	#	}
		
 		#For brief gaps in the data, use the zoo package function 
		# na.approx() to generate linearly-interpolated values for the missing
		# data. For 5-second interval data, a maxGapFill = 12 will fill up to
		# a 1 minute gap max.
		# If the dataset starts off or ends with NAs, they will not be altered
		tempoutput = na.approx(tempoutput, maxgap = maxGapFill, na.rm=FALSE)
	
	  tempoutput$SN=oysters
		# Convert tempoutput back to a data frame and stick it onto an output
		# dataframe
	#	if (i == 1){
			outputdf = data.frame(DateTime = index(tempoutput), tempoutput)
			names(outputdf)[2] = "Hall"
		#} else if (i > 1) {
		#	tempoutput = as.data.frame(tempoutput)
		#	outputdf = cbind(outputdf,tempoutput)
		#	names(outputdf)[ncol(outputdf)] = as.character(oysters[i])
	#	}
	}
	outputdf  # return outputdf	
	
} # End of exciseHall function
```

#list individal gape files and combine into one csv per SN
```{r get-list-of-files-and-make-big-csv, echo=FALSE}
# Get a list of the IR heart sensor files by looking for filenames that end in IR.csv
#IRfilenames = dir(path = paste0(NewportDir, WestcliffEelgrassDir, 'SN107_2022-07-13_2022-07-30'), 
#		full.names=TRUE, pattern= "*IR.csv")

# Get a list of gape (and temperature) filenames by looking for filenames that end in GAPE.csv
#gapefiles = dir(path = paste0(NewportDir, WestcliffEelgrassDir, 'SN107'), full.names=TRUE, pattern= "*GAPE.csv")
# These files tend to have a comma after the last column's data, which makes read.csv think there's one more column of data than there are names in the header row, which then treats the 1st column of POSIXt values as row numbers, which is unfortunate. 

#import csv metadata_with edits. use unique() on first column. Save result as "serialnumber"
metafile = loadFieldMetaData(filename = (paste0(metafilePath,"field_metadata_forcoding_withoutSN132 - Sheet1.csv")))

serialnumber=unique(metafile[,1])

# go through each serial number in the vector 'serialnumber' and find all files, 
# combine them together, and spit out combogapefile at end
######################################## 

for(i in (serialnumber)){
  gapepath=file.path(paste0(NewportDir,"alldata/",i))
    if(!file.exists(gapepath)) {
     next
    }
  #dir(path=gapepath, full.names = TRUE, pattern=paste0(i,"_GAPE.csv"))
  #generate list of file names
  gapecombo=dir(path=gapepath, full.names = TRUE, pattern=paste0(i,"_GAPE.csv"))
    if(length(gapecombo)==0){
      next # if there are no data files found, skip to next serial number
    }
  #Use concatgapefiles to combine
  gapedat = ConcatGapeFiles(gapecombo, myTimeZone = 'UTC', verbose = FALSE)
# Call exciseHall function to generate a master data frame of raw hall effect readings from the oysters that were present in the field. 
  gapedat = exciseHall(halldata = gapedat, metadf = metafile)
  write.csv(gapedat, paste0(combogapefile, i, "_combogape.csv"), row.names = FALSE)

}
```

#write over combogape files to add in percent gape and percentiles for binary graphing
```{r calcPercentageGape,echo=showcode, eval = evalAll}

#get data from channel 1 of SN20 to calibrate sensor. Generate set of regression coefficients and put them in cRegress. Use cRegress to do % calculation.
chNum=1
cRegress = cRegressEstimate("SN20", chNum,gapecalibfile)

for(i in (serialnumber)){
  #j=paste0(i,"_GAPE.csv")
  gapepath=file.path(paste0(combogapefile, i, "_combogape.csv"))
    if(!file.exists(gapepath)) {
     next
    }
  gapedat=read.csv(gapepath)
	# Convert the Hall effect values into estimated percent gape (0-100%)
	gapedat[,"hallpercent"] = calcPercentGapeNLMEsubset(gapedat[,"Hall"], cRegress, percentileLim = c(0.01,0.95), maxrow=1440*7) 
	    #1440=# min in a day, *7 days for one week of sampling. 
	    #Every day 7 days calculate percent gape. 
	#make empty binary column
	gapedat$binary=NA
	#for hallpercent rows less than or equal to 20, write 0 in binary column 
	gapedat[which(gapedat$hallpercent<=20), "binary"]=0
	#for hallpercent rows greater than or equal to 50, write 1 in binary column 
	gapedat[which(gapedat$hallpercent>=50), "binary"]=1
	#save as one csv per SN
	write.csv(gapedat, paste0(combogapefile, i, "_combogape.csv"))
}

#gapeSN = 'SN21'  
#gape.perc = gapedat

#converts Hall column to 0-100 values so now on scale of 1-100
	#currentChannel = colnames(gapedat)[i]
	# Locate the channel number from currentChannel name
	#chNumRegex = regexpr('[0-9]+', currentChannel)
	# Extract the channel number
	#chNum = as.numeric(substr(currentChannel,
				#	start=chNumRegex,
				#	stop=(chNumRegex+attr(chNumRegex,'match.length')-1)))
	# Generate the regression fit for the relationship between curvature parameter
	# 'c' and the log-transform of 'b'.
  #chNum=1
	#cRegress = cRegressEstimate(gapeSN, chNum,gapecalibfile)
	# Convert the Hall effect values into estimated percent gape (0-100%)
	#gape.perc[,"hallpercent"] = calcPercentGapeNLME(gapedat[,4], cRegress, 
	#		percentileLim = c(0.01,0.95))
	 
# The output data frame LPLgape.pc should now contain estimated percentage
# gape values for each oyster at each time point. 

# Save a copy of the data frame that can be reopened for quicker execution
# of this script instead of re-running the whole raw data processing
#write.csv(gape.perc,
#		file = paste0(gapeDir,SN107combogapefile),
#		row.names=FALSE)

# Create a decimated copy of the data frame (1 sample per minute) to make it
# easier to handle
#LPLgape.perc.dec = LPLgape.perc[seq(1,nrow(LPLgape.perc), by = 12),]
#write.csv(LPLgape.perc.dec,
#		file = paste0(gapeDir,LPL1minGapeFile),
#		row.names=FALSE)

```

#combine combogape files (w/hallpercent, percentile, binary columns) into one giant dataset. Combine with metadata

```{r combine-all-gape-files-and-add-metadata}
#combine all data frames into 1
allgape <- list.files(path=combogapefile, full.names = T) %>% 
  lapply(read_csv) %>% 
  bind_rows 

#combine metafile and allgape by SN to get treatment/site for each sensor
allgapemeta<-merge(allgape,metafile, by='SN')

```

#Subset data by every 10th row, treatment, and site. Plot
```{r}
#take every 10th row from dataframe
my10MinuteSubset = allgapemeta[seq(1,nrow(allgapemeta), by = 10), 'hall.percent'] 

my10MinuteSubset %>%
  #use filter to subset the rows based on the LOCATION column values
  filter(SN == c("SN150", "SN138", "SN164", "SN143","SN191","")) %>% 
  filter(Site=="DeAnza")%>%
  ggplot(aes(x = DateTime, y = binary, col=SN))+
    geom_point(shape=1)+
    geom_jitter(height=0.1)+
    facet_wrap(~Treatment)+
    ylim(-0.1, 1.1)

#Use metadata to plot all sensors from oyster treatment (plot hall %, make time series plot). 3 panel plot- seagrass, mud, oyster treatment for De Anza, do for each site. Or all on same plot w/ diff colors for diff treatments. 
#ideally also align with temp data/tide data. Can be in separate datasets just also needs to have POSIXxt to align timestamps. 
#rtide package that generates time series of predicated tides. Use long beach (LA) harbor tide station. 

#can have giant data file with all sensor data. 

#will need to calculate average open for individuals over time. 
```

#cut out specific times from meta data that shouldn't be used
```{r processRawHallData,echo=showcode, eval = evalAll}
# Call exciseHall function to generate a master data frame of raw hall effect readings from the oysters that were present in the field. This will remove the other sensor channels in the raw data files that didn't have oysters  attached. This function could take several minutes to run for bigger data sets
gapemaster = exciseHall(halldata = allgapemeta, metadf = metafile)      

# Generate a set of oyster locations (bottom or surface) from the metadata file
#LPLoysterLocations = unique(LPLmeta2019[,c(2,4)])
#LPLoysterLocations = LPLoysterLocations[order(LPLoysterLocations$Location),]

# Create an additional column with timestamps in the local standard time zone
# Pacific Standard Time (this will still ignore daylight savings time)
#LPLmaster$DateTimePST = LPLmaster$DateTimeUTC 
#attr(LPLmaster$DateTimePST,'tzone') = 'etc/GMT+8'
```

```{r GapePlotCheckingFunctions}

# Function to load a particular sensor's Hall effect data. This assumes that a path already exists called combogapefile
loadComboGape = function(SN) {
	gape = read.csv(paste0(combogapefile,SN,'_combogape.csv'))
	gape$DateTime = as.POSIXct(gape$DateTime, tz = 'UTC')
	gape
}

# Function to make a quick 2-panel plot of the raw Hall sensor output and the calculate gape percentage
plot2HallFunction = function(gape){
	par(mfrow = c(2,1), mar = c(4,6,2,1))
	plot(gape$DateTime, gape$Hall, type = 'l', col = 3, 
			ylab = 'Raw Hall counts', 
			xlab = 'Date',
			las = 1,
			main = gape$SN[1])
	
	plot(gape$DateTime, gape$hallpercent, type = 'l', col = 2,
			ylab = 'Gape opening %',
			xlab = 'Date',
			las = 1,
			main = gape$SN[1])
	
	par(mfrow=c(1,1))
}

# Function to only plot the raw Hall effect counts
plotRawHallFunction = function(gape){
	par(mfrow = c(1,1), mar = c(4,6,2,1))
	plot(gape$DateTime, gape$Hall, type = 'l', col = 3, 
			ylab = 'Raw Hall counts', 
			xlab = 'Date',
			las = 1,
			main = gape$SN[1])

}

```

```{r testPlotsGape, eval=FALSE}

SN142 = loadComboGape('SN142')
plotRawHallFunction(SN142)


SN191 = loadComboGape('SN191')
plot2HallFunction(SN191)


SN118 = loadComboGape('SN118')
plot2HallFunction(SN118)


SN187 = loadComboGape('SN187')
plot2HallFunction(SN187)

SN152 = loadComboGape('SN152') # The low hump at Jul 27 is shared with SN176
plot2HallFunction(SN152)

SN104 = loadComboGape('SN104')
plot2HallFunction(SN104)

SN176 = loadComboGape('SN176')  # The low hump around Jul 27 is shared with SN152
plot2HallFunction(SN176)

SN174 = loadComboGape('SN174')
plot2HallFunction(SN174)

SN130 = loadComboGape('SN130')
plot2HallFunction(SN130)

SN132 = loadComboGape('SN132')
plot2HallFunction(SN132)

SN123 = loadComboGape('SN123')
plot2HallFunction(SN123)

SN150 = loadComboGape('SN150')
plot2HallFunction(SN150)

SN138 = loadComboGape('SN138')
plot2HallFunction(SN138)

SN164 = loadComboGape('SN164')  # Weird change in opening limit around Aug 6, like it could no longer open as far. SN164
plot2HallFunction(SN164)

SN170 = loadComboGape('SN170')
plot2HallFunction(SN170)

SN115 = loadComboGape('SN115')
plot2HallFunction(SN115)

SN143 = loadComboGape('SN143') # Some kind of malfunction
plot2HallFunction(SN143)

SN191 = loadComboGape('SN191')
plot2HallFunction(SN191)

SN125 = loadComboGape('SN125')  # Listed as dead, maybe why the upper limit shifts down in the last few days?
plot2HallFunction(SN125)

SN197 = loadComboGape('SN197')
plot2HallFunction(SN197)

SN153 = loadComboGape('SN153')
plot2HallFunction(SN153)

SN171 = loadComboGape('SN171') # Strange dip around Aug 1-5
plot2HallFunction(SN171)

SN112 = loadComboGape('SN112')  # I think this one is saturating the sensor when closed, magnet is too close, baseline closed value is possibly spurious
plot2HallFunction(SN112)

SN193 = loadComboGape('SN193')  # Bad sensor
plot2HallFunction(SN193)

SN118 = loadComboGape('SN118') # Maybe died around Aug 9? Or did the magnet fall off? 
plot2HallFunction(SN118)

SN187 = loadComboGape('SN187')
plot2HallFunction(SN187)

SN161 = loadComboGape('SN161')
plot2HallFunction(SN161)

SN157 = loadComboGape('SN157')  # Bad sensor (magnetic field strength saturated probably)
plot2HallFunction(SN157)

SN144 = loadComboGape('SN144')
plot2HallFunction(SN144)

SN159 = loadComboGape('SN159') # Different looking pattern than most others
plot2HallFunction(SN159)

SN148 = loadComboGape('SN148')
plot2HallFunction(SN148)

SN133 = loadComboGape('SN133')
plot2HallFunction(SN133)

```

```{r testPlot}
# Plot the timeseries of gape values
plot(SN107_gape$DateTime, SN107_gape$Hall, type = 'l')

#subset only time in field before death
SN107_gape_fieldonly <- SN107_gape %>%
                        filter(DateTime >= as.POSIXct('2022-07-15 13:00:00',tz="UTC") & DateTime<= as.POSIXct('2022-07-20 07:42:00',tz="UTC"))

plot(SN107_gape_fieldonly$DateTime, SN107_gape_fieldonly$Hall, type = 'l')

```

```{r convertHalltobinary attempts}
#make for loop that converts Hall values to 0s if between 0-40, and 1 if between 70-100 percentile. Use allgapemeta

#code for finding percentile
quantile(df$columnname, probs=0.9)

#didn't work- doesn't separate by SN, assigned 0s and 1s based on all Hall percentiles combined
allgapemeta$binary <- with(allgapemeta, +(Hall >= quantile(Hall, probs=0.75)))

#didn't work- only took out 3 Hall numbers per SN that were in 0.25, 0.5, and 0.75 percentile
allgapemetabinary<-allgapemeta %>% 
  group_by(SN) %>% 
  summarise(Hall = quantile(Hall, c(0.25, 0.5, 0.75)), q = c(0.25, 0.5, 0.75))%>%
  mutate(allgapemeta$q)

SN104$binary <- with(SN104, +(hallpercent >= quantile(hallpercent, probs=c(0,0.75))))

#make everything greater than or equal to 0.75 percentile a 1, everything else is 0
SN104$binary <- with(SN104, +(hallpercent >= quantile(hallpercent, 0.75)))

SN104$percentile=transform(SN104, percentile=findInterval(hallpercent, quantile(hallpercent, seq(0,1, by=.1))))

#could separate by SN in large dataset but don't know how to assign 0s and 1s to 0-0.25 and 0.75-1 respectively
allgapemetabinary%>%
  group_by(SN)%>%
  summarize(binary=quantile(hallpercent, 0.75))

#create column quartile with 1,2,3,4 based on hallpercent percentiles 0-0.25, 0.25-0.5, 0.5-0.75, 0.75-1.0
SN104 <- within(SN104, quartile <- as.integer(cut(hallpercent, quantile(hallpercent, probs=0:4/4), include.lowest=TRUE)))

#create binary column that sets 1 for percentile 1, and 2 for percentile 4, percentile 2 and 3 are NAs
SN104=SN104 %>% mutate(binary =
                     case_when(quartile == 1 ~ "1", 
                               quartile == 2&3 ~ "NA",
                               quartile == 4 ~ "2"))

```

```{r Extracode}
#concatgapefiles works outside of for loop 
#gapedat = ConcatGapeFiles(gapecombo, myTimeZone = 'UTC', verbose = FALSE)

#for(i in (serialnumber)){
 # list.files(dir(path=paste0(NewportDir, WestcliffEelgrassDir,i), full.names = #TRUE, pattern=paste0(i,"_GAPE.csv")))}

#for(i in (serialnumber)){
#  list.files(path=paste0(NewportDir, #WestcliffEelgrassDir,i),pattern=paste0(i,"_GAPE.csv"))}

#gapecombo=paste0(ConcatGapeFiles('../data', myTimeZone="UTC",verbose=FALSE),serialnumber[i],"combogape.csv")

#use "serialnumber" (every row is serial number to be processed) of all serial numbers in field. in this directory, look for folder "SN107" then get list of all gape files in that folder and process them. with dir function can find folder names using something like pattern= "SN107". 
#make for loop go through vector. 
#at end get calculated ranges for sensors. 
#paste0(gapeDir(###place where it saves),serialnumber[i],"combogape.csv")
#if there's no folder for serial number (haven't downloaded data yet) then have it end the loop and go back to the start and try the next number
```

```{r- concatGapeFunction extra}
#' Concatenate multiple daily gape files into one data frame
#' 
#' @param filenames A vector of filenames (including path) to be concatenated. The input files should contain a column named DateTime that will be converted to a POSIXct timestamp class. 
#' @param myTimeZone Specify the time zone of the timestamp data in the imported DateTime column. Default = UTC
#' @param verbose A logical argument specifying whether to show verbose progress output during file ingestion
#'  @return A data frame consisting of the input files concatenated and sorted chronologically. 

# ConcatGapeFiles <- function(filenames, myTimeZone = 'UTC',verbose=FALSE){
# 	if(verbose){
# 		pb = txtProgressBar(min=0,max = length(filenames), style = 3)
# 	}
# # Open the raw data files and concatenate them.
# 	for (f in 1:length(filenames)){
# 		if(verbose) setTxtProgressBar(pb,f)
# 		
# 		dattemp = read.csv(filenames[f])
# 		###########################
# 		# Columns:
# 		# POSIXt: elapsed seconds since 1970-01-01 00:00:00 (unix epoch) in whatever timezone the sensor was set to during deployment. Presumably UTC
# 		# DateTime: human-readable character date and time, in whatever timezone the sensor was set to during deployment 
# 		# Hallx: Raw analog to digital converter value for the indicated Hall effect channel. There are 16 channels (numbered 0-15) on the GapeTracker
# 		# Battery.V:  Supply battery voltage
# 		#########################
# 		# Convert the DateTime column to a POSIXct object.
# 		dattemp$DateTime = as.POSIXct(dattemp$DateTime, tz=myTimeZone) 	
# 		
# 		
# 		# Concatenate the data files. 
# 		if (f == 1){
# 			dat = dattemp
# 		} else if (f > 1){
# 			dat = rbind(dat,dattemp)
# 		}
# 	}
# 	if(verbose) close(pb)
# # Reorder the concatenated data frame by the DateTime values in case the files
# # were not fed in in chronological order
# 	dat = dat[order(dat$DateTime),]
# }
```
