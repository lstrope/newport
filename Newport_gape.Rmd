---
title: "Newport_gape"
output: html_document
date: '2022-08-03'
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#libraries
```{r loadLibraries, echo=FALSE, message=FALSE}
library(tidyverse)
library(scales)
library(readr) 
library(dplyr)
library(Tides)
library(janitor)
library(zoo)
library(esquisse)
library(timetk)
library(ggpattern)
library(ggpubr)
library(EnvStats)
library(FSA) #dunntest
library(rstatix)
```

#import files
```{r fileLocations, echo=FALSE}
# Directory path on Lauren's machine
setwd("~/Documents/Oyster/Rcode")

# Comment out one of the two localpath lines below depending on what computer you're on
#localpath = '../../data/'  # Luke's path
localpath = '../data/'  # Lauren's path

NewportDir = paste0(localpath,'Newport_SSINP/')
WestcliffEelgrassDir = paste0(localpath,'Newport_SSINP/Westcliff/Eelgrass/')
gapeplot=paste0(localpath, 'Newport_SSINP/gapeplotdata/')

# Collated calibration data from May 2019, used for Nov 2019 + 2020 oyster deployments
gapecalibfile = paste0(localpath,'Newport_SSINP/calibration/201905_calibrations.csv')
combogapefile= paste0(localpath,'Newport_SSINP/combinedgape/')

#meta file with serial numbers
metafilePath = paste0(localpath,'Newport_SSINP/metadata/')

#tides
tides=read.csv("../data/Newport_SSINP/environmental_data/LosAngeles_tides_202205-202307.csv")
tides$DateTime=as.POSIXct(tides$TimeUTC, tz="UTC", format="%Y-%m-%d %H:%M")

#precipitation
precip=read.csv("../data/Newport_SSINP/environmental_data/Weather_data_KSNA_202205-202307.csv")
precip$DateTimeUTC=as.POSIXct(precip$DateTimeUTC, tz="UTC", format="%Y-%m-%d %H:%M")

#plots
plots=paste0(localpath,'Newport_SSINP/plots/')



```

#graphing settings
```{r}
sitePalette <- c("#E69F00", "#56B4E9", "#009E73", "#0072B2")
sitePalette2 <- c("#E69F00", "#E69F00","#56B4E9","#56B4E9", "#009E73", "#009E73","#0072B2","#0072B2")
treatment2_palette = c("#C9C9C9", "#999999", "#70DCBE", "#009E73", "#DC9867", "chocolate4")
treatment_palette=c("chocolate4","#999999", "#009E73")
season_palette=c("#009E73","#D55E00","#56B4E9","#CC79A7")

# Function to scale secondary axis
scale_function <- function(x, scale, shift){
  return ((x)*scale - shift)
}

# Function to scale secondary variable values
inv_scale_function <- function(x, scale, shift){
  return ((x + shift)/scale)
}

my_comparisons_season2=list(c("Winter","Summer"),c("Fall","Winter"),
  c("Winter","Spring"))
my_comparisons_season=list(c("Fall","Summer"),c(
  "Winter","Summer"),c("Spring","Summer"),c("Fall","Winter"),c("Fall","Spring"),c("Winter","Spring"))
my_comparisons_treat2=list(c("Oyster","Eelgrass"),c("Oyster","Mud"),c(
  "Eelgrass","Mud"))
my_comparisons=list(c("Winter","Summer"),c("Fall","Winter"),c("Winter","Spring"))
my_comparisons_site=list(c("DeAnza","PCH"),c("DeAnza","Shellmaker"),c(
  "DeAnza","Westcliff"),c("PCH","Shellmaker"),c("PCH","Westcliff"),c("Shellmaker","Westcliff"))
```


#Functions: gapBounds, gapBounds2, hallFilter, DistFromCounts,percentileRange, calcPercentGapeNLME, calcPercentGapeNLMEsubset,CRegressEstimate
```{r gapBoundsFunctions, echo=showcode}

# A set of functions originally taken from MusselTracker_data_proc4.R to 
# handle the gape data

################################################################################
### gapBounds function
# A function to determine where there are gaps of missing data, so that we can 
# apply a filter to the good chunks of data. Most filters fail if there are NAs
# present, and our remaining gaps in the data are big enough that they probably
# shouldn't be smoothed-over anyhow. 
#' A function to find the start and end indices of numeric data in a vector. 
#' Useful for dealing with timeseries that have gaps, and operations that need
#' to operate the separate chunks of contiguous numeric values
#' @param value A vector of numeric values, with NAs 
#' @return A data frame with 2 columns, Start and End, which contain the row
#' indices for the first good and last good values in each contiguous run of
#' numeric values. 	
gapBounds <- function (values){
	# This function returns a data frame of 2 columns, start + end, that give
	# the row indices of the start and end of each run of good data in the 
	# input vector. It should handle data streams that start with NAs and
	# should handle streams that end with NAs. 
	
	# Use run length encoding function to find NA gaps
	gaps = rle(is.na(values))
	# If the first entry in gaps is TRUE, this indicates that the data started
	# with NA values.
	if (gaps$values[1] == TRUE) {
		startNA = TRUE	# Data started with NAs
	} else {	
		startNA = FALSE	 # Data started with real values
	}
	# If the last entry in gaps is TRUE, the data end on NAs
	if (gaps$values[length(gaps$values)] == TRUE){
		endNA = TRUE	# Data ends with NAs
	} else {
		endNA = FALSE	# Data ends with real values
	}
	
	# The number of gaps with value == TRUE is the number of good runs of data
	# and gaps with value == FALSE are the starts of runs of NAs. This will
	# get the number of FALSE (negated to TRUE) values in gaps. A dataset
	# ending in good data will have a last entry in gaps of FALSE. 
	numgaps = sum(!gaps$values) 
	
	# Create the output data frame
	results = data.frame(Start = integer(numgaps), End = integer(numgaps))
	
	if (!startNA) {
		# If startNA == FALSE (started on good data, the first entry should 
		# always be 1 (1st row) 
		# If there are no gaps, the
		# contents of gaps$lengths will just be the length of the values vector
		results$Start[1] = 1
		results$End[1] = gaps$lengths[1]
	} else if (startNA){
		# However, if the dataset starts with NAs, the first entry in gaps will
		# be the index of the first good data, while the 2nd entry will be the 
		# start of the next stretch of NAs
		results$Start[1] = gaps$lengths[1]+1
		results$End[1] = sum(gaps$lengths[1:2])
	}
	
	# If there is more than 1 entry in gaps$lengths, process the other gaps
	j = 2; # counter
	if (numgaps > 1){
		if (!startNA & endNA){
			# If the dataset ends on NAs (TRUE last), truncate gapind by 1
			gapind = seq(2,length(gaps$lengths)-1, by = 2)
		} else if (!startNA & !endNA) {
			# If the dataset ends on good values (FALSE last)
			gapind = seq(2,length(gaps$lengths), by = 2)
		} else if (startNA & endNA) {
			# If dataset starts on NAs (TRUE 1st) and ends on NAs (TRUE last)
			gapind = seq(3, length(gaps$lengths)-1, by = 2)
		} else if (startNA & !endNA) {
			# If dataset starts on NAs (TRUE 1st) and ends on good data 
			# (FALSE last)
			gapind = seq(3, length(gaps$lengths), by = 2)
		}	
		# Step through the rest of the gaps object to find the start and end
		# points of each stretch of good data. 
		for (i in gapind){
			nextstart = sum(gaps$lengths[1:i]) + 1
			nextend = sum(gaps$lengths[1:(i+1)])
			results$Start[j] = nextstart
			results$End[j] = nextend
			j = j + 1
		}
	} # end of if (numgaps > 1)
	results	# return the results dataframe, 2 columns Start and End
}

################################################################################
### gapBounds2 function
#' Function that sets bounds for upper and lower limit of hall counts to calculate gape opening % while dealing with growth rates. Change chunk length to certain number of days (hall count rows) (arbitrary 5 days). Use gapbounds function to set new ranges. 
#' @param mygaps A data frame from gapbounds function
#' @param maxrow Max length of rows (time) to subset out 

gapBounds2=function(mygaps, maxrow){
  mydiff=mygaps$End[1]-mygaps$Start[1]
  if (mydiff>maxrow){
    quotient=mydiff/maxrow #quotient=number of subsets (chunks) to make based on max row
    nloops = floor(quotient)
    
    for (i in 1:nloops){
      if (i == 1 & nloops>=2){
        mygaps2 = data.frame(Start = mygaps$Start[1], End = mygaps$Start[1]+maxrow)
      } else if (i==1 & nloops<2){
        mygaps2 = data.frame(Start = mygaps$Start[1], End = mygaps$End[1])
      } else if (i > 1 & i < nloops){
        temp = data.frame(Start = mygaps2$End[i-1]+1, End = mygaps2$End[i-1]+maxrow )
        mygaps2 = rbind(mygaps2, temp)
      } else if (i > 1 & i == nloops){
        temp = data.frame(Start = mygaps2$End[i-1]+1, End = mygaps$End[1])
        mygaps2 = rbind(mygaps2, temp)
      }
    }
  } else if (mydiff <= maxrow){
    # Handle the case where the 1st entry in mygaps is shorter than maxrow, just copy the values
    # over to a dataframe we're calling mygaps2
    mygaps2 = data.frame(Start = mygaps$Start[1], End = mygaps$End[1])
  }
  # Handle cases where there is more than a single row in mygaps
  if (nrow(mygaps) > 1) {
    for (j in 2:nrow(mygaps)){
      mydiff=mygaps$End[j]-mygaps$Start[j]
      if (mydiff>maxrow){
        quotient=mydiff/maxrow #quotient=number of subsets (chunks) to make based on max row
        nloops = floor(quotient)
        
        for (i in 1:nloops){
          if (i == 1){
            temp = data.frame(Start = mygaps$Start[j], End = mygaps$Start[j]+maxrow)
            mygaps2 = rbind(mygaps2, temp)
          } else if (i > 1 & i < nloops){
            temp = data.frame(Start = mygaps2$End[nrow(mygaps2)]+1, End = mygaps2$End[nrow(mygaps2)]+maxrow)
            mygaps2 = rbind(mygaps2, temp)
          } else if (i == nloops){
            temp = data.frame(Start = mygaps2$End[nrow(mygaps2)]+1, End = mygaps$End[j])
            mygaps2 = rbind(mygaps2, temp)
          }
        }
      } else {
        # handle the case where the next entry in mygaps is shorter than maxrow, just copy the start/end from 
        # mygaps into mygaps2
        mygaps2 = rbind(mygaps2, mygaps[j,])
      }
    }
  }
  mygaps2 # return this data frame at the end of the function
  
}



#{r hallFilter_function}
################################################################################
### hallFilter function
#' A function to apply a Butterworth 1st order low-pass filter to a vector
#'  of data. If there are NA values in the original data, they will be 
#'  preserved in the output vector. The filter is set to filter at 1/10 the
#' original sampling rate (which is 5 sec interval (0.2Hz), producing an approximate
#' 50-second window of smoothing. Raw input data are initially centered on 0
#' to avoid artifacts at the start of the filter, and the output is re-centered
#' at the original starting value after the filtering is applied. 
#' 
#' @param hallData A vector of hall effect data.
#' @return A vector of filtered hall effect sensor data.
#'  
hallFilter = function(hallData){	
	# Define a butterworth low-pass filter, 1st order, to filter at 1/10 the 
	# sampling rate (which was 5 secs (0.2Hz))
	# myfilter = signal:::butter(1,0.1,type='low', plane = 'z')
	# Find the start and end of any gaps in the data set using the gapBounds
	# function defined earlier
	mygaps = gapBounds(hallData)
	# Apply the filter to the good runs of data
	for (i in 1:nrow(mygaps)){
		# Extract the run of good data
		dats = hallData[mygaps$Start[i]:mygaps$End[i]]
		# Find the offset of the data from zero
		offset = dats[1]
		# Subtract the offset off of all of the data in this run
		# (The butterworth filter returns large transient values at the start
		# of the run if the value is much different from zero)
		dats = dats - offset
		# Call the filter routine to apply the filter
		yfiltered = signal:::filter(myfilter,dats)
		# Add the offset back on so the data are back on their original scale
		yfiltered = yfiltered + offset
		# Write the filtered data back into the data vector
		hallData[mygaps$Start[i]:mygaps$End[i]] = yfiltered
	}
	# Round the filtered data back to the nearest whole number, since these
	# represent ADC count data.
	hallData = round(hallData)
	hallData	# return vector of filtered data
}



####{r DistFromCounts_function}
################################################################################
### DistFromCounts function
# Define a non-linear function that allows you to plug in a Hall effect sensor
# count value and generate an approximate distance in mm. Supply the 
# coefficients a, b, c, based on a nls curve fit for counts as a function of
# distance. 
# In the above equation, 'a' is the effective asymptotic value
# 'b' is the range of values between the minimum observed count and the 
# asymptotic count (so a min count of 400 and asymptote of 500 gives b = 100)
# 'c' is effectively a curvature parameter. Larger values of 'c' yield a 
# smaller change in Distance as Counts increases (when back-calculating 
# Distance), particularly for low Counts values. 
#' A function to back-calculate an asymptotic function to generate a distance
#' value based on a hall effect sensor count value input.
#' 
#' @param Counts A vector of hall effect sensor values, usually between 0 and 
#' 512 (from a 10-bit analog-to-digital converter). 
#' @param a A parameter derived from an asymptotic curve fit, representing the
#' asymptotic count value
#' @param b A parameter derived from an asymptotic curve fit, representing the 
#' range from the minimum count up to the asymptotic value. 
#' @param c A parameter derived from an asymptotic curve fit, representing the
#' curvature of the fit. 
#' @returns A vector of distance values of the same length as Counts. Any input 
#' values equal or larger than the asymptotic parameter a will return Inf.

DistFromCounts <- function(Counts, a,b,c){
	dist = (log((Counts - a)/(-1*b))) / (-1*c)
	dist
}


###{r percentileRange_function}
################################################################################
### percentileRange function
# Define a function to calculate the average reading for the upper and lower
# x percentile of the hall effect data. 
#' Calculate the average of the specified lower percentile and upper percentile
#' and return the values that correspond to those means (rounded to integers).
#' 
#' @param hallData A vector of analog-to-digital convertor counts from a 
#' Hall effect sensor. The values are assumed to start low (<512) when the 
#' magnetic signal is strong (close to sensor) and increase towards 512 when
#' the magnetic signal is weak (far from sensor).
#' @param percentileLim A set of 2 numeric values between 0 and 1, usually close
#'  to 0.01 and 0.99
#' used to define the lower and upper percentile limits. A value of 
#' 0.01 would cause the 1st percentile sensor values to be used as the
#' lower limits, and similar for the upper limit. 
#' @return A two element vector for the lower and upper values in hallData that
#' represent the lower and upper percentiles

percentileRange <- function (Hallvec, percentileLim = c(0.01,0.99)){
	# Remove any NA's
	temp = Hallvec[!is.na(Hallvec)]
	# Reorder the values smallest to largest
	temp = temp[order(temp)]
	# Get the index of the entry closest to percentileLim
	indx = round(percentileLim[1] * length(temp)) 
	# Calculate the mean value for the lower % of closed valve values and
	# round up to the next integer value
	closedVal = ceiling(mean(temp[1:indx]))
	
	# Now do the same for the other end of the range of hall effect values
	# These would normally represent "fully open" readings near 512 if the 
	# magnet and sensor are situated so that the magnet drives the signal
	# below 512 when it approaches the sensor as the shell valves close. 
	indx = round(percentileLim[2] * length(temp))
	# Calculate the mean value for the upper % of open valve values and round
	# down to the next integer value
	openVal = floor(mean(temp[indx:length(temp)]))
	result = c(closedVal, openVal)
	result # return the two values, always smallest then largest
}


####{r calcPercentGapeNLME_function}
################################################################################
### calcPercentGapeNLME function
# Use this function to calculate the percentage gape (0-100%)
# This calculates the baseline fully-closed and fully-open values based on the upper and lower 1% of hall effect sensor data for the given range of rows in the input dataset. Supply a vector of Hall effect sensor data. This function will find any gaps in the vector (NA's) and calculate a new baseline for each section of good data. 
#' Convert hall effect count data (integer values from analog-digital converter) into percent gape values between 0 and 100\%. ' 
#' @param hallData A vector of analog-to-digital convertor counts from a  Hall effect sensor. The values are assumed to start low (<512) when the magnetic signal is strong (close to sensor) and increase towards 512 when the magnetic signal is weak (far from sensor).
#' @param cRegress A linear model object (lm) for the relationship of 'c' versus the log-transform of 'b'. 
#' @param percentileLim A numeric value between 0 and 1, usually close to 0.01, used to define the lower and upper percentile limits (the upper limit is mirrored from the lower limit). A value of 0.01 would cause the 1st and 99th percentile sensor values to be used as the lower and upper limits.
#' @return A vector of the same length as hallData containing percentage values 0 to 100, representing the shell valve gape opening. A value of 0 is a fully closed shell (strong magnetic signal) and 100 is a fully open shell (weak magnetic signal).

calcPercentGapeNLME <- function (hallData, cRegress, percentileLim = 0.01){
	require(nlme)
	# Get the row indices for the major gaps that exist now
	mygaps = gapBounds(hallData)
	# Create an empty output vector of the same length as the input data. 
	outputData = vector(mode = 'numeric', length = length(hallData))
	outputData[] = NA # Convert all the values to NA to begin with
	
	# Now for each section of the input data, calculate the percent gape. This involves using the entries in mygaps to subset the input data and calculate individual percentile values for each contiguous section of data. If there is a significant gap in the data (usually > 30 sec), the percentages will be re-calculated for each contiguous section of data. This should accommodate any sensor/magnet re-glueing issues. 
	for (i in 1:nrow(mygaps)){
		st = mygaps$Start[i]
		end = mygaps$End[i]
		# First calculate the upper and lower 1% values of the Hall readings. These will be count values representing the fully closed (lowest) and fully open (highest, closest to 512) values.
		myrange = percentileRange(hallData[st:end], 
				percentileLim = percentileLim)
		
		# Now truncate count values that are outside myrange to the limits of myrange.
		rowIndices = which(hallData[st:end] < myrange[1])
		hallData[(st+rowIndices)] = myrange[1]
		rowIndices = which(hallData[st:end] > myrange[2])
		hallData[(st+rowIndices)] = myrange[2]
		
		# The count values are now constrained within a range that should encompass fully closed (0mm) to fully open (~5mm gape). Now calculate the approximate gape opening based on the count values and a curve fit. 
		
		# Define the three asymptotic curve parameters a,b,c using the data for the current chunk of data.
		aVal = max(hallData[st:end],na.rm=TRUE)+1 # add 1 count to avoid Inf output.
		bVal = max(hallData[st:end],na.rm=TRUE) - 
				min(hallData[st:end],na.rm=TRUE)
		# Calculating c requires the linear model object supplied as cRegress, which will use the log-transformed 'b' value to estimate the 'c' parameter
		cVal = predict(cRegress, newdata = list(b = bVal))
		
		# Use DistFromCounts function to estimate valve gape distance using the input hall effect count data at each time point and the parameters a,b,c
		temp2 = DistFromCounts( hallData[st:end], 
				a = aVal,
				b = bVal,
				c = cVal)
		
		# Replace any fully-open Inf values with the largest good max distance
		temp2[is.infinite(temp2)] = max(temp2[is.finite(temp2)])
		# Replace any negative distance values with the 0 distance value
		temp2[which(temp2 < 0)] = 0
		# Convert to percentage by dividing all distances by the maximum value
		outputData[st:end] = (temp2 / max(temp2,na.rm=TRUE)) * 100
		
		# Round the values off to a reasonable precision
		outputData[st:end] = round(outputData[st:end],1)
		
	}	
	outputData	# return the output data vector
}	# end of calcPercentGapeNLME function




####{r calcPercentGapeNLMEsubset_function}
################################################################################
### calcPercentGapeNLMEsubset function
# Use this function to calculate the percentage gape (0-100%) of only a subset of data (days) at a time to account for growth of oyster shell. 
# This calculates the baseline fully-closed and fully-open values based on the upper and lower 1% of hall effect sensor data for the given range of rows in the input dataset. Supply a vector of Hall effect sensor data. This function will find any gaps in the vector (NA's) and calculate a new baseline for each section of good data. 
#' Convert hall effect count data (integer values from analog-digital converter) into percent gape values between 0 and 100\%. ' 
#' @param hallData A vector of analog-to-digital converter counts from a  Hall effect sensor. The values are assumed to start low (<512) when the magnetic signal is strong (close to sensor) and increase towards 512 when the magnetic signal is weak (far from sensor).
#' @param cRegress A linear model object (lm) for the relationship of 'c' versus the log-transform of 'b'. 
#' @param percentileLim A numeric value between 0 and 1, usually close to 0.01, used to define the lower and upper percentile limits (the upper limit is mirrored from the lower limit). A value of 0.01 would cause the 1st and 99th percentile sensor values to be used as the lower and upper limits.
#' @param myPercentThreshold A value from 0 to 1 used during the final conversion from distance to 
#' percentage gape to filter all very wide values to just be 100%. You can put a value such as 0.9
#' and this will take any estimated distance value greater than the 90th percentile for the current
#' chunk of hall data and convert all of those large values to 100%. All of the smaller distance values
#' will also be scaled relative to this 90th percentile distance, meaning that smaller distances will 
#' appear as greater percentage opening values than they would if myPercentThreshold were set to the 
#' default 1.0. 
#' @return A vector of the same length as hallData containing percentage values 0 to 100, representing the shell valve gape opening. A value of 0 is a fully closed shell (strong magnetic signal) and 100 is a fully open shell (weak magnetic signal).
#' @param maxrow A number of rows used to define the upper and lower limits of hall sensor calculations. Chose arbitrary 7 day chunk. may want to be 5 days bc if have little bit of leftover chunk then can  add on to previous so then last chunk is 7 days. 
#' @param minrows The minimum number of rows of contiguous hall data that we will allow to be converted
#' to a percentage gape. Because very short chunks of hall data will probably not encompass the full 
#' range of opening and closing, we wouldn't want to try to calculate 0-100% open values. minrows will be
#' specified as a number of rows, so the conversion to length of time depends on how frequently the 
#' hall data were being sampled. Lauren's BivalveBit sensors were collecting once a minute. 1440 minutes
#' in a day

calcPercentGapeNLMEsubset <- function (hallData, cRegress, percentileLim = c(0.01,0.99), maxrow, minrows = 1440, myPercentThreshold = 1.0){
	require(nlme)
  if (exists('aValOld')) {rm(aValOld)}
  if (exists('oldthresholdvalue')) {rm(oldthresholdvalue)}
	# Get the row indices for the major gaps that exist now
	mygaps = gapBounds(hallData) #identify start and end of non-NA chunks of data.
	
	# Check if any of the Start/End values in mygaps are less than minrows apart. If they are, remove
	# that entry from mygaps
	mydiffs = mygaps$End - mygaps$Start
	# remove chunks that are too small 
	mygaps = mygaps[which(mydiffs > minrows),] 
	
	if (nrow(mygaps > 0)){
	  # Use gapBounds2 to subdivide the continuous data into chunks of time that are defined by maxrow
  	mygaps = gapBounds2(mygaps, maxrow)
  	
  	# Create an empty output vector of the same length as the input data. 
  	outputData = vector(mode = 'numeric', length = length(hallData))
  	outputData[] = NA # Convert all the values to NA to begin with
  	
  	# Now for each section of the input data, calculate the percent gape. This involves using the entries in mygaps to subset the input data and calculate individual percentile values for each contiguous section of data. If there is a significant gap in the data (usually > 30 sec), the percentages will be re-calculated for each contiguous section of data. This should accommodate any sensor/magnet re-glueing issues. 
  	for (i in 1:nrow(mygaps)){
  		st = mygaps$Start[i]
  		end = mygaps$End[i]
  		# First calculate the upper and lower 1% values of the Hall readings. These will be count values representing the fully closed (lowest) and fully open (highest, closest to 512) values.
  		myrange = percentileRange(hallData[st:end], 
  				percentileLim = percentileLim)
  		
  		# Now truncate count values that are outside myrange to the limits of myrange.
  		rowIndices = which(hallData[st:end] < myrange[1])
  		hallData[((st-1)+rowIndices)] = myrange[1]
  		rowIndices = which(hallData[st:end] > myrange[2])
  		hallData[((st-1)+rowIndices)] = myrange[2]
  		
  		# The count values are now constrained within a range that should encompass fully closed 
  		# (0mm) to fully open (~5mm gape). Now calculate the approximate gape opening based on the count values and a curve fit. 
  		# Define the three asymptotic curve parameters a,b,c using the data for the current chunk of data.
  		# Compare to previous chunk's aVal, if current chunk is less then use previous chunk's aVal
  		if(exists('aValOld')){
  		  aVal = max(hallData[st:end],na.rm=TRUE)+1 # add 1 count to avoid Inf output
  		  if (aValOld > aVal){
  		    aVal = aValOld  # if the current aVal is lower than the previous aValOld, keep using the aValOld
  		  } else if (aValOld <= aVal){
  		    aValOld = aVal # Update aValOld to use this new larger aVal from the current chunk of data
  		  }
  		} else {
  		  # If aValOld doesn't exist yet, find the aVal value and also set aValOld to equal aVal'
  		  # aVal=value at maximum opening
  		  aVal = max(hallData[st:end],na.rm=TRUE)+1 # add 1 count to avoid Inf output
  		  aValOld = aVal  # store the max value for the next time through this loop
  		}
  		# bVal=the difference between the highest value (aVal) and the lowest (closed)
  		# value in this chunk
  		bVal = aVal - 
  				min(hallData[st:end],na.rm=TRUE)
  		# Calculating c requires the linear model object supplied as cRegress, 
  		# which will use the log-transformed 'b' value to estimate the 'c' parameter
  		cVal = predict(cRegress, newdata = list(b = bVal))
  		
  		# Use DistFromCounts function to estimate valve gape distance using the input hall
  		# effect count data at each time point and the parameters a,b,c
  		temp2 = DistFromCounts(hallData[st:end], 
  				a = aVal,
  				b = bVal,
  				c = cVal)
  		
  		# Replace any fully-open Inf values with the largest good max distance
  		temp2[is.infinite(temp2)] = max(temp2[is.finite(temp2)])
  		# Replace any negative distance values with the 0 distance value
  		temp2[which(temp2 < 0)] = 0
  		# Convert to percentage by dividing all distances by the maximum value
  		# outputData[st:end] = (temp2 / max(temp2,na.rm=TRUE)) * 100  # original version 0-100%
  		
  		orderedtemp2 = temp2[order(temp2)] # order all the distance values from small to large
  		
  		thresholdindex =  floor(length(temp2) * myPercentThreshold) # find row index of the percentile
  		thresholdvalue  = orderedtemp2[thresholdindex] # grab the value that is the percentile limit
  		
  		if (!exists('oldthresholdvalue')) {
  		  oldthresholdvalue = thresholdvalue
  		} else if (exists('oldthresholdvalue')) {
  		  if (oldthresholdvalue > thresholdvalue) {
  		    # The previous chunk's X-percentile value was larger than this chunk's, so use the old x-percentile value
  		    thresholdvalue = oldthresholdvalue
  		  } else if (thresholdvalue >= oldthresholdvalue){
  		    # If this new chunk's x-percentile value is larger than the previous chunk's, update old x-percentile value
  		    oldthresholdvalue = thresholdvalue
  		  }
  		}
  		temp3 = (temp2 / thresholdvalue ) * 100 # divide all values by the percentile threshold value
  		temp3[which(temp3 > 100)] = 100 # filter any percentages > 100 to just be 100
  		outputData[st:end] = temp3
  		
  		
  		# Round the values off to a reasonable precision
  		outputData[st:end] = round(outputData[st:end],1)
  	
	  }
	}	else if (nrow(mygaps) == 0){
	  # None of the chunks was long enough to use, so we have no percentage data to return
	  outputData = vector(mode = 'numeric', length = 0L)
	}
	
	outputData	# return the output data vector
}	# end of calcPercentGapeNLME function



###{r cRegress_function}
#cRegressEstimate helps calibrate sensors using CalibFileName.csv file 
################################################################################
################################################################################
# Function cRegressEstimate
# Currently this represents the best data showing how the relationship between
# magnet signal and distance is repeatable and consistent among different
# magnets (all from the same batch/part number). As such, a non-linear curve
# can be fit to the various calibration runs to generate a set of 
# coefficients a,b,c for an asymptotic curve. Then the values from a field 
# mussel can be analyzed to determine the asymptotic maximum count value
# (widest opening) and minimum count value (fully closed), which give us the
# parameters for coefficients a and b in the curve fit. Because the curvature
# coefficient 'c' of the curves is very linear with respect to the coefficient
# 'b' (range of readings), we can plug in the max vs. min range for a particular
# mussel and use that to predict the appropriate curve coefficient 'c' to use
# for that mussel. 

#' A function to generate a set of hall effect calibration data and estimate
#' the relationship between parameters 'b' and 'c', for later use in the 
#' function calcPercentGapeNLME(). 
#' @param SN A text string naming the board serial number, in the form 'SN21'
#' @param Channel numeric value naming the Hall effect sensor channel to be 
#' calibrated (expected values 0-15). 
#' @param gapeCalibFile A text path to the file that holds Hall effect
#' calibration data
#' @return cRegress A linear model object representing the best fit regression
#'  of parameter 'c' on the log-transformed parameter 'b'. 

cRegressEstimate <- function(SN, Channel, gapeCalibFile){
	require(nlme)
	# Create an identifier for the board/channel combination 'SN21Ch01'
	if (Channel < 10){
		newChannel = paste0('0',as.character(Channel))
	} else {
		newChannel = as.character(Channel)
	}
	currentID = paste0(SN,'Ch',newChannel)
	 
	# Get list of calibration data
	gapeCalibs = read.csv(gapeCalibFile)

	gapeCalibs$Serial = factor(gapeCalibs$Serial)
	gapeCalibs$Trial = factor(gapeCalibs$Trial)
	# Create a unique identifier for each board/channel combination,
	# ensuring that the Channel number is always 2 digits
	gapeCalibs$ID = factor(paste0(gapeCalibs$Serial,'Ch',
					ifelse(gapeCalibs$HallChannel<10,
							paste0('0',gapeCalibs$HallChannel),
							gapeCalibs$HallChannel)))

# For any instance where magnet was set up so that values climbed above 512
# when close to the sensor, reverse those values so they are all less than 
# 512. 
	for (i in 1:length(levels(gapeCalibs$ID))){
		if (max(gapeCalibs$Reading[gapeCalibs$ID == 
								levels(gapeCalibs$ID)[i]]) > 515){
			gapeCalibs$Reading[gapeCalibs$ID == 
							levels(gapeCalibs$ID)[i]] = 1023 - 
					gapeCalibs$Reading[gapeCalibs$ID == 
									levels(gapeCalibs$ID)[i]]
		}
	}	
	# Begin generating the asymptotic curve fit parameters for the magnetic
# hall effect sensor data. 
	gC2 = groupedData(Reading~Distance.mm|ID/Trial,data=gapeCalibs)
	

# Fit the asymptotic model and generate a set of coefficient values
# This will adjust each set of a,b,c coefs based on a random effect of
# magID and Trial. 
	
	# Fit separate curves to each group (trial) in the grouped data frame
	mod3 = nlsList(Reading~a-(b*exp(-c*Distance.mm)), 
			data = subset(gC2,subset = ID == currentID), 
			start = c(a = 460, b = 60, c = 0.3),
			control = list(minFactor = 1e-9, msMaxIter = 200, maxiter = 200))
# Extract the a,b,c coefficients and put them in a data frame
	abcvals = data.frame(a = coef(mod3)[,1], b = coef(mod3)[,2],
			c = coef(mod3)[,3])
	
# The fit between 'b' and 'c' can be linearized with a log transform of 'b'
	cRegress = lm(c~log(b), data = abcvals)  

#	plot(c~`log(b)`, data = cRegress$model) # Plot the original data
#	abline(cRegress) # fitted regression line
	
	# Return cRegress as the output
	cRegress
}  # end of cRegressEstimate function.
################################################################################

```

#Functions: loadFieldMetaData, Excisehall, concatGape_function
```{r importFunctions, echo=showcode}

########################################
# loadFieldMetaData function
#' Load a csv data file containing deployment metadata and maintenance data
#' 
#' The deployment metadata file should contain entries for each oyster and 
#' its associated hall effect sensor channel for a given period of time denoted 
#' by the timestamps in 2 columns titled StartIncludeUTC and EndIncludeUTC, which
#' are assumed to have Excel-formatted date and time stamps in the UTC time zone
#' marking the start and end of each known-good deployment period (thus ignoring
#' time periods when the sensors were pulled from the mooring for maintenance or
#' other interruptions). 
#' 
#' @param filename The path and filename of the metadata csv file
#' @param timezone The timezone of the timestamp data in the metadata file. 
#' Default = UTC.
#' @return A data frame containing the same original columns as the metadata 
#' file, but with timestamps formatted as POSIXct values in the appropriate
#' time zone. 

loadFieldMetaData <- function(filename, timezone = 'UTC'){
	metadata = read.csv(file= filename)
	metadata$StartIncludeUTC = as.POSIXct(metadata$StartIncludeUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	metadata$EndIncludeUTC = as.POSIXct(metadata$EndIncludeUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
		metadata$timeofdeath = as.POSIXct(metadata$timeofdeath,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	metadata # return data frame
}

loadFieldMaintData <- function(filename,timezone = 'UTC'){
	maintdata = read.csv(file=filename)
	maintdata$StartMaintUTC = as.POSIXct(maintdata$StartMaintUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	maintdata$EndMaintUTC = as.POSIXct(maintdata$EndMaintUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	maintdata # return data frame
}

################################################################
# ConcatGapeFiles
# This function deals with the GAPE data files that have an 
# unnecessary comma at the end of each row of data, which messes with the read.csv import function

#' Concatenate multiple daily gape files into one data frame
#' 
#' @param filenames A vector of filenames (including path) to be concatenated
#' @param myTimeZone A text representation of the timezone that the datalogger clock was set to (i.e. 'UTC' or 'PST8PDT' or 'etc/GMT+8')
#' @param verbose Turn on the progress bar if set to TRUE
#' @return A data frame consisting of the input files concatenated and sorted chronologically. 

# ConcatGapeFiles <- function(filenames, myTimeZone = 'UTC',verbose=TRUE){
ConcatGapeFiles <- function(filenames, myTimeZone = 'UTC', code = '', startDate = NULL, endDate = NULL, verbose=TRUE){
  if (exists('dat')) { rm(dat)} # clear dat from memory
	if(verbose){
		pb = txtProgressBar(min=0,max = length(filenames), style = 3)
	}
# Open the raw data files and concatenate them.
	for (f in 1:length(filenames)){
		if(verbose) setTxtProgressBar(pb,f)
		con = file(filenames[f])
		line1 = scan(con, what='character', nlines = 1, sep = ',', quiet = TRUE)
		line2 = scan(con, what = 'character', skip = 1, nlines = 1, sep = ',', quiet = TRUE)
		close(con)
		if (length(line2) > length(line1)){
		  # There is an extra column in the data section due to a trailing comma in the data rows so we need to handle the column names correctly on import
		  dattemp = read.csv(filenames[f], skip = 1, header = FALSE,
		                    col.names = c(line1,'Extra'))
		  # Remove the extra column at the end
		  dattemp = dattemp[,-(ncol(dattemp))]
		} else {
		  # number of headers is same as number of data columns, so just import as normal
		  dattemp = read.csv(filenames[f])
		}
		
		
		###########################
	# Columns:
	# POSIXt: elapsed seconds since 1970-01-01 00:00:00 (unix epoch) in whatever
	#         timezone the sensor was set to during deployment. Presumably UTC
	# DateTime: human-readable character date and time, in whatever timezone the
	#         sensor was set to during deployment 
	# SN: serial number of the sensor
	# Hall: raw Hall effect gape sensor reading
	# TempC: temperature in Celsius from TMP107 sensor on the heart rate dongle	
	# Battery.V:  Supply battery voltage
		#########################
	# Convert the DateTime column to a POSIXct object.
		dattemp$DateTime = as.POSIXct(dattemp$DateTime, tz=myTimeZone, format="%Y-%m-%d %H:%M:%S") 
		# Look for any dates that happened before July 2022, these must be mis-entered
		# time values from Jan 2023 that were recorded as Jan 2022
		if (min(dattemp$DateTime) < as.POSIXct('2022-07-01')){
		  adjustYearRows = which(dattemp$DateTime < as.POSIXct('2022-07-01'))
		  # Add one year's worth of seconds to the DateTime value
		  dattemp$DateTime[adjustYearRows] = dattemp$DateTime[adjustYearRows] + (60*60*24*365)
		}

		
		# Concatenate the data files. 
		if (exists('dat')){
		  dat = rbind(dat,dattemp)
		} else {
		  dat = dattemp
		}
		
		# if (f == 1){
		# 	dat = dattemp
		# } else if (f > 1){
		# 	dat = rbind(dat,dattemp)
		# }

	}
  
  		# Add in a column with the oyster code that was passed as an argument to ConcatGapeFile
		dat$Code = code 
		
		# Reorder the concatenated data frame by the DateTime values in case the files were not fed in in chronological order
	dat = dat[order(dat$DateTime),]
		# Excise time points when this SN was not associated with the current Code(oyster),
		# using a start and end time passed to the function as arguments
	if (!is.null(endDate) & !is.na(endDate)){
	  dat = dat[dat$DateTime>=startDate & dat$DateTime<=endDate, ]
	} else if (is.null(endDate) | is.na(endDate)){
	  # If no endDate was supplied, just trim data before start Date
	  dat = dat[dat$DateTime>=startDate, ]
	}
		
			
	if(verbose) close(pb)
  dat  # return this data frame
}

##############################################################
#' Excise questionable rows of data from Hall effect sensor data
#' 
#' New idea: take a halldata data frame, which may have a few disjointed periods
#' of sampling data (from different serial numbers), and map those data onto
#' a single continuous time series data frame. Then go through the low tide metadata
#' file and identify all the rows that apply to this particular oyster (based on 
#' its Code value, which gives site and treatment info), and then use the low tide
#' servicing start and end times to further NA any rows in the giant time series that
#' were during low tide service periods. At the end of that, then take the giant
#' time series and interpolate any small gaps in data, and return the big time
#' series as a data frame that will later get saved to a csv file. 
#' 
#' @param halldata A dataframe containing timestamps and hall effect data from
#' multiple sensors.
#' @param metadf A data frame of start and end times to use in the data set,
#' formatted as POSIXct time stamps in the same time zone as values in the 
#' timestamps argument. The data frame should also have a column with an OysterID
#' and a hall effect sensor channel for that oyster for each time period laid
#' out in the data frame.
#' @param maxGapFill The maximum number of missing values in the input time 
#' series to impute via linear interpolation. Gaps longer than this will remain 
#' as NA values. 
#' @param maintenance A data frame of start and end times of maintenance of 
#' battery changes in the field. Start time=start of maintenance. End time=
#' end of maintenance. Separated by site and treatment. 
#' @param timeofdeath A timestamp of approximate death of oyster in field. Column
#' within metadf. 
#' @return A data frame with columns
#' added on for each oyster with available hall effect sensor data. Missing 
#' periods of hall effect values will be denoted with NA values.  

#halldata = gapedat
#metadf = metafile
exciseHall = function(halldata, metadf, maintenance, timeofdeath, maxGapFill = 12) {
	# Get the list of unique OysterID values from excised
	# oysters = halldata$SN[1]
	#oysters = oysters[order(oysters)] # reorder if needed 
	
	# Generate output data frame based on initial deployment time and final 
	# raw data time
	time1 = min(metadf$StartIncludeUTC, na.rm=T)
	time2 = max(halldata$DateTime, na.rm=T)
	totaltimeseries = seq(time1,time2,by=60) #every 60 seconds
	# Create vector to hold hall effect data for this oyster
	temphall = matrix(data = NA, nrow = length(totaltimeseries), ncol = 5)
	colnames(temphall) = c('Hall', 'Temp.C','Battery.V','SN', 'Code')
	# Convert outputdf to a zoo object for easier time handling
	tempoutput = zoo(temphall,order.by = totaltimeseries)
	
	# Remove any possible duplicate timestamps, usually caused by the sensor
	# briefly resetting itself (losing power) within the same minute
	dupeRows = (which(diff(halldata$POSIXt) < 1) + 1)
	if (length(dupeRows)!=0){
	  halldata = halldata[-dupeRows, ]
	}
	
	
	# Convert the input halldata into a zoo object also
	temp = zoo(halldata[,c('Hall', 'Temp.C','Battery.V','SN', 'Code')], 
	           order.by = halldata[,"DateTime"], frequency = 2)
	
	# Insert the temp data into the zoo object tempoutput
	# The syntax is fun here: On the left side of the equals sign,
	# you want a set of indices in tempoutput where the time value
	# from tempoutput is found %in% the time values of temp. And on the
	# right side of the equals sign, you want to get hall effect values
	# from temp where the time value in temp is found %in% the time
	# values of tempoutput (a much bigger set)
	tempoutput[index(tempoutput) %in% index(temp), ] = 
			temp[index(temp) %in% index(tempoutput),]

	  #######################################################
	# At this point tempoutput should be a long zoo object with timestamnps
	# running from the date of the first deployment to the last available date
	# of this particular oyster, with NA's in rows where hall/temperature data
	# weren't available
		#######################################################
	# Go to the low tide servicing metadata file and get the relevant
	# start and end times for low tide service periods for the site/treatment
	# that this oyster is at (contained in the Code column)
	
	sitecode = substr(halldata$Code[1],1,1)
	treatmentcode=substr(halldata$Code[1],3,3)
	# Go to low tide service metafile and find all rows that match
	# this sitecode (and treatment code) and get the list of those row numbers
	# Then cycle through those row numbers, in each case grabbing the start time
	# and end time of the low tide servicing. Then go to tempoutput and find
	# any rows that are > start time and < end time, and convert NAs. 
	services = which( (maintenance$sitecode == sitecode) & (maintenance$treatmentcode == treatmentcode) )
	for (k in (services) ){
	  # Find the matching time stamps and write NAs in between them
	  startTime = maintenance$StartMaintUTC[k]
	  endTime = maintenance$EndMaintUTC[k]
	  # This may not work with tempoutput in the zoo format, but the point is to 
	  # NA those lines that are within the startTime and endTime boundaries
	  tempoutput[which((index(tempoutput) > startTime) & (index(tempoutput) < endTime)),] = NA
	  #tempoutput[which((index(temp) > startTime) & (index(temp) < endTime)),] = NA
	}
	
	
		# Convert tempoutput back to a data frame and stick it onto an output
		# dataframe
	outputdf = data.frame(DateTime = index(tempoutput), tempoutput)
	#outputdf = data.frame(DateTime = index(temp), temp)
	  # Remove hall data for after an oyster has died

	if (!is.null(timeofdeath)){
	  if(!is.na(timeofdeath)){
	    outputdf = outputdf[outputdf$DateTime<timeofdeath, ]
	  }
	}

  outputdf$Hall = as.numeric(outputdf$Hall)
  outputdf$Temp.C = as.numeric(outputdf$Temp.C)
  outputdf$Battery.V = as.numeric(outputdf$Battery.V)


	outputdf  # return outputdf	
	
} # End of exciseHall function
```

#Functions: loadComboGape, plot2HallFunction, plotRawHallfunction
```{r GapePlotCheckingFunctions}

# Function to load a particular sensor's Hall effect data. This assumes that a path already exists called combogapefile
loadComboGape = function(oy) {
	gape = read.csv(paste0(combogapefile,oy,'_combogape.csv'))
	gape$DateTime = as.POSIXct(gape$DateTime, tz = 'UTC', format="%Y-%m-%d %H:%M:%S")
	gape
}
# install.packages("tidyquant")
# library(tidyquant)
# ggplot(gape, aes(DateTime, Hall))+
#   geom_line()+
#   scale_x_datetime(limits = c(as.POSIXct("2022-07-25 03:30:00"),
#                               as.POSIXct("2022-07-25 06:00:00")))

# Function to make a quick 2-panel plot of the raw Hall sensor output and the calculate gape percentage
#plot line that shows every 5 day subsetted chunk. pick first point and draw line for 1440*5 points (or plus 5 days) and draw vertical line. abline(v="xvaluelocation")

#move 1440 rows down, figure out what time value is and feed into abline 
plot2HallFunction = function(gape){
	par(mfrow = c(2,1), mar = c(4,6,2,1))
	plot(gape$DateTime, gape$Hall, type = 'l', col = 3, 
			ylab = 'Raw Hall counts', 
			xlab = 'Date',
			las = 1,
			main = gape$SN[1])
	    startRow = min(which(!is.na(gape$Hall)))
	    abline(v = gape$DateTime[startRow + (1440*5)])
	     abline(v = gape$DateTime[startRow + (1440*15)])
	     abline(v = gape$DateTime[startRow + (1440*10)])
	    abline(v = gape$DateTime[startRow + (1440*20)])
	    abline(v = gape$DateTime[startRow + (1440*25)])
	if(!all(is.na(gape[,'hallpercent']))){
  	plot(gape$DateTime, gape$hallpercent, type = 'l', col = 2,
  			ylab = 'Gape opening %',
  			xlab = 'Date',
  			las = 1,
  			main = gape$SN[1])
	    abline(v = gape$DateTime[startRow + (1440*5)])
	     abline(v = gape$DateTime[startRow + (1440*15)])
	     abline(v = gape$DateTime[startRow + (1440*10)])
	    abline(v = gape$DateTime[startRow + (1440*20)])
	    abline(v = gape$DateTime[startRow + (1440*25)])
  	}
	par(mfrow=c(1,1))
}

# Function to only plot the raw Hall effect counts
plotRawHallFunction = function(gape){
	par(mfrow = c(1,1), mar = c(4,6,2,1))
	plot(gape$DateTime, gape$Hall, type = 'l', col = 3, 
			ylab = 'Raw Hall counts', 
			xlab = 'Date',
			las = 1,
			main = gape$SN[1])

}

```

#loessfunc function
```{r}
loessFunc = function(x,y, col = 1, lwd = 2, pch = '.', cex = 1, span = 0.75, pchcex = 1){
	#points(x, y, col = col, pch = pch, cex = pchcex)
	lo = loess(y~as.numeric(x), na.action = 'na.omit', span = span)
	predy = predict(lo, newdata = as.numeric(x))
	lines(x, predy, col = col, lwd = lwd)
}
```

#import metadata
```{r}
#import csv metadata_with edits. 
metafile = loadFieldMetaData(filename = (paste0(metafilePath,"field_metadata_forcoding - Sheet1.csv")))

#import csv maintenance
maintenance=loadFieldMaintData(filename = (paste0(metafilePath,"lowtide_metadata - Sheet1.csv")))

oystercode = unique(metafile[,'Code'])

#import growth measurements
growth=read.csv(paste0(metafilePath, "growth_measurements - Sheet1.csv"))

#import allgapemeta
allgapemeta=read.csv(paste0(NewportDir,"combinedgapemeta/", "allgapemeta.csv"))
#change datetime to posixct
allgapemeta$DateTime=as.POSIXct(allgapemeta$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")

#import allhrgapemeta
allhrgapemeta=read.csv(paste0(metafilePath,"/allhrgapemeta.csv"))
allhrgapemeta$DateTime=as.POSIXct(allhrgapemeta$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")

```

#calculate proportion open/closed for each code, then average them per treatment/site
```{r}
#average by site and treatment to get average %open/closed for each oyster in each site/treatment
gapebinarypercent <- allgapemeta %>%
  group_by(Code) %>% #do calculations by oyster code
  drop_na(binary) %>% 
  group_by(Treatment,Site) %>% 
  count(binary) %>% 
  mutate(percentopen=n/sum(n)) %>% 
  filter(binary=="1")

#calculate average open/closed per code
gapebinarypercentcode <- allgapemeta %>%
  group_by(Code) %>% #do calculations by oyster code
  drop_na(binary) %>% 
  count(binary) %>% 
  mutate(percentopen=n/sum(n)) %>% 
  filter(binary=="1") %>% 
  select(-binary)

#weighted proportion within site, calculate total open and total observations for treatments within sites, same numbers as gapebinarypercent
weightedgapebinary<-gapebinarypercent %>%
  group_by(Treatment,Site) %>% 
  mutate(total_obs=sum(n)) %>% 
  filter(binary=="1") %>% 
  rename(totalopen=n) %>% 
  select(-c(percentopen,binary)) %>% 
  mutate(prop_open=totalopen/total_obs)

ggplot(gapebinarypercent, aes(Treatment, percentopen, fill = Treatment)) + # ggplot2 barplot with error bars
  geom_bar(position="dodge", stat="summary", fun="mean", fill=c("#999999", "#009E73", "chocolate4"), col="black")+
  stat_summary(geom = "errorbar", fun.data = mean_se, position = "dodge",width = 0.2, size = 0.75)+
  ylim(0,1)+
  theme_classic()+
  labs(y="Proportion of open valves")+
  theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))

ggsave(path=plots, "proportion_open_treatment_weighted.png", width=7, height=6)

ggplot(gapebinarypercent, aes(Site, percentopen))+
  #geom_bar(position="dodge", stat="summary", fun="mean", fill=c("#999999", "#009E73", "chocolate4"), col="black")+
  geom_bar(position="dodge", stat="summary", fun="mean", fill=sitePalette,col="black")+
  #check that it's standard error
  #geom_errorbar( aes(x=name, ymin=value-sd, ymax=value+sd), width=0.4, colour="orange", alpha=0.9, size=1.3)
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  ylim(0,1)+
  theme_classic()+
  labs(y="Proportion of open valves")+
  theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))

ggsave(path=plots, "proportion_open_site.png", width=7, height=6)

```

#find water temps
```{r}
#combine tide height and temp.c 
hightidetemp=merge(allhrgapemeta,tides, by="DateTime")
hightidetemp$DateTime=as.POSIXct(hightidetemp$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")

hightidetemp=hightidetemp %>% 
  slice(-c(1622, 1636, 1722, 1750, 1764, 1772, 1776, 1788))%>% #remove outliers 
  subset(TideHT.m> 0.5) #only use rows where tide height is >0

#add season column
seasontemp=hightidetemp %>%
  dplyr::select(-c("finalBPM","hallpercent","binary","stationId","datum","TimeUTC")) %>% 
  mutate(Season = case_when( #new variable
      DateTime >= "2022-07-15 00:00:00" & DateTime <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
      DateTime >="2022-10-01 00:00:00" & DateTime <= "2022-12-31 23:59:00" ~ "Fall",
      DateTime >= "2023-01-01 00:00:00" & DateTime <= "2023-03-31 23:59:00" ~ "Winter",
      DateTime >= "2023-04-01 00:00:00" & DateTime <= "2023-06-19 23:59:00" ~ "Spring",
      TRUE ~ NA)) %>% 
  filter(!is.na(Season)) %>% #display error if a value is not assigned to one of the previous groups
  mutate_if(is.character, as.factor) %>% 
  mutate(Season = fct_relevel(Season,c("Summer", "Fall", "Winter", "Spring")),
         Treatment= fct_relevel(Treatment,c("Oyster","Eelgrass", "Mud"))) %>% 
  distinct()

dailytempseason=seasontemp %>% 
  subset(Temp.C>0) %>% #remove the bad data
  dplyr::select(c(-Code,TideHT.m)) %>% 
  group_by(day=lubridate::floor_date(DateTime, "day"),Season) %>%
  dplyr::select(-DateTime) %>% 
  summarise(avgtemp=mean(Temp.C, na.rm=TRUE))

dailytempseason %>% 
  group_by(Season) %>% 
  summarise(avgtemp=mean(avgtemp))
  
ggplot(dailytempseason,aes(Season, avgtemp,fill=Season))+
  geom_boxplot(col="black")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  stat_compare_means(comparisons = my_comparisons_season,label.y = c(24.5, 26, 27.5,29,30.5,32),aes(label = after_stat(p.signif)))+
  theme_classicmodify()+
  stat_n_text(size=4,y.pos=1.27) +
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")+
  labs(y="Average daily water temperature (C)")+
  ylim(0,33)

ggsave("avgwatertemp_season.png",path=plots, width=6, height=6)

# Compute the analysis of variance
res.aov <- aov(avgtemp ~ Season, data=dailytempseason)
# Summary of the analysis
summary(res.aov)
#pvalue=<2e-16

#test for homogeneity
leveneTest(avgtemp ~ Season, data=dailytempseason)
#not equal variances p=<2e-16
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
#pvalue=6.145e-06 so not normal

kruskal.test(avgtemp ~ Season, data=dailytempseason)
#pvalue=<2.2e-16

#dunn test pairwise for KW
dunnTest(avgtemp ~ Season, data=dailytempseason, method="bonferroni")
#says all sig diff from each other

##########
#SITE
dailytempsite=seasontemp %>% 
  subset(Temp.C>0) %>% #remove the bad data
  select(c(-Code,TideHT.m)) %>% 
  group_by(day=lubridate::floor_date(DateTime, "day"),Site) %>%
  select(-DateTime) %>% 
  summarise(avgtemp=mean(Temp.C, na.rm=TRUE))

ggplot(dailytempsite,aes(Site, avgtemp,fill=Site))+
  geom_boxplot(col="black")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
 # stat_compare_means(comparisons = my_comparisons_site,label.y = c(26, 27.5, 29,30.5,32),aes(label = after_stat(p.signif)))+ 
  theme_classicmodify()+
  scale_fill_manual(values=sitePalette)+
  theme(legend.position="none")+
  stat_n_text(size=4,y.pos=1.27) +
  labs(y="Average daily water temperature (C)")+
  ylim(0,33)

ggsave("avgwatertemp_site.png",path=plots, width=6, height=6)

# Compute the analysis of variance
res.aov <- aov(avgtemp ~ Site, data=dailytempsite)
# Summary of the analysis
summary(res.aov)
#pvalue=0.207

#tukey pairwise
TukeyHSD(res.aov)
#non sig

#or pairwise
pairwise.t.test(dailytempsite$avgtemp,dailytempsite$Site,
                 p.adjust.method = "BH")
#non sig

#test for homogeneity
leveneTest(avgtemp ~ Site, data=dailytempsite)
#equal variances 0.5653
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
#pvalue=<2.2e-16 so not normal

kruskal.test(avgtemp ~ Site, data=dailytempsite)
#pvalue=0.05703

##########
#TREATMENT

dailytemptreat=seasontemp %>% 
  subset(Temp.C>0) %>% #remove the bad data
  dplyr::select(c(-Code,TideHT.m)) %>% 
  group_by(day=lubridate::floor_date(DateTime, "day"),Treatment) %>%
  dplyr::select(-DateTime) %>% 
  summarise(avgtemp=mean(Temp.C, na.rm=TRUE))

ggplot(dailytemptreat,aes(Treatment, avgtemp,fill=Treatment))+
  geom_boxplot(col="black")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
 # stat_compare_means(comparisons = my_comparisons_treat2,label.y = c(25, 26.7, 29),aes(label = after_stat(p.signif)))+
  theme_classicmodify()+
  scale_fill_manual(values=treatment_palette)+
  theme(legend.position="none")+
  stat_n_text(size=4,y.pos=1.27) +
  labs(y="Average daily water temperature (C)")+
  ylim(0,33)

ggsave("avgwatertemp_treat.png",path=plots, width=6, height=6)

# Compute the analysis of variance
res.aov <- aov(avgtemp ~ Treatment, data=dailytemptreat)
# Summary of the analysis
summary(res.aov)
#pvalue=<2e-16

#test for homogeneity
leveneTest(avgtemp ~ Treatment, data=dailytemptreat)
#not equal variances p=<2e-16
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
#pvalue=<2.2e-16 so not normal

kruskal.test(avgtemp ~ Treatment, data=dailytemptreat)
#pvalue=0.054

```

#find lowtide temps
```{r}
#combine tide height and temp.c 
lowtidetemp=merge(allhrgapemeta,tides, by="DateTime")

lowtidetemp=lowtidetemp %>% 
  subset(TideHT.m< -0.1) 
  #select(-c("stationId","datum","TimeUTC"))

lowseasontemp=lowtidetemp %>%
  select(-c("finalBPM","hallpercent","binary","stationId","datum","TimeUTC")) %>% 
  mutate(Season = case_when( #new variable
      DateTime >= "2022-07-15 00:00:00" & DateTime <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
      DateTime >="2022-10-01 00:00:00" & DateTime <= "2022-12-31 23:59:00" ~ "Fall",
      DateTime >= "2023-01-01 00:00:00" & DateTime <= "2023-03-31 23:59:00" ~ "Winter",
      DateTime >= "2023-04-01 00:00:00" & DateTime <= "2023-06-19 23:59:00" ~ "Spring",
      TRUE ~ NA)) %>% 
  filter(!is.na(Season)) %>% #display error if a value is not assigned to one of the previous groups
  mutate_if(is.character, as.factor) %>% 
  mutate(Season = fct_relevel(Season,c("Summer","Fall", "Winter", "Spring")),
         Treatment= fct_relevel(Treatment,c("Oyster","Eelgrass", "Mud"))) %>% 
  distinct()



lowdailytempseason=lowseasontemp %>% 
  subset(Temp.C>0) %>% #remove the bad data
  select(c(-Code,TideHT.m)) %>% 
  group_by(day=lubridate::floor_date(DateTime, "day"),Season) %>%
  select(-DateTime) %>% 
  summarise(avgtemp=mean(Temp.C, na.rm=TRUE))
  
ggplot(lowdailytempseason,aes(Season, avgtemp,fill=Season))+
  geom_boxplot(col="black")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  stat_compare_means(comparisons = my_comparisons_season,label.y = c(24, 25.5, 27,28.5,30,32),aes(label = after_stat(p.signif)))+
  theme_classicmodify()+
  stat_n_text(size=4,y.pos=1.27) +
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")+
  labs(y="Average daily low tide oyster temp (C)")+
  ylim(0,33)

ggsave("lowtide_avgwatertemp_season.png",path=plots, width=6, height=6)

# Compute the analysis of variance
res.aov <- aov(avgtemp ~ Season, data=lowdailytempseason)
# Summary of the analysis
summary(res.aov)
#pvalue=<2e-16

#tukey pairwise
TukeyHSD(res.aov)
#All other comparisons sig except fall and spring

#or pairwise
pairwise.t.test(lowdailytempseason$avgtemp,lowdailytempseason$Season,
                 p.adjust.method = "BH")
#All other comparisons sig except fall and spring

#test for homogeneity
leveneTest(avgtemp ~ Season, data=lowdailytempseason)
#equal variances p=0.1322
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
#pvalue=0.2202 so normal

##########
#SITE
lowdailytempsite=lowseasontemp %>% 
  subset(Temp.C>0) %>% #remove the bad data
  select(c(-Code,TideHT.m)) %>% 
  group_by(day=lubridate::floor_date(DateTime, "day"),Site) %>%
  select(-DateTime) %>% 
  summarise(avgtemp=mean(Temp.C, na.rm=TRUE))

ggplot(lowdailytempsite,aes(Site, avgtemp,fill=Site))+
  geom_boxplot(col="black")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
 # stat_compare_means(comparisons = my_comparisons_site,label.y = c(26, 27.5, 29,30.5,32),aes(label = after_stat(p.signif)))+
  theme_classicmodify()+
  scale_fill_manual(values=sitePalette)+
  theme(legend.position="none")+
  stat_n_text(size=4,y.pos=1.27) +
  labs(y="Average daily low tide oyster temp (C)")+
  ylim(0,33)

ggsave("lowavgwatertemp_site.png",path=plots, width=6, height=6)

# Compute the analysis of variance
res.aov <- aov(avgtemp ~ Site, data=lowdailytempsite)
# Summary of the analysis
summary(res.aov)
#pvalue=0.708

#tukey pairwise
TukeyHSD(res.aov)
#non sig

#or pairwise
pairwise.t.test(lowdailytempsite$avgtemp,lowdailytempsite$Site,
                 p.adjust.method = "BH")
#non sig

#test for homogeneity
leveneTest(avgtemp ~ Site, data=lowdailytempsite)
#equal variances 0.2914
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
#pvalue=0.00044 so not normal

kruskal.test(avgtemp ~ Site, data=lowdailytempsite)
#pvalue=0.85333

##########
#TREATMENT

lowdailytemptreat=lowseasontemp %>% 
  subset(Temp.C>0) %>% #remove the bad data
  select(-c("Code","TideHT.m")) %>% 
  group_by(day=lubridate::floor_date(DateTime, "day"),Treatment) %>%
  select(-DateTime) %>% 
  summarise(avgtemp=mean(Temp.C, na.rm=TRUE))


ggplot(lowdailytemptreat,aes(Treatment, avgtemp,fill=Treatment))+
  geom_boxplot(col="black")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  #stat_compare_means(comparisons = my_comparisons_treat2,label.y = c(25, 26.7, 29),aes(label = after_stat(p.signif)))+
  theme_classicmodify()+
  scale_fill_manual(values=treatment_palette)+
  theme(legend.position="none")+
  stat_n_text(size=4,y.pos=1.27) +
  labs(y="Average daily low tide oyster temp (C)")+
  ylim(0,33)

ggsave("lowavgwatertemp_treat.png",path=plots, width=6, height=6)

# Compute the analysis of variance
res.aov <- aov(avgtemp ~ Treatment, data=lowdailytemptreat)
# Summary of the analysis
summary(res.aov)
#pvalue=0.45

#tukey pairwise
TukeyHSD(res.aov)
#All other comparisons sig

#or pairwise
pairwise.t.test(lowdailytemptreat$avgtemp,lowdailytemptreat$Treatment,
                 p.adjust.method = "BH")
#winter sig diff from fall,spring, and summer
#All other comparisons not sig

#test for homogeneity
leveneTest(avgtemp ~ Treatment, data=lowdailytemptreat)
#not equal variances p=0.01448
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
#pvalue=0.00015 so not normal

kruskal.test(avgtemp ~ Treatment, data=lowdailytemptreat)
#pvalue=0.7815

#######
#Treatment by Season
lowdailytempseasontreat=lowseasontemp %>% 
  subset(Temp.C>0) %>% #remove the bad data
  select(-c("Code","TideHT.m","Site")) %>% 
  group_by(day=lubridate::floor_date(DateTime, "day"),Season,Treatment) %>%
  select(-DateTime) %>% 
  summarise(avgtemp=mean(Temp.C, na.rm=TRUE)) %>% 
  distinct()

ggplot(lowdailytempseasontreat,aes(Season, avgtemp,fill=Treatment))+
  geom_boxplot(col="black")+
  #stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  #stat_compare_means(comparisons = my_comparisons_treat2,label.y = c(25, 26.7, 29),aes(label = after_stat(p.signif)))+
  theme_classicmodify()+
  scale_fill_manual(values=treatment_palette)+
  theme(legend.position="none")+
  stat_n_text(size=4,y.pos=1.27) +
  labs(y="Average daily low tide oyster temp (C)")+
  ylim(0,33)

ggsave("lowavgwatertemp_treat_season.png",path=plots, width=6, height=6)

```

#find oyster temps
```{r}
#combine tide height and temp.c 
hightidetemp=merge(allhrgapemeta,tides, by="DateTime")
hightidetemp$DateTime=as.POSIXct(hightidetemp$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")

oystertemp=hightidetemp %>% 
  slice(-c(1622, 1636, 1722, 1750, 1764, 1772, 1776, 1788)) #remove outliers 

#add season column
seasontemp=oystertemp %>%
  dplyr::select(-c("finalBPM","hallpercent","binary","stationId","datum","TimeUTC")) %>% 
  mutate(Season = case_when( #new variable
      DateTime >= "2022-07-15 00:00:00" & DateTime <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
      DateTime >="2022-10-01 00:00:00" & DateTime <= "2022-12-31 23:59:00" ~ "Fall",
      DateTime >= "2023-01-01 00:00:00" & DateTime <= "2023-03-31 23:59:00" ~ "Winter",
      DateTime >= "2023-04-01 00:00:00" & DateTime <= "2023-06-19 23:59:00" ~ "Spring",
      TRUE ~ NA)) %>% 
  filter(!is.na(Season)) %>% #display error if a value is not assigned to one of the previous groups
  mutate_if(is.character, as.factor) %>% 
  mutate(Season = fct_relevel(Season,c("Summer", "Fall", "Winter", "Spring")),
         Treatment= fct_relevel(Treatment,c("Oyster","Eelgrass", "Mud"))) %>% 
  distinct()

dailytempseason=seasontemp %>% 
  subset(Temp.C>0) %>% #remove the bad data
  dplyr::select(c(-Code,TideHT.m)) %>% 
  group_by(day=lubridate::floor_date(DateTime, "day"),Season) %>%
  dplyr::select(-DateTime) %>% 
  summarise(avgtemp=mean(Temp.C, na.rm=TRUE))

dailytempseason %>% 
  group_by(Season) %>% 
  summarise(avgtemp=mean(avgtemp))
  
ggplot(dailytempseason,aes(Season, avgtemp,fill=Season))+
  geom_boxplot(col="black")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  stat_compare_means(comparisons = my_comparisons_season,label.y = c(24.5, 26, 27.5,29,30.5,32),aes(label = after_stat(p.signif)))+
  theme_classicmodify()+
  stat_n_text(size=4,y.pos=1.27) +
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")+
  labs(y="Average daily water temperature (C)")+
  ylim(0,33)

ggsave("avgwatertemp_season.png",path=plots, width=6, height=6)

# Compute the analysis of variance
res.aov <- aov(avgtemp ~ Season, data=dailytempseason)
# Summary of the analysis
summary(res.aov)
#pvalue=<2e-16

#test for homogeneity
leveneTest(avgtemp ~ Season, data=dailytempseason)
#not equal variances p=<2e-16
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
#pvalue=6.145e-06 so not normal

kruskal.test(avgtemp ~ Season, data=dailytempseason)
#pvalue=<2.2e-16

#dunn test pairwise for KW
dunnTest(avgtemp ~ Season, data=dailytempseason, method="bonferroni")
#says all sig diff from each other

##########
#SITE
dailytempsite=seasontemp %>% 
  subset(Temp.C>0) %>% #remove the bad data
  select(c(-Code,TideHT.m)) %>% 
  group_by(day=lubridate::floor_date(DateTime, "day"),Site) %>%
  select(-DateTime) %>% 
  summarise(avgtemp=mean(Temp.C, na.rm=TRUE))

ggplot(dailytempsite,aes(Site, avgtemp,fill=Site))+
  geom_boxplot(col="black")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
 # stat_compare_means(comparisons = my_comparisons_site,label.y = c(26, 27.5, 29,30.5,32),aes(label = after_stat(p.signif)))+ 
  theme_classicmodify()+
  scale_fill_manual(values=sitePalette)+
  theme(legend.position="none")+
  stat_n_text(size=4,y.pos=1.27) +
  labs(y="Average daily water temperature (C)")+
  ylim(0,33)

ggsave("avgwatertemp_site.png",path=plots, width=6, height=6)

# Compute the analysis of variance
res.aov <- aov(avgtemp ~ Site, data=dailytempsite)
# Summary of the analysis
summary(res.aov)
#pvalue=0.207

#test for homogeneity
leveneTest(avgtemp ~ Site, data=dailytempsite)
#equal variances 0.5653
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
#pvalue=<2.2e-16 so not normal

kruskal.test(avgtemp ~ Site, data=dailytempsite)
#pvalue=0.09288

##########
#TREATMENT

dailytemptreat=seasontemp %>% 
  subset(Temp.C>0) %>% #remove the bad data
  dplyr::select(c(-Code,TideHT.m)) %>% 
  group_by(day=lubridate::floor_date(DateTime, "day"),Treatment) %>%
  dplyr::select(-DateTime) %>% 
  summarise(avgtemp=mean(Temp.C, na.rm=TRUE))

ggplot(dailytemptreat,aes(Treatment, avgtemp,fill=Treatment))+
  geom_boxplot(col="black")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
 # stat_compare_means(comparisons = my_comparisons_treat2,label.y = c(25, 26.7, 29),aes(label = after_stat(p.signif)))+
  theme_classicmodify()+
  scale_fill_manual(values=treatment_palette)+
  theme(legend.position="none")+
  stat_n_text(size=4,y.pos=1.27) +
  labs(y="Average daily water temperature (C)")+
  ylim(0,33)

ggsave("avgwatertemp_treat.png",path=plots, width=6, height=6)

# Compute the analysis of variance
res.aov <- aov(avgtemp ~ Treatment, data=dailytemptreat)
# Summary of the analysis
summary(res.aov)
#pvalue=<2e-16

#test for homogeneity
leveneTest(avgtemp ~ Treatment, data=dailytemptreat)
#not equal variances p=<2e-16
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
#pvalue=<2.2e-16 so not normal

kruskal.test(avgtemp ~ Treatment, data=dailytemptreat)
#pvalue=0.054

```

#multipanel of gape, precip, temp, and tide height 
```{r}
# full experiment
mytimestart = as.POSIXct('2022-07-15 00:00:00', tz = 'UTC')
mytimeend = as.POSIXct('2023-06-19 00:00:00', tz = 'UTC')

#wet season
mytimestart = as.POSIXct('2022-11-06 00:00:00', tz = 'UTC')
mytimeend = as.POSIXct('2022-11-12 23:59:00', tz = 'UTC')

#november week 
mytimestart = as.POSIXct('2023-01-01 00:00:00', tz = 'UTC')
mytimeend = as.POSIXct('2023-01-07 00:00:00', tz = 'UTC')

##Precip data
#only pull out 00:53:00 numbers from precip. Precip every hour
precip53 <- subset(precip, format(precip$DateTimeUTC, "%M") %in% c('53'))
precip53$DateTime=precip53$DateTimeUTC
precip53sub=subset(precip53, select=-c(StationID, DateTimeUTC, RH))
#get culm rainfall per day
precip53sub$Precip1hr.mm[is.na(precip53sub$Precip1hr.mm)] <- 0

#oyster/temp data
# oy="W.E.3.1010"
# hall=loadComboGape(oy)
# plot(hall$DateTime, hall$hallpercent)
#subset 23/01/20-23/07/01

#tides
tidessub= subset(tides, select = -c(TimeUTC,datum, stationId))
# 
#put all data frames into list
# multidata <- list(hall, precip53sub, tidessub)
# #merge all data frames in list
# multidata=multidata %>% reduce(full_join, by='DateTime')
# 
# multisub=multidata[multidata$DateTime >= "2023-01-20" & multidata$DateTime <= "2023-06-15", ]

#oy lists
##################### 
#lists 
#focusing on just Jan-April 
#pull out just a few oysters from dataframe
alllist=c("D.E.1.1032", "D.E.2.1028", "D.E.3.1029", "D.E.4.1033", "D.E.5.1031", "P.E.1.1073", "P.E.1.1078", "P.E.2.1074", "P.E.3.1075", "P.E.3.1079", "P.E.4.1077", "P.E.5.1076", "W.E.3.1007", "W.E.3.1010", "W.E.5.1002", "W.E.5.1009", "W.E.6.1004", "W.E.6.1005","D.O.1.1041", "D.O.1.1100", "D.O.2.1037","D.O.3.1044","D.O.5.1038", "P.O.2.1060","P.O.3.1059","P.O.3.1060","P.O.4.1062","P.O.5.1065","S.O.3.1082","S.O.4.1083","W.O.1.1096","W.O.2.1011","W.O.2.1012","W.O.2.1018","W.O.3.1013","W.O.4.1015","W.O.5.1016","D.M.1.1050", "D.M.1.1091", "D.M.1.1102","D.M.2.1047","D.M.2.1052", "D.M.2.1053","D.M.3.1045","D.M.4.1049","D.M.5.1047","D.M.6.1054","P.M.1.1071","P.M.2.1070","P.M.2.1072","S.M.3.1088","W.M.1.1026","W.M.1.1027","W.M.2.1019","W.M.2.1020","W.M.3.1021","W.M.3.1022","W.M.4.1023")

alllisttest=c("D.E.1.1032", "D.E.2.1028", "D.E.3.1029", "D.E.4.1033", "D.E.5.1031", "P.E.1.1073", "P.E.1.1078", "P.E.2.1074", "P.E.3.1075", "P.E.3.1079", "P.E.4.1077", "P.E.5.1076", "W.E.3.1007", "W.E.3.1010", "W.E.5.1002", "W.E.5.1009",  "W.E.6.1005","D.O.1.1041", "D.O.1.1100", "D.O.2.1037","D.O.3.1044","D.O.5.1038", "P.O.2.1060","P.O.3.1059","P.O.3.1060","P.O.4.1062","S.O.3.1082","S.O.4.1083","W.O.1.1096","W.O.2.1011","W.O.2.1012","W.O.2.1018","W.O.3.1013","W.O.4.1015","W.O.5.1016","D.M.1.1050", "D.M.1.1091", "D.M.1.1102","D.M.2.1047","D.M.2.1052", "D.M.2.1053","D.M.3.1045","D.M.4.1049","D.M.5.1047","D.M.6.1054","P.M.1.1071","P.M.2.1070","P.M.2.1072","S.M.3.1088","W.M.1.1026","W.M.1.1027","W.M.2.1019","W.M.2.1020","W.M.3.1021","W.M.3.1022","W.M.4.1023")
#EELGRASS TREATMENT ######
eelgrasslist=c("D.E.1.1032", "D.E.2.1028", "D.E.3.1029", "D.E.4.1033", "D.E.5.1031", "P.E.1.1073", "P.E.1.1078", "P.E.2.1074", "P.E.3.1075", "P.E.3.1079", "P.E.4.1077", "P.E.5.1076", "W.E.3.1007", "W.E.3.1010", "W.E.5.1002", "W.E.5.1009", "W.E.6.1004", "W.E.6.1005")

#OYSTER TREATMENT ######
oytreatlist=c("D.O.1.1041", "D.O.1.1100", "D.O.2.1037","D.O.3.1044","D.O.5.1038", "P.O.2.1060","P.O.3.1059","P.O.3.1060","P.O.4.1062","P.O.5.1065","S.O.3.1082","S.O.4.1083","W.O.1.1096","W.O.2.1011","W.O.2.1012","W.O.2.1018","W.O.3.1013","W.O.4.1015","W.O.5.1016")

#MUD TREATMENT ######
mudtreatlist=c("D.M.1.1050", "D.M.1.1091", "D.M.1.1102","D.M.2.1047","D.M.2.1052", "D.M.2.1053","D.M.3.1045","D.M.4.1049","D.M.5.1047","D.M.6.1054","P.M.1.1071","P.M.2.1070","P.M.2.1072","S.M.3.1088","W.M.1.1026","W.M.1.1027","W.M.2.1019","W.M.2.1020","W.M.3.1021","W.M.3.1022","W.M.4.1023")

#DeAnza list ###
dzlist=c("D.E.1.1032", "D.E.2.1028", "D.E.3.1029", "D.E.4.1033", "D.E.5.1031","D.O.1.1041", "D.O.1.1100", "D.O.2.1037","D.O.3.1044","D.O.5.1038","D.M.1.1050", "D.M.1.1091", "D.M.1.1102","D.M.2.1047","D.M.2.1052", "D.M.2.1053","D.M.3.1045","D.M.4.1049","D.M.5.1047","D.M.6.1054")

#PCH list
pchlist=c("P.E.1.1073", "P.E.1.1078", "P.E.2.1074", "P.E.3.1075", "P.E.3.1079", "P.E.4.1077", "P.E.5.1076","P.O.2.1060","P.O.3.1059","P.O.3.1060","P.O.4.1062","P.O.5.1065","P.M.1.1071","P.M.2.1070","P.M.2.1072")

#Shellmaker list
slist=c("S.O.3.1082","S.O.4.1083","S.M.3.1088")

#Westcliff list
westlist=c("W.E.3.1007", "W.E.3.1010", "W.E.5.1002", "W.E.5.1009", "W.E.6.1004", "W.E.6.1005","W.O.1.1096","W.O.2.1011","W.O.2.1012","W.O.2.1018","W.O.3.1013","W.O.4.1015","W.O.5.1016","W.M.1.1026","W.M.1.1027","W.M.2.1019","W.M.2.1020","W.M.3.1021","W.M.3.1022","W.M.4.1023")

#############

for (i in alllist){
  df=allhrgapemeta[allhrgapemeta$Code == i, c("DateTime","hallpercent","Code","finalBPM")]
  write.csv(df, paste0(gapeplot, i, "_hall.csv"),row.names=FALSE)
}

for (i in alllist){
    assign(paste0(i),  # Read and store data frames
         read.csv(paste0(gapeplot, i, "_hall.csv"),as.is=TRUE))
}
#changing Eelgrass to POSIXct
######### 

D.E.1.1032$DateTime=as.POSIXct(D.E.1.1032$DateTime, tz="UTC")
D.E.2.1028$DateTime=as.POSIXct(D.E.2.1028$DateTime, tz="UTC")
D.E.3.1029$DateTime=as.POSIXct(D.E.3.1029$DateTime, tz="UTC")
D.E.4.1033$DateTime=as.POSIXct(D.E.4.1033$DateTime, tz="UTC")
D.E.5.1031$DateTime=as.POSIXct(D.E.5.1031$DateTime, tz="UTC")
P.E.1.1073$DateTime=as.POSIXct(P.E.1.1073$DateTime, tz="UTC")
P.E.1.1078$DateTime=as.POSIXct(P.E.1.1078$DateTime, tz="UTC")
P.E.2.1074$DateTime=as.POSIXct(P.E.2.1074$DateTime, tz="UTC")
P.E.3.1075$DateTime=as.POSIXct(P.E.3.1075$DateTime, tz="UTC")
P.E.3.1079$DateTime=as.POSIXct(P.E.3.1079$DateTime, tz="UTC")
P.E.4.1077$DateTime=as.POSIXct(P.E.4.1077$DateTime, tz="UTC")
P.E.5.1076$DateTime=as.POSIXct(P.E.5.1076$DateTime, tz="UTC")
W.E.3.1007$DateTime=as.POSIXct(W.E.3.1007$DateTime, tz="UTC")
W.E.3.1010$DateTime=as.POSIXct(W.E.3.1010$DateTime, tz="UTC")
W.E.5.1002$DateTime=as.POSIXct(W.E.5.1002$DateTime, tz="UTC")
W.E.5.1009$DateTime=as.POSIXct(W.E.5.1009$DateTime, tz="UTC")
W.E.6.1004$DateTime=as.POSIXct(W.E.6.1004$DateTime, tz="UTC")
W.E.6.1005$DateTime=as.POSIXct(W.E.6.1005$DateTime, tz="UTC")

#changing oyster to POSIXct
######

D.O.1.1041$DateTime=as.POSIXct(D.O.1.1041$DateTime, tz="UTC")
D.O.1.1100$DateTime=as.POSIXct(D.O.1.1100$DateTime, tz="UTC")
D.O.2.1037$DateTime=as.POSIXct(D.O.2.1037$DateTime, tz="UTC")
D.O.3.1044$DateTime=as.POSIXct(D.O.3.1044$DateTime, tz="UTC")
D.O.5.1038$DateTime=as.POSIXct(D.O.5.1038$DateTime, tz="UTC")
P.O.2.1060$DateTime=as.POSIXct(P.O.2.1060$DateTime, tz="UTC")
P.O.3.1059$DateTime=as.POSIXct(P.O.3.1059$DateTime, tz="UTC")
P.O.3.1060$DateTime=as.POSIXct(P.O.3.1060$DateTime, tz="UTC")
P.O.4.1062$DateTime=as.POSIXct(P.O.4.1062$DateTime, tz="UTC")
P.O.5.1065$DateTime=as.POSIXct(P.O.5.1065$DateTime, tz="UTC")
S.O.3.1082$DateTime=as.POSIXct(S.O.3.1082$DateTime, tz="UTC")
S.O.4.1083$DateTime=as.POSIXct(S.O.4.1083$DateTime, tz="UTC")
W.O.1.1096$DateTime=as.POSIXct(W.O.1.1096$DateTime, tz="UTC")
W.O.2.1011$DateTime=as.POSIXct(W.O.2.1011$DateTime, tz="UTC")
W.O.2.1012$DateTime=as.POSIXct(W.O.2.1012$DateTime, tz="UTC")
W.O.2.1018$DateTime=as.POSIXct(W.O.2.1018$DateTime, tz="UTC")
W.O.3.1013$DateTime=as.POSIXct(W.O.3.1013$DateTime, tz="UTC")
W.O.4.1015$DateTime=as.POSIXct(W.O.4.1015$DateTime, tz="UTC")
W.O.5.1016$DateTime=as.POSIXct(W.O.5.1016$DateTime, tz="UTC")
#changing Mud to POSIXct
##########

D.M.1.1050$DateTime=as.POSIXct(D.M.1.1050$DateTime, tz="UTC")
D.M.1.1091$DateTime=as.POSIXct(D.M.1.1091$DateTime, tz="UTC")
D.M.1.1102$DateTime=as.POSIXct(D.M.1.1102$DateTime, tz="UTC")
D.M.2.1047$DateTime=as.POSIXct(D.M.2.1047$DateTime, tz="UTC")
D.M.2.1052$DateTime=as.POSIXct(D.M.2.1052$DateTime, tz="UTC")
D.M.2.1053$DateTime=as.POSIXct(D.M.2.1053$DateTime, tz="UTC")
D.M.3.1045$DateTime=as.POSIXct(D.M.3.1045$DateTime, tz="UTC")
D.M.4.1049$DateTime=as.POSIXct(D.M.4.1049$DateTime, tz="UTC")
D.M.5.1047$DateTime=as.POSIXct(D.M.5.1047$DateTime, tz="UTC")
D.M.6.1054$DateTime=as.POSIXct(D.M.6.1054$DateTime, tz="UTC")
P.M.1.1071$DateTime=as.POSIXct(P.M.1.1071$DateTime, tz="UTC")
P.M.2.1070$DateTime=as.POSIXct(P.M.2.1070$DateTime, tz="UTC")
P.M.2.1072$DateTime=as.POSIXct(P.M.2.1072$DateTime, tz="UTC")
S.M.3.1088$DateTime=as.POSIXct(S.M.3.1088$DateTime, tz="UTC")
W.M.1.1026$DateTime=as.POSIXct(W.M.1.1026$DateTime, tz="UTC")
W.M.1.1027$DateTime=as.POSIXct(W.M.1.1027$DateTime, tz="UTC")
W.M.2.1019$DateTime=as.POSIXct(W.M.2.1019$DateTime, tz="UTC")
W.M.2.1020$DateTime=as.POSIXct(W.M.2.1020$DateTime, tz="UTC")
W.M.3.1021$DateTime=as.POSIXct(W.M.3.1021$DateTime, tz="UTC")
W.M.3.1022$DateTime=as.POSIXct(W.M.3.1022$DateTime, tz="UTC")
W.M.4.1023$DateTime=as.POSIXct(W.M.4.1023$DateTime, tz="UTC")

##########

```

#figure label function
```{r}
fig_label <- function(text, region="figure", pos="topleft", cex=NULL, ...) {
 
  region <- match.arg(region, c("figure", "plot", "device"))
  pos <- match.arg(pos, c("topleft", "top", "topright", 
                          "left", "center", "right", 
                          "bottomleft", "bottom", "bottomright"))
 
  if(region %in% c("figure", "device")) {
    ds <- dev.size("in")
    # xy coordinates of device corners in user coordinates
    x <- grconvertX(c(0, ds[1]), from="in", to="user")
    y <- grconvertY(c(0, ds[2]), from="in", to="user")
 
    # fragment of the device we use to plot
    if(region == "figure") {
      # account for the fragment of the device that 
      # the figure is using
      fig <- par("fig")
      dx <- (x[2] - x[1])
      dy <- (y[2] - y[1])
      x <- x[1] + dx * fig[1:2]
      y <- y[1] + dy * fig[3:4]
    } 
  }
 
  # much simpler if in plotting region
  if(region == "plot") {
    u <- par("usr")
    x <- u[1:2]
    y <- u[3:4]
  }
 
  sw <- strwidth(text, cex=cex) * 60/100
  sh <- strheight(text, cex=cex) * 60/100
 
  x1 <- switch(pos,
    topleft     =x[1] + sw, 
    left        =x[1] + sw,
    bottomleft  =x[1] + sw,
    top         =(x[1] + x[2])/2,
    center      =(x[1] + x[2])/2,
    bottom      =(x[1] + x[2])/2,
    topright    =x[2] - sw,
    right       =x[2] - sw,
    bottomright =x[2] - sw)
 
  y1 <- switch(pos,
    topleft     =y[2] - sh,
    top         =y[2] - sh,
    topright    =y[2] - sh,
    left        =(y[1] + y[2])/2,
    center      =(y[1] + y[2])/2,
    right       =(y[1] + y[2])/2,
    bottomleft  =y[1] + sh,
    bottom      =y[1] + sh,
    bottomright =y[1] + sh)
 
  old.par <- par(xpd=NA)
  on.exit(par(old.par))
 
  text(x1, y1, text, cex=cex, ...)
  return(invisible(c(x,y)))
}
```


#graph multipanel
```{r}
#graph 
png(paste0(plots,"multiplot_november.png"), width = 8, height = 6, units = 'in', res = 300)

#Set up graphing metrics 4 panel without HR
# par(mfrow= c(4,1),mar = c(0,6,1,1.5),las = 1) #col=1, row=4 c(bottom, left, top, right)
# mat<-matrix(c(1:5),5,1, byrow=T)
# layout(mat, widths=1, heights= c(3, 3, 3, 3, 1))

#graphing metrics for 5 panel plot
par(mfrow= c(6,1),mar = c(0,6,1,1.5),las = 1) #col=1, row=5 c(bottom, left, top, right)
mat<-matrix(c(1:6),nrow=6,ncol=1, byrow=T) #c(data elements), nrow=number of rows, ncol=number of columns, byrow=fill matrix by rows 
layout(mat, widths=1, heights= c(1.25, 1.25, 1.25, 1.25, 1.25,0.5)) #widths=1 bc 1 column. 


mylwd = 2
mycex = 1.25
mygray = 'grey80'
mycex = 1.2
mypch = 16 #"." #make tiny dots #16 regular dots
myspan = 0.02
pchcex = 0.5

#library(RColorBrewer)
# cols = brewer.pal('Set1', n = 9)
# cl <- rainbow(60)
# cols = paste0(cols,'AA')
# col_vector=viridis::viridis(30)
# col_vector=brewer.pal(n =16, name = "RdBu")
#n <- 50
#qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
#col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
#make colors transparant

#color = grDevices::colors()[grep('gr(a|e)y', grDevices::colors(), invert = T)]
col_vector = c("#9e0142","#d53e4f","#f46d43","#fdae61","#fee08b","#e6f598","#abdda4","#66c2a5","#3288bd","#5e4fa2","#543005","#8c510a","#80cdc1","#35978f","#01665e","#003c30","#8e0152","#c51b7d","#de77ae","#f1b6da","#fde0ef","#b8e186","#4d9221","#9e0142","#d53e4f","#fdae61","#e6f598","#fee08b","#abdda4","#66c2a5","#ffffbf","#3288bd","#543005","#8c510a","#bf812d","#dfc27d","#276419","#f6e8c3","#c7eae5","#7fbc41","#f46d43","#5e4fa2","#80cdc1","#35978f","#003c30","#8e0152","#c51b7d","#de77ae","#f1b6da","#fde0ef","#e6f5d0","#b8e186","#7fbc41","#4d9221","#276419","#01665e")
col_vector=paste0(col_vector,"AA")

#water temp 
plot(x=hightidetemp$DateTime, y=hightidetemp$Temp.C,xaxt='n', ylab="Temperature (C)", type="n",cex.lab=1.17, cex.axis=1,xlim=c(mytimestart,mytimeend),ylim=c(0,37)) #ylim=c(8.5,22))
rect(xleft = par()$usr[1],xright = par()$usr[2], ybottom = par()$usr[3], ytop = par()$usr[4], col = mygray) #gray background
lines(precip53sub$DateTime, precip53sub$AirTempC, col="red")
colcount=1
#plot(x=hightidetemp$DateTime, y=hightidetemp$Temp.C)
#plot(allgapemeta$DateTime, allgapemeta$hallpercent)
for (c in alllist){
  if (length(which(hightidetemp$Code==c))>0){
      if (c!="P.O.2.1060"&c!="W.O.2.1012"&c!="P.O.5.1065"&c!="P.E.2.1074"&c!="W.M.2.1019"&c!="W.E.6.1005"&c!="W.M.1.1026"){
   data = subset(hightidetemp, Code == c)
  points(x = data$DateTime, y = data$Temp.C, 
	col = col_vector[colcount],cex.lab=1.25)#pch=16)
  #data$DateTime=as.numeric(data$DateTime)
  #lo <- smooth.spline(data$Temp.C~data$DateTime)
 # lines(lo, col=col_vector[colcount], lwd=2)
  #qplot(x = data$DateTime, y = data$Temp.C, geom="smooth",
	#	col = col_vector[colcount])
  #,cex=mycex, span = myspan, pch = mypch, pchcex = pchcex)
   colcount=colcount+1
      }
  }
}
fig_label("A", cex=2) 

#lines(hightidetemp$Temp.C~hightidetemp$DateTime, col="red")
#tides
plot(x=tidessub$DateTime, y=tidessub$TideHT.m,xaxt='n', ylab="Tide Height (m)",col="blue", las=2, type="l",cex.lab=1.25, cex.axis=1,xlim=c(mytimestart,mytimeend))
# mtext("Tide\n Height (m)",                     # Add title manually
#       side = 2,
#       line = 3,
#       las = 1)
rect(xleft = par()$usr[1],xright = par()$usr[2], ybottom = par()$usr[3], ytop = par()$usr[4], col = mygray) #gray background
lines(tidessub$TideHT.m~tidessub$DateTime, col="blue")
fig_label("B", cex=2) 


#precip
plot(x=precip53sub$DateTime, y=precip53sub$Precip1hr.mm, xaxt='n',ylab="Rainfall (mm)",col="orange",cex.lab=1.25, cex.axis=1, type="l",xlim=c(mytimestart,mytimeend))
rect(xleft = par()$usr[1],xright = par()$usr[2], ybottom = par()$usr[3], ytop = par()$usr[4], col = mygray) #gray background
lines(precip53sub$Precip1hr.mm~precip53sub$DateTime, col="#e57700")
fig_label("C", cex=2) 


#gape
# Make base empty plot
plot(hallpercent~DateTime,
		data = na.omit(P.O.3.1059), 
		col = 1,  
		type = 'n',  # type = 'n' means don't plot any lines/points etc.  
		las = 1, cex = mycex,
		ylim = c(0,100), 
		xlab = 'Date',
		ylab = 'Gape opening (%)',
		xaxs = 'i',
		xlim=c(mytimestart,mytimeend),
		xaxt = 'n',  # suppress the x-axix tick labels
		cex.axis = mycex,
		cex.lab = 1.05)
rect(xleft = par()$usr[1],xright = par()$usr[2], ybottom = par()$usr[3], ytop = par()$usr[4], col = mygray) #gray background
fig_label("D", cex=2) 
# Plot individual Hall sensors
colcount=1

for (filename in alllist){
  
  i <- get(filename)
  if (nrow(i)!=0){
  points(hallpercent~DateTime, data = i, col = col_vector[colcount],pch=mypch,cex=pchcex)
  if (filename!="P.O.2.1060"&filename!="W.O.2.1012"&filename!="P.O.5.1065"&filename!="P.E.2.1074"&filename!="W.M.2.1019"&filename!="W.E.6.1005"&filename!="W.M.1.1026"&filename!="W.M.2.1020"&filename!="S.M.3.1088"&filename!="W.M.3.1022"){
  #  if (nrow(i)>1500){
   loessFunc(x = i$DateTime, y = i$hallpercent, 
 		col = col_vector[colcount],cex=mycex, span = 0.02, pch = mypch, pchcex = pchcex)
    print(paste0(filename,",",colcount))
   # }
    colcount=colcount+1
  }
    
  }
}
#show_col(col_vector)

#find ylim 
# for (filename in alllist){
#   
#   i <- get(filename)
#   ymax=max(i$finalBPM,na.rm=T)
#   ymax=as.numeric(ymax)
# }

ymax=15


#make hr plot
# Make base empty plot
plot(finalBPM~DateTime,
		data = na.omit(P.O.3.1059), 
		col = 1,  
		ylim=c(2,16),
		type = 'n',  # type = 'n' means don't plot any lines/points etc.  
		las = 1, cex = mycex,
		xlab = 'Date',
		ylab = 'Heart rate (bpm)',
		xaxs = 'i',
		xlim=c(mytimestart,mytimeend),
		#xaxt = 'n',  # suppress the x-axix tick labels
		cex.axis = mycex,
		cex.lab = mycex)
rect(xleft = par()$usr[1],xright = par()$usr[2], ybottom = par()$usr[3], ytop = par()$usr[4],
		col = mygray) #gray background
fig_label("E", cex=2) 
# Plot individual hr sensors
colcount=1

for (filename in alllist){
  i <- get(filename)
  if (nrow(i)!=0){
    if(filename!="D.M.6.1054"){
  points(finalBPM~DateTime, data = i, col = col_vector[colcount],pch=mypch,cex=pchcex)
    
   if (filename!="P.O.2.1060"&filename!="W.O.2.1012"&filename!="P.O.5.1065"&filename!="P.E.2.1074"&filename!="W.M.2.1019"&filename!="W.E.6.1005"&filename!="W.M.1.1026"&filename!="W.M.4.1023"&filename!="W.M.2.1020"&filename!="S.M.3.1088"&filename!="W.M.3.1022"){
   loessFunc(x = i$DateTime, y = i$finalBPM, 
   col = col_vector[colcount],cex=mycex, span = 0.02, pch = mypch, pchcex = pchcex)
      print(paste0(filename,",",colcount))
  colcount=colcount+1
   }
    }
  }
}
# points(i$DateTime, i$finalBPM)

dev.off()
#show_col(col_vector)
```

#gape vs precip
```{r}
raingape=merge(precipsumday, allgapemeta, by="DateTime")



```


#seasons/precip
```{r}
#get culm rainfall per day
precip[is.na(precip)] <- 0

#only pull out 00:53:00 numbers from precip
precip53 <- subset(precip, format(precip$DateTimeUTC, "%M") %in% c('53'))

#average of precip per day
precipday=precip53 %>%
    group_by(day = floor_date(DateTimeUTC, unit = "day")) %>%
  summarise(day_mean = mean(Precip1hr.mm, na.rm=T),
            sd = sd(Precip1hr.mm, na.rm=T),
            n = n())

#get sum of precip per day
precipsumday=precip53 %>%
  summarise_by_time(
    .date_var=DateTimeUTC,
    .by= "day", 
    precipday = sum(Precip1hr.mm, na.rm=T))

#get culm rainfall per month
precipculm=precip53 %>%
  summarise_by_time(
    .date_var=DateTimeUTC,
    .by= "month", 
    precipday = sum(Precip1hr.mm, na.rm=T)) %>%
  filter(DateTimeUTC>"2022-06-01" & DateTimeUTC<"2023-06-01")

#get culm rainfall per week
precipculmweek=precip53 %>%
  summarise_by_time(
    .date_var=DateTimeUTC,
    .by= "week", 
    precipday = sum(Precip1hr.mm, na.rm=T)) %>%
  filter(DateTimeUTC>"2022-06-01" & DateTimeUTC<"2023-06-01")


#get culm rainfall per month
precipmonth=precip53 %>%
  summarise_by_time(
    .date_var=DateTimeUTC,
    .by= "month", 
    precipday = sum(Precip1hr.mm, na.rm=T)) 

#data for stats
precipstats=precipsumday %>% 
      mutate(Season = case_when( #new variable
      DateTimeUTC >= "2022-07-15 00:00:00" & DateTimeUTC <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
      DateTimeUTC >="2022-10-01 00:00:00" & DateTimeUTC <= "2022-12-31 23:59:00" ~ "Fall",
      DateTimeUTC >= "2023-01-01 00:00:00" & DateTimeUTC <= "2023-03-31 23:59:00" ~ "Winter",
      DateTimeUTC >= "2023-04-01 00:00:00" & DateTimeUTC <= "2023-06-19 23:59:00" ~ "Spring",
      TRUE ~ NA)) %>% 
  filter(!is.na(Season)) %>% #display error if a value is not assigned to one of the previous groups
  mutate_if(is.character, as.factor) %>% 
    mutate(Season = fct_relevel(Season,c("Summer", "Fall", "Winter", "Spring")))



precipseasonplot=ggplot(precipstats,aes(Season, precipday,fill=Season))+ 
  stat_summary(geom="bar",fun.y="sum",stat = "identity",color="black",fill=season_palette)+
  stat_n_text(size=4,y.pos=390) +
  geom_signif(comparisons = my_comparisons_season2,map_signif_level = TRUE,y_position = c(330, 355,365),annotation = c("****", "*","*"))+
  #stat_compare_means(comparisons = my_comparisons_season2,label.y = c(330, 350,360),aes(label = after_stat(p.signif)))+
  #geom_pwc(aes(group = Season), tip.length = 0.15, label = "{p.adj.signif}", p.adjust.method = "bonferroni", p.adjust.by = "panel",hide.ns = F,y.position = c(340, 350,360))+
  scale_fill_manual(values = season_palette)+
  theme_classic()+
  labs(y="Cumulative precipitation (mm)",tag = "A")+
  theme_classicmodify()+
  theme(legend.position="none")

dailywatertempplot=ggplot(tempdayseason,aes(Season, temp_day_mean,fill=Season))+
    geom_boxplot()+
  #geom_bar(stat="summary",position="dodge",fun="mean",col="black")+
  #stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75,position=position_dodge(.9))+
  theme_classicmodify()+
  theme(legend.position = "none")+
  scale_fill_manual(values=season_palette)+
  labs(y="Average daily water temperature (C)",tag = "B") +
  stat_n_text(size=4,y.pos=37) +
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
stat_compare_means(comparisons = my_comparisons_season,label.y = c(25, 27, 29,31,33,35),aes(label = after_stat(p.signif)))

ggarrange(precipseasonplot+
                   theme(
                  axis.title.y=element_text(size = 14),
                  axis.text.y=element_text(size = 11),
                  axis.text.x = element_blank(),
                  axis.ticks.x = element_blank(),
                  axis.title.x = element_blank() ), 
           dailywatertempplot+ 
            theme(axis.title.y=element_text(size = 14),
                  axis.text.y=element_text(size = 13.5),
                  #axis.text.x = element_blank(),
                 # axis.ticks.y = element_blank(),
                 # axis.title.y = element_blank() 
              ), nrow=2, ncol=1)

ggsave("precip_temp_season.png",path=plots,height=8, width=6)

# Compute the analysis of variance
res.aov <- aov(precipday ~ Season, data=precipstats)
# Summary of the analysis
summary(res.aov)
#pvalue=0.301

#tukey pairwise
TukeyHSD(res.aov)
#All comparisons not sig

#or pairwise
pairwise.t.test(precipstats$precipday,precipstats$Season,
                 p.adjust.method = "BH")
#All comparisons not sig

#test for homogeneity
leveneTest(precipday ~ Season, data=precipstats)
#not equal variances p=1.358e-05
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=<2.2e-16 so not normal

kruskal.test(precipday ~ Season, data=precipstats)
#pvalue=2.753e-05

dunnTest(precipday ~ Season, data=precipstats, method="bonferroni")

#average rainfall per month
precipaverage=precipsumday %>% 
  group_by(month = floor_date(DateTimeUTC, unit = "month")) %>%
  summarise(month_mean = mean(precipday, na.rm=T),
            sd = sd(precipday, na.rm=T),
            n = n()) %>%
  mutate(se = sd / sqrt(n))%>% 
  filter(month>"2022-06-01" & month<"2023-06-01")

#precipdat=precipdat[precipdat$precipday != 0, ]

#average
ggplot(precipaverage, aes(month, month_mean))+ 
  geom_bar(stat = "identity")+
  theme_classic()+
  labs(y="Average precipitation (mm)", x="Date")+
  scale_x_datetime(labels = date_format("%b"),date_breaks = "1 month")+
  geom_errorbar(aes(ymin = month_mean - se, ymax = month_mean + se), width = 1, stat = "identity",position=position_dodge(.9)) +
  theme(axis.text = element_text(size = 15), axis.title = element_text(size = 20))

ggsave(path=plots, "average_precip_orangecounty.png", width=6, height=6)

#cumulative
ggplot(precipmonth, aes(DateTimeUTC, precipday))+ 
  geom_bar(stat = "identity")+
  theme_classic()+
  labs(y="Cumulative precipitation (mm)", x="Date")+
  scale_x_datetime(labels = date_format("%b"),date_breaks = "1 month")+
  #geom_errorbar(aes(ymin = month_mean - se, ymax = month_mean + se), width = 1, stat = "identity",position=position_dodge(.9)) +
  theme(axis.text = element_text(size = 15), axis.title = element_text(size = 20))

ggsave(path=plots, "cumulative_precip_orangecounty.png", width=6, height=6)

#Wet season=Jan 2023-April 2023

gapewet <- allgapemeta %>%
    filter((DateTime > '2022-12-31 00:00:00') &
        (DateTime<"2023-04-01 00:00:00")) %>%
  group_by(Code) %>% #do calculations by oyster code
  drop_na(binary) %>% 
  group_by(Treatment,Site) %>% 
  count(binary) %>% 
  mutate(percentopen=n/sum(n))%>% 
  filter(binary=="1") 

gapedry <- allgapemeta %>%
  filter((DateTime>"2022-05-01 00:00:00"& DateTime<"2022-12-31 00:00:00")|(DateTime>"2023-04-01 00:00:00")) %>% 
  group_by(Code) %>% #do calculations by oyster code
  drop_na(binary) %>% 
  group_by(Treatment,Site) %>% 
  count(binary) %>% 
  mutate(percentopen=n/sum(n))%>% 
  filter(binary=="1") 

seasondata<- bind_rows(gapewet, gapedry, .id = 'Season')
seasondata=seasondata %>% 
  mutate(Season = recode(Season, '1' = 'Wet', '2' = 'Dry')) %>% 
  select(-c(binary))

ggplot(seasondata, aes(Treatment, percentopen, fill=interaction(Season, Treatment)))+
  geom_bar(position="dodge", stat="summary", fun="mean", col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75, position = position_dodge(width = .9))+
  scale_fill_manual(values = treatment2_palette)+
  ylim(0,1)+
  theme_classic()+
  labs(y="Proportion of open valves")+
  theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))

ggsave(path=plots, "gape_season.png", width=6, height=6)

#####TREATMENT###############
avgseasontreatment=seasondata %>% 
  group_by(Treatment,Season) %>% 
  mutate(ntreats = n()) %>% 
  mutate(avgpercentopen=mean(percentopen),se=sd(percentopen)/sqrt(ntreats)) 

avgseasontreatnew=avgseasontreatment %>% 
  select(-c(Site,n,percentopen)) %>% 
  group_by(Treatment,Season,ntreats) %>% 
  distinct(avgpercentopen,.keep_all = TRUE) #ignore treatment,n,percentopen column

ggplot(avgseasontreatnew, aes(Treatment, avgpercentopen,fill=interaction(Season,Treatment))) +
  geom_col_pattern(
    aes(pattern = Season),
    colour = "black",
    pattern_fill = "black",
    pattern_angle = 45,
    pattern_density = 0.05,
    pattern_spacing = 0.02,
    pattern_key_scale_factor = 0.6,
    position = position_dodge(width = .9)) +
  geom_errorbar(aes(ymin=avgpercentopen-se, ymax=avgpercentopen+se),position=position_dodge(width = .9), width=0.2,size = 0.75)+
  scale_pattern_manual(values = c(Dry="stripe", Wet="none")) +
  scale_fill_manual(values=c("#999999", "#999999","#009E73","#009E73", "chocolate4","chocolate4"))+
  guides(pattern = guide_legend(override.aes = list(fill = "white"),title="Season",size=20),
         fill = guide_legend(override.aes = list(pattern = "none")))+
  ylim(0,1)+
  theme_classic()+
  labs(y="Proportion of open valves")+
  theme(axis.text = element_text(size = 20), axis.title = element_text(size = 25),legend.title = element_text(size=25), legend.text = element_text(size=15))+
  guides(fill = "none")

ggsave(path=plots, "gape_seasontreatment.png", width=8, height=6)

#####SITE###############
avgseasonsite=seasondata %>% 
  group_by(Site,Season) %>% 
  mutate(ntreats = n()) %>% 
  mutate(avgpercentopen=mean(percentopen),se=sd(percentopen)/sqrt(ntreats)) 

avgseasonsitenew=avgseasonsite %>% 
  select(-c(Treatment,n,percentopen)) %>% 
  group_by(Site,Season,ntreats) %>% 
  distinct(avgpercentopen,.keep_all = TRUE) #ignore treatment,n,percentopen column
  
ggplot(avgseasonsite, aes(Site, avgpercentopen,fill=interaction(Season,Site))) +
  geom_col_pattern(
    aes(pattern = Season),
    colour = "black",
    pattern_fill = "black",
    pattern_angle = 45,
    pattern_density = 0.05,
    pattern_spacing = 0.02,
    pattern_key_scale_factor = 0.6,
    position = position_dodge(width = .9)) +
  geom_errorbar(aes(ymin=avgpercentopen-se, ymax=avgpercentopen+se),position=position_dodge(width = .9), width=0.2,size = 0.75)+
  scale_pattern_manual(values = c(Dry="stripe", Wet="none")) +
  scale_fill_manual(values=sitePalette2)+
  guides(pattern = guide_legend(override.aes = list(fill = "white"),title="Season",size=20),
         fill = guide_legend(override.aes = list(pattern = "none")))+
  ylim(0,1)+
  theme_classic()+
  labs(y="Proportion of open valves")+
  theme(axis.text = element_text(size = 20), axis.title = element_text(size = 25),legend.title = element_text(size=25), legend.text = element_text(size=15))+
  guides(fill = "none")

ggsave(path=plots, "gape_seasonSite.png", width=8, height=6)


##########Oyster v precip ############
oy="W.O.2.1018"
hallprecip=loadComboGape(oy)
hallprecip$DateTimeUTC=hallprecip$DateTime
hallprecip<- left_join(hallprecip, precip53, by=c("DateTimeUTC"))

max_first  <- max(hallprecip$hallpercent,na.rm=T)   # Specify max of first y axis
max_second <- max(hallprecip$Precip1hr.mm,na.rm=T) # Specify max of second y axis
min_first  <- min(hallprecip$hallpercent, na.rm=T)   # Specify min of first y axis
min_second <- min(hallprecip$Precip1hr.mm,na.rm=T) # Specify min of second y axis

# scale and shift variables calculated based on desired mins and maxes
scale = (max_second - min_second)/(max_first - min_first)
shift = min_first - min_second

ggplot(hallprecip, aes(x = DateTimeUTC, y = hallpercent)) +
  geom_line(aes(color="Gape opening %")) +
  geom_smooth(color="red")+
  geom_point(aes(y = inv_scale_function(Precip1hr.mm,scale,shift), color = "Cumulative precipitation (mm/day)")) +
  scale_y_continuous(limits = c(min_first, max_first), sec.axis = sec_axis(~scale_function(., scale, shift), name="Cumulative precipitation (mm/day)")) +
  labs(x = "Date", y = "Gape opening %", color = "", title=oy) +
  scale_color_manual(values = c("blue", "black"))+
  theme_classic()+
  scale_x_datetime(limits = as.POSIXct(strptime(c("2022-12-25 00:00:00", "2023-01-20 00:00:00"),format = "%Y-%m-%d %H:%M")))

ggsave(path=plots, paste0(oy,"_precip.png"), width=6, height=6)

coeff=10

ggplot(hallprecip, aes(x = DateTimeUTC)) +
  #graph gape opening lines
  #smooth gape line
  #geom_smooth(aes(y=hallpercent),color="red")+
  #smooth precip line
  geom_bar(aes(y=Precip1hr.mm), stat="identity", size=3, color="blue", alpha=.4) +
  geom_line(aes(y=hallpercent/coeff, color="Gape opening %"),color="black") +
  #precip points
  #geom_point(aes(y=Precip1hr.mm/coeff),color="blue") +
  scale_y_continuous(
    name="Cumulative precipitation (mm/day)", 
    sec.axis = sec_axis(~.*coeff, name="Gape opening %")) +
  labs(x = "Date", y = "Gape opening %", color = "", title=oy) +
  #scale_color_manual(values = c("blue", "black"))+
  theme_classic()+
  scale_x_datetime(limits = as.POSIXct(strptime(c("2022-12-25 00:00:00", "2023-01-20 00:00:00"),format = "%Y-%m-%d %H:%M")))

ggsave(path=plots, paste0(oy,"_precip_diff.png"), width=6, height=6)
```

#four seasons
```{r}
#NAs when datetime is 00:00:00 because DateTime column has NA for those timestamps
fourseasons=allgapemeta %>%
      mutate(Season = case_when( #new variable
      DateTime >= "2022-07-15 00:00:00" & DateTime <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
      DateTime >="2022-10-01 00:00:00" & DateTime <= "2022-12-31 23:59:00" ~ "Fall",
      DateTime >= "2023-01-01 00:00:00" & DateTime <= "2023-03-31 23:59:00" ~ "Winter",
      DateTime >= "2023-04-01 00:00:00" & DateTime <= "2023-06-19 23:59:00" ~ "Spring",
      TRUE ~ NA)) %>% 
  filter(!is.na(Season)) %>% #display error if a value is not assigned to one of the previous groups
  mutate_if(is.character, as.factor) %>% 
  mutate(Season = fct_relevel(Season,c( "Summer","Fall", "Winter", "Spring")),
         Treatment= fct_relevel(Treatment,c("Mud","Oyster", "Eelgrass"))) %>% 
  distinct()

seasonspercentopencode=fourseasons %>% 
  group_by(Treatment,Site,Season,Code) %>% 
  count(binary) %>% 
  mutate(percentopen=n/sum(n))%>% 
  filter(binary=="1") %>% 
  ungroup()

seasonsavgsite=seasonspercentopen %>% 
  group_by(Treatment,Season) %>% 
  mutate(ntreats = n()) %>% 
  mutate(avgpercentopen=mean(percentopen),se=sd(percentopen)/sqrt(ntreats)) 

seasonsavgsitecode=seasonspercentopencode %>% 
  select(-c("binary")) %>%
  group_by(Treatment,Season,Site) %>% 
  mutate(ntreats = n()) %>% 
  mutate(avgpercentopen=mean(percentopen),se=sd(percentopen)/sqrt(ntreats)) 

test=seasonsavgsitecode %>% 
  select(-c("Code","n","percentopen")) %>%
  distinct()

gapeseasonavg=test %>% 
  group_by(Season) %>% 
  summarise(gapeavg=mean(avgpercentopen))

#group by season
ggplot(test,aes(Season,avgpercentopen,fill=Season))+
  geom_boxplot()+
  stat_summary(fun=mean, geom="point", shape=23, size=5, color="black", fill="red")+
  theme_classicmodify()+
  labs(y="Percent open (%)")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")+
  ylim(0,1)

ggsave(path=plots, "gape_fourseasons_box_gapepercent_stats.png",width=6, height=6)

#avg test for stats
seasontest=test %>% 
  group_by(Season) %>% 
  ntreats=sum(ntreats)

# Compute the analysis of variance
res.aov <- aov(avgpercentopen ~ Season, data=test)
# Summary of the analysis
summary(res.aov)
#pvalue=4.3e-07

#tukey pairwise
TukeyHSD(res.aov)
#winter sig diff from fall,spring, and summer
#All other comparisons not sig

#or pairwise
pairwise.t.test(test$avgpercentopen,test$Season,
                 p.adjust.method = "BH")
#winter sig diff from fall,spring, and summer
#All other comparisons not sig

#test for homogeneity
leveneTest(avgpercentopen ~ Season, data=test)
#equal variances p=0.4135
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
#pvalue=0.1878 so normal

#plot with significance indicator and make boxplots size of sample
#group by season
ggplot(test,aes(Season,avgpercentopen,fill=Season))+
  geom_boxplot(vardwidth=T)+
  stat_summary(fun=mean, geom="point", shape=23, size=5, color="black", fill="red")+
  theme_classicmodify()+
  labs(y="Percent open (%)")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")+
  ylim(0,1)

seasonplot=ggboxplot(test, x = "Season", y = "avgpercentopen",
          color = "black", palette = c("#009E73","#D55E00","#56B4E9","#CC79A7"),fill="Season")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  labs(y="Proportion of open valves")+
  geom_signif(comparisons = my_comparisons,map_signif_level = TRUE,y_position = c(0.97, 1.05, 1.10),annotation = c("****", "***","****"))+
  stat_n_text(size=4,y.pos=1.27) + 
  ylim(0,1.3)+
  theme(axis.text=element_text(size=15),
        axis.title=element_text(size=18),legend.text=element_text(size=12),legend.title=element_text(size=15))

ggsave("percentopen_season_stats.png",path=plots,width=6, height=5)

#####################
#SITE
# Compute the analysis of variance
res.aov <- aov(avgpercentopen ~ Site, data=test)
# Summary of the analysis
summary(res.aov)
#pvalue=0.981

#tukey pairwise
TukeyHSD(res.aov)
#All comparisons not sig

#or pairwise
pairwise.t.test(test$avgpercentopen,test$Site,
                 p.adjust.method = "BH")
#All comparisons not sig

#test for homogeneity
leveneTest(avgpercentopen ~ Site, data=test)
#equal variances p=0.9023
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.007843 so not normal

kruskal.test(avgpercentopen ~ Site, data=test)
#pvalue=0.9882

siteplot=ggboxplot(test, x = "Site", y = "avgpercentopen",
          color = "black", palette = sitePalette,fill="Site")+
  stat_summary(fun=mean, geom="point",shape=23, size=4, color="black", fill="red")+
  labs(y="Proportion of open valves")+
  #compare_means(avgpercentopen ~ Site,  data = test)+
  #stat_compare_means(method = "anova")+
  #stat_compare_means(label = "p.signif", method = "t.test",
  #                   ref.group = "Winter") +
  #stat_compare_means(comparisons = my_comparisons_site,label.y = c(1, 1.05, 1.13),aes(label = after_stat(p.signif)))+
  stat_n_text(size=4,y.pos=1.27) + 
  ylim(0,1.3)+
  theme(axis.text=element_text(size=15),
        axis.title=element_text(size=18),legend.text=element_text(size=12),legend.title=element_text(size=15))

ggsave("percentopen_season_stats_site.png",path=plots,width=6, height=5)

############
#TREATMENT

# Compute the analysis of variance
res.aov <- aov(avgpercentopen ~ Treatment, data=test)
# Summary of the analysis
summary(res.aov)
#pvalue=0.561

#tukey pairwise
TukeyHSD(res.aov)
#All comparisons not sig

#or pairwise
pairwise.t.test(test$avgpercentopen,test$Treatment,
                 p.adjust.method = "BH")
#All comparisons not sig

#test for homogeneity
leveneTest(avgpercentopen ~ Treatment, data=test)
#equal variances p=0.4697
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.03451 so not normal

kruskal.test(avgpercentopen ~ Treatment, data=test)
#pvalue=0.4907

treatplot=ggboxplot(test, x = "Treatment", y = "avgpercentopen",
          color = "black", palette = c("chocolate4","#999999","#009E73"),fill="Treatment")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  labs(y="Proportion of open valves")+
  #compare_means(avgpercentopen ~ Site,  data = test)+
  #stat_compare_means(method = "anova")+
  #stat_compare_means(label = "p.signif", method = "t.test",
  #                   ref.group = "Winter") +
  #stat_compare_means(comparisons = my_comparisons_site,label.y = c(1, 1.05, 1.13),aes(label = after_stat(p.signif)))+
  stat_n_text(size=4,y.pos=1.27) + 
  ylim(0,1.3)+
  theme(axis.text=element_text(size=15),
        axis.title=element_text(size=18),legend.text=element_text(size=12),legend.title=element_text(size=15))

ggsave("percentopen_season_stats_treatment.png",path=plots,width=6, height=5)

ggplot(data = test,
       aes(x = Season, y = avgpercentopen, fill = Treatment))+
  geom_boxplot(col="black",position=position_dodge(.9))+
  stat_summary(position=position_dodge(.9),fun="mean", geom="point", shape=18, size=3, color="black")+
  theme_classicmodify()+
  labs(y="Proportion of open valves")+
  ylim(0,1)+
  scale_fill_manual(values=c( "chocolate4","#999999", "#009E73"))+
  theme(legend.title=element_text(size=12),legend.text=element_text(size=12))

ggsave("percentopen_season_boxplot_treat.png",path=plots,width=6, height=5)

ggplot(test,aes(x = Season, y = avgpercentopen, fill = Site))+
  geom_boxplot(col="black",position=position_dodge(.9))+
  stat_summary(position=position_dodge(.9),fun="mean", geom="point", shape=18, size=3, color="black")+
  theme_classicmodify()+
  labs(y="Proportion of open valves")+
  ylim(0,1)+
  scale_fill_manual(values=sitePalette)+
  theme(legend.title=element_text(size=12),legend.text=element_text(size=12))

ggsave("percentopen_season_boxplot_site.png",path=plots,width=6, height=5)

###################
ggplot(seasonsavgsite,aes(Season,avgpercentopen,fill=Season))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  labs(y="Proportion of open valves")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")

ggsave(path=plots, "gape_fourseasons.png",width=6, height=6)

ggplot(seasonsavgsite,aes(Season,avgpercentopen,fill=Season))+
  geom_boxplot()+
  stat_summary(fun=mean, geom="point", shape=18, size=5, color="red", fill="red")+
  theme_classicmodify()+
  labs(y="Proportion of open valves")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")+
  ylim(0,1)

ggsave(path=plots, "gape_fourseasons_boxplot.png",width=6, height=6)

ggplot(fourseasons,aes(Season,hallpercent,fill=Season))+
  geom_boxplot()+
  stat_summary(fun=mean, geom="point", shape=18, size=5, color="black", fill="red")+
  theme_classicmodify()+
  labs(y="Percent open (%)")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")+
  ylim(0,1)

ggsave(path=plots, "gape_fourseasons_box_gapepercent.png",width=6, height=6)

ggplot(seasonsavgsite,aes(Season,avgpercentopen,fill=Treatment))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75,position=position_dodge(.9))+
  theme_classicmodify()+
  labs(y="Proportion of open valves")+
  scale_fill_manual(values=c( "chocolate4","#999999", "#009E73"))

ggsave(path=plots, "gape_fourseasons_treatment.png",width=6, height=6)

ggplot(seasonsavgsite,aes(Season,avgpercentopen,fill=Treatment))+
  geom_boxplot()+
  stat_summary(position=position_dodge(.9),fun=mean, geom="point", shape=18, size=5, color="red", fill="red")+
  theme_classicmodify()+
  labs(y="Proportion of open valves")+
  scale_fill_manual(values=c( "chocolate4","#999999", "#009E73"))+
  ylim(0,1)

ggsave(path=plots, "gape_fourseasons_treat_boxplot.png",width=6, height=6)

ggplot(seasonsavgsite,aes(Season,avgpercentopen,fill=Site))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75,position=position_dodge(.9))+
  theme_classicmodify()+
  labs(y="Proportion of open valves")+
  scale_fill_manual(values=sitePalette)

ggsave(path=plots, "gape_fourseasons_site.png",width=6, height=6)

##########
#oyster temp
fourseasons %>% 
  subset(Temp.C>0) %>% #remove the bad data
  ggplot(aes(Season, Temp.C,fill=Season))+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  stat_compare_means(comparisons = my_comparisons,label.y = c(25, 26, 27,28,29),aes(label = after_stat(p.signif)))+
  geom_boxplot(col="black")+
  theme_classicmodify()+
  scale_fill_manual(values=season_palette)+
  labs(y="Water temp (C)")+
  ylim(0,30)

ggsave("temp_4seasons_box.png",path=plots, width=6, height=6)


fourseasonsbinary=fourseasons %>% 
  filter(!is.na(binary)) %>% 
  filter (Temp.C>0) #remove the bad data
  
fit <- glm(binary ~ Temp.C + factor(Season), data = fourseasonsbinary, family = binomial)

fourseasons$binarynew <- predict(fit, newdata = fourseasonsbinary, type = "response")

ggplot(data = fourseasonsbinary, 
       aes(Temp.C,binarynew,group=Season)) +
  geom_line(size = 1,aes(color=Season))+
  scale_color_manual(values=season_palette)+
  labs(x="Water temperature (C)", y="Gape binary (0=closed, 1=open)")+
  theme_classicmodify()+
  ylim(0,1)
  
ggsave("seasonbinary_temp.png",path=plots,height=6, width=6)

allgapemetabinary=allgapemeta %>% 
  filter(!is.na(binary)) %>% 
  filter (Temp.C>0) #remove the bad data
  
fittemp <- glm(binary ~ Temp.C, data = fourseasonsbinary, family = binomial)

fourseasonsbinary$binary2 <- predict(fittemp, newdata = fourseasonsbinary, type = "response")

ggplot(data = fourseasonsbinary, 
       aes(x = Temp.C, y = binary2)) +
  geom_line(size = 1)+
  labs(x="Water temperature (C)", y="Gape binary (0=closed, 1=open)")+
  theme_classicmodify()+
  ylim(0,1)

ggsave("binary_temp.png",path=plots, width=6, height=6)

##########
#precipitation
#find avg temp and hallpercent per month per oyster?

temphallseason=fourseasons %>% 
  group_by(DateTimeUTC = floor_date(DateTime, unit = "month")) %>% #make month column 
  group_by(Treatment,Site,Season,DateTimeUTC,Code) %>% 
  count(binary) %>% 
  mutate(percentopen=n/sum(n))%>% 
  filter(binary=="1") %>% 
  ungroup()

temphallseasonday=fourseasons %>% 
  group_by(DateTimeUTC = floor_date(DateTime, unit = "day")) %>% #make month column 
  group_by(Treatment,Site,Season,DateTimeUTC,Code) %>% 
  count(binary) %>% 
  mutate(percentopen=n/sum(n))%>% 
  filter(binary=="1") %>% 
  ungroup()

temphallseasonprecip=temphallseasonday %>% 
  group_by(DateTimeUTC) %>% 
  summarise(meanpercentopen=mean(percentopen))
  
temphallseasonprecip=merge(temphallseasonprecip,precipsumday,by="DateTimeUTC")


# ggplot(temphallseasonprecip,aes(precipday,percentopen))+
#   geom_point()+
#   geom_smooth()+
#   theme_classicmodify()+
#   labs(y="Proportion of open valves",x="Daily rainfall (mm)")
# 
# ggsave("precip_prop_open.png",path=plots, height=6, width=6)

coeff=55

ggplot(temphallseasonprecip, aes(x=DateTimeUTC)) +
  geom_bar(aes(y=precipday/coeff), stat="summary",position="dodge",fun="sum", size=1, color="blue", alpha=.4) + 
  geom_line(aes(y=meanpercentopen), size=1,alpha=0.6) +
  scale_y_continuous(
    # Features of the first axis
    name = "Proportion of open valves",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~.*coeff, name="Precipitation (mm)"))+
  theme_classicmodifygrid()+
    theme(
    axis.title.y = element_text(color = "black"),
    axis.title.y.right = element_text(color = "blue")) 

ggsave("precip_prop_open.png",path=plots, dpi = 500, width = 9, height = 4, units = "in")

coeff=55

ggplot(temphallseasonprecip, aes(x=DateTimeUTC)) +
  geom_line(aes(y=meanpercentopen), size=2,alpha=0.6) +
  geom_bar(aes(y=precipday/coeff), stat="summary",position="dodge",fun="sum", size=1, color="blue", fill="blue",alpha=.4) + 
  scale_y_continuous(
    # Features of the first axis
    name = "Proportion of open valves",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~.*coeff, name="Precipitation (mm)"))+
  theme_classicmodify()+
    theme(
    axis.title.y = element_text(color = "black"),
    axis.title.y.right = element_text(color = "blue")) +
  scale_x_datetime(limits = as.POSIXct(strptime(c("2023-01-01 00:00:00", "2023-04-01 00:00:00"), 
                   format = "%Y-%m-%d %H:%M")))

ggsave("precip_prop_open_zoom.png",path=plots, height=6, width=6)

###########################
#combined prop of open valves plots

figure=ggarrange(seasonplot+
                   theme(
                  axis.title.y = element_blank(),
                  legend.position="none"), 
           treatplot+ 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank(),
                  legend.position="none"), 
          siteplot+ 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank(),
                  legend.position="none") , nrow=1, ncol=3)

figure=annotate_figure(figure, left = textGrob("Proportion of open valves", rot = 90, vjust = 1, gp = gpar(cex = 1.5)))
figure
ggsave(plot=figure,"propgape_seatreatsite.png",path=plots,height=4, width=9)
#ggexport(figure, filename = "figure2.png",path=plots,dpi = 600, width = 800, height = 600, units = "in")

seasonplot=ggboxplot(test, x = "Season", y = "avgpercentopen",
          color = "black", palette = c("#009E73","#D55E00","#56B4E9","#CC79A7"),fill="Season")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  labs(y="Proportion of open valves",tag = "A")+
  stat_compare_means(comparisons = my_comparisons,label.y = c(0.97, 1.06, 1.10),aes(label = after_stat(p.signif)))+
  stat_n_text(size=4,y.pos=1.27) + 
  ylim(0,1.3)+
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=18),legend.position="none")

treatplot=ggboxplot(test, x = "Treatment", y = "avgpercentopen",
          color = "black", palette = c("chocolate4","#999999","#009E73"),fill="Treatment")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  labs(y="Proportion of open valves",tag = "B")+
  stat_n_text(size=4,y.pos=1.27) + 
  ylim(0,1.3)+
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=18),legend.position="none")

siteplot=ggboxplot(test, x = "Site", y = "avgpercentopen",
          color = "black", palette = sitePalette,fill="Site")+
  stat_summary(fun=mean, geom="point",shape=23, size=4, color="black", fill="red")+
  labs(y="Proportion of open valves",tag = "C")+
  stat_n_text(size=4,y.pos=1.27) + 
  ylim(0,1.3)+
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=18),legend.position="none")
```

#calculate 1 percent closed value for each code, graph and glm
```{r}
#calculate percent closed column 
allgapemetapercentopen=as.data.frame(allgapemeta %>% 
  group_by(Code, binary) %>% 
  summarise(n = n(), 
            Treatment=unique(Treatment), #keep treatment 
            Site=unique(Site)) %>%     #keep site
  mutate(percentopen = n/sum(n)) %>% 
  dplyr::filter(binary=="1") %>% 
  ungroup () 
  )

#graph? eelgrass closed most, then oyster, then mud
ggplot(allgapemetapercentopen, aes(Treatment, percentopen))+
  geom_bar(position="dodge", stat="summary", fun="mean", fill=c("#999999", "#009E73", "chocolate4"), col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  ylim(0,1)+
  theme_classic()+
  labs(y="Proportion of open valves")+
  theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))

ggsave("proportion_open_treatment.png", width=6, height=5,path=plots)

allgapemetapercentopen$Treatment=as.factor(allgapemetapercentopen$Treatment)
allgapemetapercentopen$percentopen=as.integer(allgapemetapercentopen$percentopen)

shapiro.test(allgapemetapercentopen$percentopen)
#not normal 

#check if equal variances
leveneTest(percentopen~Treatment, allgapemetapercentopen)
#p=0.66553 equal variances

percentopenmodel=aov(log(percentopen)~Treatment, data=allgapemetapercentopen)

summary(percentopenmodel)
#not significantly different
#p=0.481

ggplot(allgapemetapercentopen, aes(Site, percentopen))+
  geom_bar(position="dodge", stat="summary", fun="mean", fill=sitePalette, col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  ylim(0,1)+
  theme_classic()+
  labs(y="Proportion of open valves")+
  theme(axis.text = element_text(size = 20), axis.title = element_text(size = 30))

ggsave("proportion_open_site.png", path=plots, width=6, height=5)

allgapemetapercentopen$Site=as.factor(allgapemetapercentopen$Site)

shapiro.test(allgapemetapercentopen$percentopen)
#not normal 

#check if equal variances
leveneTest(percentopen~Site, allgapemetapercentopen)
#p=0.69257 equal variances

percentopenmodel=aov(log(percentopen)~Site, data=allgapemetapercentopen)

summary(percentopenmodel)
#not significantly different
#p=0.464
```

#tides
```{r}
ggplot(tides, aes(DateTime, TideHT.m))+
  geom_line()+
  #xlim(as.Date(c("2022-07-18", "2022-08-10")), format="%Y/%m/%d")
  #scale_x_datetime(limits = as.POSIXct(strptime(c("2022-07-18 00:00", "2022-08-10 23:59"),format = "%Y-%m-%d %H:%M")))+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black"))+
  labs(x="Date", y="Tide height (ft)")

ggsave("julyaugusttides.pdf", width = 8, height = 4)

#rtide package 
#make separate plot below that's just tides align with 

#plot of hallpercent of one oyster with datetime range

halltide=loadComboGape('D.M.2.1047')

par(mfrow = c(2,1), mar = c(4,6,2,1))
plot(as.POSIXct(halltide$DateTime), halltide$hallpercent, type = 'l', col = 2,
	ylab = 'Gape opening %',
	xlab = 'Date',
	las = 1,
	main = 'P.E.5.1076',
	xlim=as.POSIXct(c("2022-07-30 00:00:00", "2022-08-21 00:00:00")))
	
plot(tides$DateTime,tides$TideHT.m, type='l',
     xlab="Date",
     ylab="Tide height (m)",
     xlim=as.POSIXct(c("2022-07-30 00:00:00", "2022-08-21 00:00:00")))
dev.off()

halltide=merge(halltide,tides, by="DateTime")

max_first  <- max(halltide$hallpercent,na.rm=T)   # Specify max of first y axis
max_second <- max(halltide$TideHT.m) # Specify max of second y axis
min_first  <- min(halltide$hallpercent, na.rm=T)   # Specify min of first y axis
min_second <- min(halltide$TideHT.m) # Specify min of second y axis

# scale and shift variables calculated based on desired mins and maxes
scale = (max_second - min_second)/(max_first - min_first)
shift = min_first - min_second

ggplot(halltide, aes(x = DateTime, y = hallpercent)) +
  geom_line(aes(color="Gape opening %")) +
  geom_line(aes(y = inv_scale_function(TideHT.m,scale,shift), color = "Tide Height (m)")) +
  #scale_x_continuous(breaks = seq(0, 336, 24)) +
  scale_y_continuous(limits = c(min_first, max_first), sec.axis = sec_axis(~scale_function(., scale, shift), name="Tide Height (m)")) +
  labs(x = "Date", y = "Gape opening %", color = "") +
  scale_color_manual(values = c("black", "blue"))+
  theme_classic()
  #scale_x_datetime(limits = as.POSIXct(strptime(c("2022-07-30 00:00:00", "2022-08-22 00:00:00"),format = "%Y-%m-%d %H:%M")))

ggsave(path=plots, "D.M.2.1047_tides.png", width=6, height=6)
  
#find tidal elevation of oysters deployed, in tidal dataset- make column that says if oysters are under or out of water depending on tide height and shore height. Column of time stamps and column of yes/no for in/out of water. 
```

#temp
```{r}
#temp vs gapepercent
allgapemeta %>% 
  filter(Temp.C>0) %>% #remove the bad data
  ggplot(aes(Temp.C, hallpercent))+
  geom_smooth(se=T)+
  labs(x="Oyster temperature (C)",y="Percent gaping (%)")+
  theme_classicmodify()+
  ylim(0,100)

ggsave("hallpercent_temp.png",path=plots,height=6, width=6)

allgapemeta %>% 
  filter(Temp.C>0) %>% #remove the bad data
  ggplot(aes(Temp.C, hallpercent,col=Treatment))+
  geom_smooth(se=T)+
  scale_color_manual(values=treatment_palette)+
  labs(x="Oyster temperature (C)",y="Percent gaping (%)")+
  theme_classicmodify()+
  ylim(0,100)

ggsave("hallpercent_temp_treat.png",path=plots,height=6, width=6)

allgapemeta %>% 
  filter(Temp.C>0) %>% #remove the bad data
  ggplot(aes(Temp.C, hallpercent,col=Site))+
  geom_smooth(se=T)+
  scale_color_manual(values=sitePalette)+
  labs(x="Oyster temperature (C)",y="Percent gaping (%)")+
  theme_classicmodify()+
  ylim(0,100)

ggsave("hallpercent_temp_site.png",path=plots,height=6, width=6)

allgapemeta %>% 
  filter(!is.na(binary)) %>% 
  filter(Temp.C>0) %>% #remove the bad data
 # mutate_at("binary", as.factor) %>% 
  ggplot(aes(Temp.C,binary))+
    geom_point() +
     stat_smooth(method="glm", color="blue", se=FALSE,
                 method.args = list(family=binomial))+
  labs(x="Water temperature (C)", y="Gape binary (0=closed, 1=open)")+
  theme_classicmodify()

ggsave("binary_temp.png",path=plots, width=6, height=6)

halltemp=loadComboGape('D.M.2.1047')

max_first  <- max(halltemp$hallpercent,na.rm=T)   # Specify max of first y axis
max_second <- max(halltemp$Temp.C,na.rm=T) # Specify max of second y axis
min_first  <- min(halltemp$hallpercent, na.rm=T)   # Specify min of first y axis
min_second <- min(halltemp$Temp.C,na.rm=T) # Specify min of second y axis

# scale and shift variables calculated based on desired mins and maxes
scale = (max_second - min_second)/(max_first - min_first)
shift = min_first - min_second

ggplot(halltemp, aes(x = DateTime, y = hallpercent)) +
  geom_line(aes(color="Gape opening %")) +
  geom_line(aes(y = inv_scale_function(Temp.C,scale,shift), color = "Temp (C)")) +
  #scale_x_continuous(breaks = seq(0, 336, 24)) +
  scale_y_continuous(limits = c(min_first, max_first), sec.axis = sec_axis(~scale_function(., scale, shift), name="Temp (C)")) +
  labs(x = "Date", y = "Gape opening %", color = "") +
  scale_color_manual(values = c("black", "red"))+
  theme_classic()
  #scale_x_datetime(limits = as.POSIXct(strptime(c("2022-07-30 00:00:00", "2022-10-06 00:00:00"),format = "%Y-%m-%d %H:%M")))

ggsave(path=plots, "D.M.2.1047_temp.png", width=6, height=6)

########### Temp across treatments/sites ##############
ggplot(allgapemeta, aes(Treatment,Temp.C))+
  geom_boxplot(col="black", fill=treatment_palette)+
  scale_y_continuous(limits = c(0, 30))+
  theme_classic()

ggsave(path=plots, "Temp_vs_treatments.png",width=6, height=6)

ggplot(allgapemeta, aes(Site,Temp.C))+
  geom_boxplot(col="black",fill=sitePalette)+
  scale_y_continuous(limits = c(0, 30))+
  theme_classic()

ggsave(path=plots, "Temp_vs_sites.png",width=6, height=6)

#############Temp vs seasons ################
tempwet <- allgapemeta %>%
    # filter((DateTime > '2022-05-01 00:00:00') &
    #         (DateTime<"2022-12-31 00:00:00")) %>%
    filter((DateTime > '2022-12-31 00:00:00') &
        (DateTime<"2023-04-01 00:00:00"))

tempdry <- allgapemeta %>%
  filter((DateTime>"2022-05-01 00:00:00"& DateTime<"2022-12-31 00:00:00")|(DateTime>"2023-04-01 00:00:00"))

tempseason<- bind_rows(tempwet, tempdry, .id = 'Season')
tempseason=tempseason %>% 
  mutate(Season = recode(Season, '1' = 'Wet', '2' = 'Dry'))

ggplot(tempseason, aes(Season,Temp.C))+
  geom_boxplot()+
  scale_y_continuous(limits = c(0, 30))+
  theme_classic()

ggsave(path=plots, "Temp_vs_season.png", width=6, height=6)

######### Temp vs seasons plus treatments/sites #######
ggplot(tempseason, aes(Treatment,Temp.C, fill=Season))+
  geom_boxplot()+
  scale_y_continuous(limits = c(0, 30))+
  theme_classic()

ggsave(path=plots, "Temp_vs_season_trmt.png", width=6, height=6)

ggplot(tempseason, aes(Site,Temp.C, fill=Season))+
  geom_boxplot()+
  scale_y_continuous(limits = c(0, 30))+
  theme_classic()

ggsave(path=plots, "Temp_vs_season_site.png", width=6, height=6)

#############Temp vs gape across months ############
#average by site and treatment to get average %open/closed for each oyster in each site/treatment
#graph months on x axis, with bars of %open, with line of temp running across. May need to use temp from precip instead of sensor 

gapebinarytemp <- allgapemeta %>%
  group_by(Code) %>% #do calculations by oyster code
  group_by(day = floor_date(DateTime, unit = "day")) %>% #
  drop_na(binary) %>% 
  group_by(Code,day,Site,Treatment) %>% 
  count(binary) %>% 
  mutate(percentopen=n/sum(n)) %>% 
  subset(binary == '1')

#make a column the averages days for precip
precipday<-precip %>% 
  mutate(day = floor_date(DateTimeUTC, "day")) %>%
  group_by(day) %>%
  summarize(Dailytemp = mean(AirTempC))

gapebinarytemp1 <- left_join(gapebinarytemp, precipday, by=c("day"))

ggplot(gapebinarytemp1, aes(day, percentopen))+
  geom_point()+
  geom_point(y=gapebinarytemp1$Dailytemp, col="red")

max_first  <- max(gapebinarytemp1$percentopen,na.rm=T)   # Specify max of first y axis
max_second <- max(gapebinarytemp1$Dailytemp,na.rm=T) # Specify max of second y axis
min_first  <- min(gapebinarytemp1$percentopen, na.rm=T)   # Specify min of first y axis
min_second <- min(gapebinarytemp1$Dailytemp,na.rm=T) # Specify min of second y axis

# scale and shift variables calculated based on desired mins and maxes
scale = (max_second - min_second)/(max_first - min_first)
shift = min_first - min_second

ggplot(gapebinarytemp1, aes(x = day, y = percentopen)) +
  geom_line(aes(color="Gape opening %")) +
  geom_smooth(color="red")+
  geom_point(aes(y = inv_scale_function(Dailytemp,scale,shift), color = "Daily temp (C)")) +
  scale_y_continuous(limits = c(min_first, max_first), sec.axis = sec_axis(~scale_function(., scale, shift), name="Daily temp (C)")) +
  labs(x = "Date", y = "Fraction of oyster gaping %", color = "") +
  scale_color_manual(values = c("blue", "black"))+
  theme_classic()

ggsave(path=plots, "gape_vs_temp.png",width=6, height=6)

gapetemp <- allgapemeta %>%
  group_by(day = floor_date(DateTime, unit = "day")) %>% #
  drop_na(binary) %>% 
  group_by(day,Site,Treatment) %>% 
  count(binary) %>% 
  mutate(percentopen=n/sum(n)) %>% 
  subset(binary == '1')

gapebinarytemp2<- left_join(gapetemp, precipday, by=c("day"))

ggplot(gapebinarytemp2, aes(x = day, y = percentopen)) +
  geom_line(aes(color="Gape opening %")) +
  geom_smooth(color="red")+
  geom_point(aes(y = inv_scale_function(Dailytemp,scale,shift), color = "Daily temp (C)")) +
  scale_y_continuous(limits = c(min_first, max_first), sec.axis = sec_axis(~scale_function(., scale, shift), name="Daily temp (C)")) +
  labs(x = "Date", y = "Gape opening %", color = "") +
  scale_color_manual(values = c("blue", "black"))+
  theme_classic()
```

#air temp vs oyster temp
```{r}
#combine precip53sub and allgapemeta

# #pull out 53 from hightidetemp (oyster temps only at high tide)
# hightidetemp53 <- subset(hightidetemp, format(hightidetemp$DateTime, "%M") %in% c('53'))

precip$DateTime=precip$DateTimeUTC

airoytemp=merge(precip[,c("DateTime","AirTempC")],hightidetemp[,c("DateTime","Temp.C")], by="DateTime")

coeff=1

#D.E.1032 low temps in September

airoytemp %>% 
  subset(Temp.C>0) %>% #remove the bad data
  filter(!row_number() %in% c(1661,1667,1671,1677,1689,1540,1662,1650,1622)) %>% 
ggplot(aes(x=DateTime)) +
  geom_line(aes(y=AirTempC), size=2,alpha=0.6) +
  geom_line(aes(y=Temp.C), size=1, color="red", alpha=.4) + 
  scale_y_continuous(
    # Features of the first axis
    name = "Air Temperature (C)",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~.*coeff, name="Oyster temperature (C)"))+
  theme_classicmodify()+
    theme(
    axis.title.y = element_text(color = "black"),
    axis.title.y.right = element_text(color = "red")) 

ggsave("air_lowtideoy_temp.png",path=plots, height=6, width=6)

airoytemp %>% 
  subset(Temp.C>0) %>% #remove the bad data
ggplot(aes(x=AirTempC,y=Temp.C)) +
  geom_point() +
  geom_smooth()+
  theme_classicmodify()+
  labs(x="Air Temp (C)", y="Oyster Temp (C)")

ggsave("air_oy_temp_correlation.png",path=plots, height=6, width=6)

airoytempseason=airoytemp %>%
      mutate(Season = case_when( #new variable
      DateTime >= "2022-07-15 00:00:00" & DateTime <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
      DateTime >="2022-10-01 00:00:00" & DateTime <= "2022-12-31 23:59:00" ~ "Fall",
      DateTime >= "2023-01-01 00:00:00" & DateTime <= "2023-03-31 23:59:00" ~ "Winter",
      DateTime >= "2023-04-01 00:00:00" & DateTime <= "2023-06-19 23:59:00" ~ "Spring",
      TRUE ~ NA)) %>% 
  filter(!is.na(Season)) %>% #display error if a value is not assigned to one of the previous groups
  mutate_if(is.character, as.factor) %>% 
  mutate(Season = fct_relevel(Season,c("Summer", "Fall", "Winter", "Spring")))

airoytemplong=airoytempseason %>% 
  pivot_longer(cols=c('AirTempC', 'Temp.C'),
                    names_to='Temp_type',
                    values_to='Temp') %>% 
  distinct() %>% 
  group_by(DateTime,Temp_type,Season) %>% 
  summarise_all(mean)

airoytemplong$Temp_type=recode_factor(airoytemplong$Temp_type, AirTempC = "Air Temp (C)", 
                                Temp.C = "Oyster Temp (C)")

ggplot(airoytemplong,aes(Season, Temp,fill=Temp_type))+
  geom_bar(stat="summary",position="dodge",fun="mean",col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75,position=position_dodge(.9))+
  theme_classicmodify()+
  scale_fill_manual(values=season_palette)+
  labs(y="Temperature (C)")

ggsave("air_oy_temp_season.png",path=plots,height=6, width=6)
  

tempday=airoytemp %>%
    group_by(DateTime = floor_date(DateTime, unit = "day")) %>%
  summarise(temp_day_mean = mean(Temp.C, na.rm=T),
            sd = sd(Temp.C, na.rm=T),
            n = n())

tempdayseason=tempday %>%
      mutate(Season = case_when( #new variable
      DateTime >= "2022-07-15 00:00:00" & DateTime <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
      DateTime >="2022-10-01 00:00:00" & DateTime <= "2022-12-31 23:59:00" ~ "Fall",
      DateTime >= "2023-01-01 00:00:00" & DateTime <= "2023-03-31 23:59:00" ~ "Winter",
      DateTime >= "2023-04-01 00:00:00" & DateTime <= "2023-06-19 23:59:00" ~ "Spring",
      TRUE ~ NA)) %>% 
  filter(!is.na(Season)) %>% #display error if a value is not assigned to one of the previous groups
  mutate_if(is.character, as.factor) %>% 
  mutate(Season = fct_relevel(Season,c("Summer","Fall", "Winter", "Spring")))

dailywatertempplot=ggplot(tempdayseason,aes(Season, temp_day_mean,fill=Season))+
    geom_boxplot()+
  #geom_bar(stat="summary",position="dodge",fun="mean",col="black")+
  #stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75,position=position_dodge(.9))+
  theme_classicmodify()+
  theme(legend.position = "none")+
  scale_fill_manual(values=season_palette)+
  labs(y="Average daily water temperature (C)") +
  stat_n_text(size=5,y.pos=35) +
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
stat_compare_means(comparisons = my_comparisons_season,label.y = c(25, 26.5, 28,29.5,31,32.5),aes(label = after_stat(p.signif)))

# Compute the analysis of variance
res.aov <- aov(temp_day_mean ~ Season, data=tempdayseason)
# Summary of the analysis
summary(res.aov)
#pvalue=<2e-16

#tukey pairwise
TukeyHSD(res.aov)
#All comparisons sig except spring/fall

#or pairwise
pairwise.t.test(tempdayseason$temp_day_mean,tempdayseason$Season,
                 p.adjust.method = "BH")
#All comparisons not sig

#test for homogeneity
leveneTest(temp_day_mean ~ Season, data=tempdayseason)
#not equal variances p=0.8861
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.00306 so not normal

kruskal.test(temp_day_mean ~ Season, data=tempdayseason)
#pvalue=0.8687


precip53season=precip53sub %>%
      mutate(Season = case_when( #new variable
      DateTime >= "2022-07-15 00:00:00" & DateTime <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
      DateTime >="2022-10-01 00:00:00" & DateTime <= "2022-12-31 23:59:00" ~ "Fall",
      DateTime >= "2023-01-01 00:00:00" & DateTime <= "2023-03-31 23:59:00" ~ "Winter",
      DateTime >= "2023-04-01 00:00:00" & DateTime <= "2023-06-19 23:59:00" ~ "Spring",
      TRUE ~ NA)) %>% 
  filter(!is.na(Season)) %>% #display error if a value is not assigned to one of the previous groups
  mutate_if(is.character, as.factor) %>% 
  mutate(Season = fct_relevel(Season,c("Summer","Fall", "Winter", "Spring")))

  ggplot(precip53season,aes(Season, AirTempC,fill=Season))+
  geom_bar(stat="summary",position="dodge",fun="mean",col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75,position=position_dodge(.9))+
  theme_classicmodify()+
  scale_fill_manual(values=season_palette)+
  labs(y="Air temp (C)")
   
```

#death
```{r}
#############Death vs Treatment and Site ################
yesdead<-metafile %>%
  group_by(Site, Treatment, Fielddeath) %>% 
  summarise(n=n()) %>% 
  group_by(Fielddeath) %>% 
  mutate(prop=n/sum(n)) %>% 
  filter(Fielddeath %in% "Yes") 

ggplot(yesdead, aes(Treatment, n))+
  geom_bar(position="dodge", stat="summary",fun="mean",fill=c("#999999", "#009E73", "chocolate4"), col="black")+
  #stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  labs(y="Number of field deaths")+
  theme_classic()+
  ylim(0,5)

ggplot(deathseasons, aes(Treatment,n))+
  geom_col(col="black")+
  #stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  labs(y="Number of field deaths")+
  theme_classic()
  #ylim(0,5)

ggsave(path=plots, "Field_deaths_treatment.png", width=6, height=6)

ggplot(yesdead, aes(Site, n))+
  geom_bar(position="dodge", stat="summary", fun="mean",fill=sitePalette,col="black")+
 # stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  labs(y="Number of field deaths")+
  theme_classic()+
   ylim(0,5)
ggsave(path=plots, "Field_deaths_site.png", width=6, height=6)

# Compute the analysis of variance
res.aov <- aov(n ~ Treatment, data=yesdead)
# Summary of the analysis
summary(res.aov)
#pvalue=0.751

#tukey pairwise
TukeyHSD(res.aov)
#All comparisons not sig

#or pairwise
pairwise.t.test(test$avgpercentopen,test$Treatment,
                 p.adjust.method = "BH")
#All comparisons not sig

#test for homogeneity
leveneTest(n ~ Treatment, data=yesdead)
#equal variances p=0.2279
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.2777 so normal


#####################################
#death vs site
# Compute the analysis of variance
res.aov <- aov(n ~ Site, data=yesdead)
# Summary of the analysis
summary(res.aov)
#pvalue=0.751

#test for homogeneity
leveneTest(n ~ Site, data=yesdead)
#equal variances p=0.2279
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.2777 so normal

################ Death vs 4 seasons ######################
deathseasons=metafile %>%
      mutate(Season = case_when( #new variable
      timeofdeath >= "2022-07-15 00:00:00" & timeofdeath <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
      timeofdeath >="2022-10-01 00:00:00" & timeofdeath <= "2022-12-31 23:59:00" ~ "Fall",
      timeofdeath >= "2023-01-01 00:00:00" & timeofdeath <= "2023-03-31 23:59:00" ~ "Winter",
      timeofdeath >= "2023-04-01 00:00:00" & timeofdeath <= "2023-06-19 23:59:00" ~ "Spring",
      TRUE ~ NA)) %>% 
  filter(!is.na(Season)) %>% #display error if a value is not assigned to one of the previous groups
  mutate_if(is.character, as.factor) %>% 
  mutate(Season = fct_relevel(Season,c( "Summer","Fall", "Winter", "Spring")),
         Treatment= fct_relevel(Treatment,c("Mud","Oyster", "Eelgrass"))) %>% 
  distinct() %>% 
  ungroup() %>% 
  group_by(Site, Treatment, Fielddeath,Season) %>% 
  summarise(n=n())

deathseasongraph=deathseasons %>% 
  group_by(Season) %>% 
  summarise(n=sum(n))

deathseason=ggplot(deathseasongraph, aes(Season, n))+
  geom_bar(position="dodge", stat="summary", fun="mean", fill=season_palette, col="black")+
  #stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  labs(y="Number of field deaths")+
  theme_classic()+
  ylim(0,18)

res.aov <- aov(n ~ Season, data=deathseasons)
# Summary of the analysis
summary(res.aov)
#pvalue=0.646

#test for homogeneity
leveneTest(n ~ Season, data=deathseasons)
#equal variances p=0.6465
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.00001963 so not normal

kruskal.test(n ~ Season, data=deathseasons)
#pvalue=0.06089

#######################
#####Death by site

deathsitegraph=deathseasons %>% 
  group_by(Site) %>% 
  summarise(n=sum(n))

deathsite=ggplot(deathsitegraph, aes(Site, n))+
  geom_bar(position="dodge", stat="summary", fun="mean", fill=sitePalette, col="black")+
  #stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  labs(y="Number of field deaths")+
  theme_classic()+
  ylim(0,18)

res.aov <- aov(n ~ Site, data=deathseasons)
# Summary of the analysis
summary(res.aov)
#pvalue=0.646

#test for homogeneity
leveneTest(n ~ Site, data=deathseasons)
#equal variances p=0.6465
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.00001963 so not normal

kruskal.test(n ~ Site, data=deathseasons)
#pvalue=0.06089

#######################
#####Death by treatment

deathtreatgraph=deathseasons %>% 
  group_by(Treatment) %>% 
  summarise(n=sum(n))

deathtreat=ggplot(deathtreatgraph, aes(Treatment, n))+
  geom_bar(stat="identity", fill=treatment_palette, col="black")+
  labs(y="Number of field deaths")+
  theme_classic()+
  ylim(0,18)

res.aov <- aov(n ~ Treatment, data=deathseasons)
# Summary of the analysis
summary(res.aov)
#pvalue=0.646

#test for homogeneity
leveneTest(n ~ Treatment, data=deathseasons)
#equal variances p=0.6465
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.00001963 so not normal

kruskal.test(n ~ Treatment, data=deathseasons)
#pvalue=0.06089

figure=ggarrange(deathseason+
                   theme(
                  axis.title.y = element_blank() ), 
           deathtreat+ 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank() ), 
          deathsite+ 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank() ) , nrow=1, ncol=3)

figure=annotate_figure(figure, left = textGrob("Number of field deaths", rot = 90, vjust = 1, gp = gpar(cex = 1.25)))
figure

ggsave(plot=figure,"deaths_seatreatsite.png",path=plots,height=4.3, width=9)

#merge countseason and deathseasons
countseason$count=countseason$n

propdeath=merge(countseason,deathseasons, by=c("Site","Treatment","Season"), all = TRUE)
propdeath[is.na(propdeath)] <- 0

propdeath=propdeath %>% 
  group_by(Treatment, Site, Season) %>% 
  #select(-c(n.x, Fielddeath)) %>% 
  filter(count != 0) %>% 
  mutate(propdeath=n.y/count)

propdeathseas=propdeath %>% 
  group_by(Season) %>% 
  #select(-c(n.x, Fielddeath)) 
  mutate(sumdeath=sum(n.y),
         sumcount=sum(count),
        propdeath=sumdeath/sumcount)%>% 
  select(-c(Site, Treatment,count,n.y)) %>% 
  distinct()

propdeathseasplot=ggplot(propdeathseas, aes(Season, propdeath))+
  geom_bar(stat="identity", fill=season_palette, col="black")+
  labs(y="Number of field deaths proportional to number of oysters deployed",tag = "A")+
  ylim(0,1)+
  theme_classic()
  
propdeathtreat=propdeath %>% 
  group_by(Treatment) %>% 
  #select(-c(n.x, Fielddeath)) 
  mutate(sumdeath=sum(n.y),
         sumcount=sum(count),
        propdeath=sumdeath/sumcount)%>% 
  select(-c(Site, Season,count,n.y)) %>% 
  distinct()

propdeathtreatplot=ggplot(propdeathtreat, aes(Treatment, propdeath))+
  geom_bar(stat="identity", fill=treatment_palette, col="black")+
  labs(y="Number of field deaths proportional to number of oysters deployed",tag = "B")+
  ylim(0,1)+
  theme_classic()

propdeathsite=propdeath %>% 
  group_by(Site) %>% 
  #select(-c(n.x, Fielddeath)) 
  mutate(sumdeath=sum(n.y),
         sumcount=sum(count),
        propdeath=sumdeath/sumcount)%>% 
  select(-c(Treatment, Season,count,n.y)) %>% 
  distinct()

propdeathsiteplot=ggplot(propdeathsite, aes(Site, propdeath))+
  geom_bar(stat="identity", fill=sitePalette, col="black")+
  labs(y="Number of field deaths proportional to number of oysters deployed",tag = "C")+
  ylim(0,1)+
  theme_classic()

figure=ggarrange(propdeathseasplot+
                   theme(
                  axis.title.y = element_blank() ), 
           propdeathtreatplot+ 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank() ), 
          propdeathsiteplot+ 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank() ) , nrow=1, ncol=3)

figure=annotate_figure(figure, left = textGrob("Number of field deaths proportional to number of oysters deployed", rot = 90, vjust = 1, gp = gpar(cex = 0.8)))
figure

ggsave(plot=figure,"deaths_prop_seatreatsite.png",path=plots,height=4.3, width=9)

################################
#stats
  

res.aov <- aov(propdeath ~ Treatment, data=propdeath)
# Summary of the analysis
summary(res.aov)
#pvalue=0.646

#test for homogeneity
leveneTest(propdeath ~ Treatment, data=propdeath)
#equal variances p=0.6465
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.00001963 so not normal

kruskal.test(propdeath ~ Treatment, data=propdeath)
#pvalue=0.06089

res.aov <- aov(propdeath ~ Season, data=propdeath)
# Summary of the analysis
summary(res.aov)
#pvalue=0.646

#test for homogeneity
leveneTest(propdeath ~ Season, data=propdeath)
#equal variances p=0.6465
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.00001963 so not normal

kruskal.test(propdeath ~ Season, data=propdeath)
#pvalue=0.06089

res.aov <- aov(propdeath ~ Site, data=propdeath)
# Summary of the analysis
summary(res.aov)
#pvalue=0.646

#test for homogeneity
leveneTest(propdeath ~Site, data=propdeath)
#equal variances p=0.6465
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.00001963 so not normal

kruskal.test(propdeath ~ Site, data=propdeath)
#pvalue=0.06089
```

#number of oysters out during seasons
```{r}
countseason=metafile %>%
      mutate(Season = case_when( #new variable
      StartIncludeUTC >= "2022-07-15 00:00:00" & StartIncludeUTC <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
      StartIncludeUTC >="2022-10-01 00:00:00" & StartIncludeUTC <= "2022-12-31 23:59:00" ~ "Fall",
      StartIncludeUTC >= "2023-01-01 00:00:00" & StartIncludeUTC <= "2023-03-31 23:59:00" ~ "Winter",
      StartIncludeUTC >= "2023-04-01 00:00:00" & StartIncludeUTC <= "2023-06-19 23:59:00" ~ "Spring",
      TRUE ~ NA)) %>% 
  filter(!is.na(Season)) %>% #display error if a value is not assigned to one of the previous groups
  mutate_if(is.character, as.factor) %>% 
  mutate(Season = fct_relevel(Season,c( "Summer","Fall", "Winter", "Spring")),
         Treatment= fct_relevel(Treatment,c("Mud","Oyster", "Eelgrass"))) %>% 
  distinct() %>% 
  ungroup() %>% 
  group_by(Site, Treatment, Season) %>% 
  summarise(n=n())

countseasongraph=countseason %>% 
  group_by(Season) %>% 
  summarise(n=sum(n))

countseason=ggplot(countseasongraph, aes(Season, n))+
  geom_bar(position="dodge", stat="summary", fun="mean", fill=season_palette, col="black")+
  theme_classicmodify()+
  labs(y="Number of oysters deployed",tag = "A")+
  ylim(0,65)

countsitegraph=countseason %>% 
  group_by(Site) %>% 
  summarise(n=sum(n))

countsite=ggplot(countsitegraph, aes(Site, n))+
  geom_bar(position="dodge", stat="summary", fun="mean", fill=sitePalette, col="black")+
  theme_classicmodify()+
  labs(y="Number of oysters deployed",tag = "C")+
  ylim(0,65)

counttreatgraph=countseason %>% 
  group_by(Treatment) %>% 
  summarise(n=sum(n))

counttreat=ggplot(counttreatgraph, aes(Treatment, n))+
  geom_bar(position="dodge", stat="summary", fun="mean", fill=treatment_palette, col="black")+
  theme_classicmodify()+
  labs(y="Number of oysters deployed",tag = "B")+
  ylim(0,65)

figure=ggarrange(countseason+
                   theme(
                  axis.title.y = element_blank(),
                  axis.text.x=element_text(size=11)), 
           counttreat+ 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank(),
                  axis.text.x=element_text(size=11)), 
          countsite+ 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank(),
                  axis.text.x=element_text(size=10.5)) , nrow=1, ncol=3)

figure=annotate_figure(figure, left = textGrob("Number of oysters deployed", rot = 90, vjust = 1, gp = gpar(cex = 1.25)))
figure

ggsave(plot=figure,"count_seatreatsite.png",path=plots,height=4.3, width=9.5)

res.aov <- aov(n ~ Season, data=countseason)
# Summary of the analysis
summary(res.aov)
#pvalue=0.646

#test for homogeneity
leveneTest(n ~ Season, data=countseason)
#equal variances p=0.6465
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.00001963 so not normal

kruskal.test(n ~ Season, data=countseason)
#pvalue=0.06089

dunnTest(n ~ Season, data=countseason, method="bonferroni")


```

#measure proportional deaths
```{r}
propdeath=merge(countseason, deathseasons,by=c("Treatment","Season","Site"))
```


#old death season and death vs precip
```{r}
############## Death vs season #############
deathwet <- metafile %>%
    filter((timeofdeath > '2022-12-31 00:00:00') &
        (timeofdeath<"2023-04-01 00:00:00")) %>%
  group_by(Treatment,Site,Fielddeath) %>% 
  summarise(n=n()) %>% 
  group_by(Fielddeath) %>% 
  mutate(prop=n/sum(n)) %>% 
  filter(Fielddeath %in% "Yes") 

deathdry <- metafile %>%
  filter((timeofdeath>"2022-05-01 00:00:00"& timeofdeath<"2022-12-31 00:00:00")|(timeofdeath>"2023-04-01 00:00:00")) %>% 
  group_by(Treatment,Site,Fielddeath) %>% 
  summarise(n=n()) %>% 
  group_by(Fielddeath) %>% 
  mutate(prop=n/sum(n)) %>% 
  filter(Fielddeath %in% "Yes") 


seasondeath<- bind_rows(deathwet, deathdry, .id = 'Season')
seasondeath=seasondeath %>% 
  mutate(Season = recode(Season, '1' = 'Wet', '2' = 'Dry'))

ggplot(seasondeath, aes(Treatment, fill=Season))+
  geom_bar()+
  theme_classic()

ggsave(path=plots, "death_season/treatment.png", width=6, height=6)

ggplot(seasondeath, aes(Site, fill=Season))+
  geom_bar()+
  theme_classic()

ggsave(path=plots, "death_season/site.png", width=6, height=6)

############### Death vs precip #####################
precipdeath=bind_rows(precipsumday, metafile)

ggplot(precipdeath, aes(DateTimeUTC, precipday))+
  geom_vline(aes(xintercept= timeofdeath,
             color = "Field death"), size = 1)+
  geom_point()+
  labs(x="Date", y="Cumulative precipitation/day (mm)")+
  theme_classic()+
  theme(axis.text = element_text(size = 15), axis.title = element_text(size = 20),legend.title = element_blank(),legend.margin = margin(0.2, 0.2, 0.2, 0.2, "cm"),legend.text = element_text(size=15))

ggsave(path=plots, "death_precip.png", width=8, height=6)

```

#Subset data by every 10th row, treatment, and site. Plot on timescale
```{r}
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

#take every 50th row from dataframe
my10MinuteSubset = allgapemeta[seq(1,nrow(allgapemeta), by = 50),] 

cols = gg_color_hue(length(levels(my10MinuteSubset$Code)))
cols[levels(my10MinuteSubset$SN)=="W.M.2.1020"]="red"

highlight_types <- c('W.M.2.1020')

lims <- as.POSIXct(strptime(c("2023-01-01 00:00:00", "2023-03-30 00:00:00"), 
                   format = "%Y-%m-%d %H:%M:%S"))

my10MinuteSubset %>%
  drop_na(binary) %>% 
  #use filter to subset the rows based on the LOCATION column values
  #filter(SN == c("SN150", "SN138", "SN164", "SN143","SN191","")) %>% 
  #remove SN115 bc just looks closed for a couple days at the end. Remove SN112 bc only data in august
  # dplyr::filter(SN!="SN194")%>%
  # dplyr::filter(Site=="DeAnza")%>%
  # dplyr::filter(Treatment=="Eelgrass")%>%
  # dplyr::filter(!is.na(binary)) %>%
  ggplot(aes(x = DateTime, col=Code))+
    #fill=factor(ifelse(SN=="SN150","Highlighted","Normal"))))+
    #geom_smooth(x=Temp.C)+
    geom_jitter(aes(y = binary), shape = 1, position = position_dodge(width=0.3))+
    #geom_line(aes(y=Prediction))+
    #geom_beeswarm(dodge.width=0,cex=2,groupOnX=FALSE)+
    facet_grid(Treatment~.)+
    scale_x_datetime(date_labels = "%b", breaks = "1 month",limits = lims) +
    #scale_colour_manual(values=cols) +
    scale_y_discrete(labels=c("0" = "Closed", "1" = "Open"))+
    labs(y="Gape", x="")+
  theme_bw()
  #remove SN legend, take out grid lines but keep facet lines. Change font sizes
  #theme(legend.position = "none", axis.line=element_line(), panel.border = element_rect(color = "black",fill = NA,size = 1), panel.grid = element_blank(), axis.text = element_text(size = 20), axis.title = element_text(size = 30), strip.text = element_text(size = 20))

ggsave(path=plots,"gapealltreatment.png", width = 10, height = 6)

ggplot(my10MinuteSubset, aes(x = DateTime, Prediction))+
  geom_line(group=1)+
  theme_bw()+
  scale_x_datetime(labels=date_format("%Y %H:%M"), breaks = "1 hour") 
plot(my10MinuteSubset$DateTime, my10MinuteSubset$Prediction)
#Add tide data to graph (can overlay or be above), can put shaded regions on plots for low tide. 
my10MinuteSubset$Code=as.factor(my10MinuteSubset$Code)
my10MinuteSubset %>%
  #dplyr::filter(Site=="DeAnza")%>%
  dplyr::filter(Code=="W.E.5.1002")%>% 
  #dplyr::filter(Treatment=="Eelgrass")%>%
  dplyr::filter(!is.na(binary)) %>%
  ggplot(aes(x = DateTime, col=Code))+
    #geom_smooth(x=Temp.C)+
    geom_jitter(aes(y = binary), shape = 1, position = position_dodge(width=0.3))+
    #geom_line(aes(y=Prediction))+
    #geom_beeswarm(dodge.width=0,cex=2,groupOnX=FALSE)+
    #facet_grid(Treatment~.)+
    #scale_x_datetime(date_labels = "%b", breaks = "1 month") +
    #scale_colour_manual(values=group.colors) +
    scale_y_discrete(labels=c("0" = "Closed", "1" = "Open"))+
    #labs(y="Gape", x="")+
    theme_bw()+
  #remove SN legend, take out grid lines but keep facet lines. Change font sizes
  theme(legend.position = "none", axis.line=element_line(), panel.border = element_rect(color = "black",fill = NA,size = 1), panel.grid = element_blank(), axis.text = element_text(size = 20), axis.title = element_text(size = 30), strip.text = element_text(size = 20))
```

#only color one oyster
```{r}
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

cols = gg_color_hue(length(levels(my10MinuteSubset$Code)))
cols[levels(my10MinuteSubset$SN)=="W.M.2.1020"]="red"

highlight_types <- c('W.M.2.1020')

subsetnoNAs=my10MinuteSubset %>% 
  drop_na(binary)
  
  ggplot(aes(x = DateTime,col=Code))+
       geom_point(data=subsetnoNAs[which(!(subsetnoNAs$Code %in% highlight_types)),],aes(y=binary),color='gray75', size=2)+
    geom_point(data=subsetnoNAs[which(subsetnoNAs$Code %in% highlight_types),],aes(color=Code,y=binary), size=2) +
    #geom_jitter(aes(y = binary), shape = 1, position = position_dodge(width=0.3))+
    facet_grid(Treatment~.)+
    scale_x_datetime(date_labels = "%b", breaks = "1 month") +
    scale_y_discrete(labels=c("0" = "Closed", "1" = "Open"))+
    labs(y="Gape", x="")+
  theme_bw()+
  #remove SN legend, take out grid lines but keep facet lines. Change font sizes
  theme(legend.position = "none", axis.line=element_line(), panel.border = element_rect(color = "black",fill = NA,size = 1), panel.grid = element_blank(), axis.text = element_text(size = 20), axis.title = element_text(size = 30), strip.text = element_text(size = 20))

ggsave(path=plots,"gapeallsites1020.png", width = 10, height = 6)



```

#graph binary plot per oyster
```{r}
#won't print out every oyster bc if first binary is NA then skips over entire thing. But prints many of them. Just to check for ones that were closed for extended periods of time to pull out of allgape graph for presentation.
subcode = unique(my10MinuteSubset[,'Code'])

for (oy in subcode){
  subdata=my10MinuteSubset[my10MinuteSubset$Code == oy,]
  if(!is.na(subdata$binary)) {
    subdata %>%
    dplyr::filter(!is.na(binary)) %>%
    ggplot(aes(x = DateTime))+
      geom_jitter(aes(y = binary), shape = 1, position = position_dodge(width=0.3))+
      scale_y_discrete(labels=c("0" = "Closed", "1" = "Open"))+
      theme_bw()+
    #remove SN legend, take out grid lines but keep facet lines. Change font sizes
    theme(legend.position = "none", axis.line=element_line(), panel.border = element_rect(color = "black",fill = NA,size = 1), panel.grid = element_blank(), axis.text = element_text(size = 20), axis.title = element_text(size = 30), strip.text = element_text(size = 20))
    
    ggsave(path=plots, file=paste0("gape",oy,".png"), width = 10, height = 6)
  }
}

for (oy in subcode){
  subdata=my10MinuteSubset[my10MinuteSubset$Code == oy,]
  subdata %>%
      ggplot(aes(x = DateTime))+
        geom_jitter(aes(y = binary), shape = 1, position = position_dodge(width=0.3))+
        scale_y_discrete(labels=c("0" = "Closed", "1" = "Open"))+
        theme_bw()+
      #remove SN legend, take out grid lines but keep facet lines. Change font sizes
      theme(legend.position = "none", axis.line=element_line(), panel.border = element_rect(color = "black",fill = NA,size = 1), panel.grid = element_blank(), axis.text = element_text(size = 20), axis.title = element_text(size = 30), strip.text = element_text(size = 20))
      
    ggsave(path=plots, file=paste0("gape",oy,".png"), width = 10, height = 6)
}

```

#calculate binary proportions
```{r}
#calculate number of non-NA values, find fraction that are 1s and fraction that are 0s to get %closed/open. 
binarycounts=data.frame(tabyl(allgapemeta, Site, Treatment, binary))
#rename Site column
binarycounts$Site=binarycounts$X0.Site

#generate with percents
tabyl(allgapemeta, binary, Treatment) %>%
  adorn_percentages("col") %>%
  adorn_pct_formatting(digits = 1)

#Remove extra columns
binarycounts = subset(binarycounts, select = -c(X1.Site,X0.Site) )

#change to long format. Choose columns to gather. 
binary_long <- gather(binarycounts, Treatment, Count, X0.Eelgrass:X1.Oyster, factor_key=TRUE)

#add 0s and 1s column and rename Treatment factors
binary_long <-binary_long %>%
  mutate(binary = case_when(
    str_detect(Treatment, "X0") ~ "0",
    str_detect(Treatment, "X1") ~ "1",)) %>% 
    #str_detect(Treatment, "NA_") ~ "NA", #if include NAs then move )) %>% down
    mutate(Treatment = case_when(
    str_detect(Treatment, "Eelgrass") ~ "Eelgrass",
    str_detect(Treatment, "Mud") ~ "Mud",
    str_detect(Treatment, "Oyster") ~ "Oyster",)) 

#make NAs true NAs
#binary_long$binary[binary_long$binary=="NA"] <- NA

#change order of binary in barplot
binary_long$binary=factor(binary_long$binary, levels=c("1","0","NA"))
#change order of sites
binary_long$Site <- factor(binary_long$Site,levels = c("Westcliff", "PCH", "DeAnza", "Shellmaker"))
#change order of treatment
binary_long$Treatment <- factor(binary_long$Treatment,levels = c("Oyster", "Eelgrass", "Mud"))

#calculate percent open/closed with NAs
binary_long=as.data.frame(binary_long %>% 
  group_by(Treatment) %>% 
  mutate(percent=Count/sum(Count)))
  
#remove NAs from binary bc not included in graph. get percent separated by site and 0s and 1s.
#calculate percents without NAs all sites
binary_long_noNAallsites=binary_long %>% 
  na.omit() %>% 
  group_by(Treatment) %>%
  mutate(percent = Count/sum(Count))

#No NAs De Anza only 
binary_long_noNADA=binary_long %>% 
  na.omit() %>% 
  group_by(Site="De Anza") %>%
  group_by(Treatment, .add=T) %>%
  mutate(percent = Count/sum(Count))
```

#graphing binary barplots
```{r}
#graph percent stacked. separted by site
ggplot(binary_long_noNAallsites, aes(Site, Count, fill=binary))+
  geom_bar(stat="identity", position="fill")+
  theme_classic()+
  scale_fill_discrete(labels=c('Open', 'Closed'), name="Valve gape")+
  labs(y="Valve gape")

#separated by treatment percent stacked
ggplot(binary_long_noNAallsites, aes(Treatment, Count, fill=binary))+
  geom_bar(stat="identity", position="fill")+
  theme_classic()+
  scale_fill_discrete(labels=c('Open', 'Closed'), name="Valve gape")+
  labs(y="Valve gape")

#separated by treatment stacked
binary_longtest=as.data.frame(binary_long %>% 
  group_by(Treatment, binary) %>% 
  summarise(percentsum=sum(percent)))
  #dplyr::filter(binary==c("0","1"))) 

#graph so no black bars between groupings on stacked barplot
binary_longtest %>% 
  dplyr::filter(!is.na(binary)) %>% 
ggplot(aes(Treatment, percentsum, fill=factor(binary, levels=c("1","0"))))+
  geom_bar(stat="identity", position="stack", col="black")+
  theme_classic()+
  scale_fill_discrete(labels=c('Open', 'Closed'), name="Valve gape")+
  labs(y="Proportion valve gape")+
  ylim(0,1.0)

binary_long %>% 
  #dplyr::filter(!is.na(binary)) %>% 
ggplot(aes(Treatment, Count, fill=binary))+
  geom_bar(stat="identity", position="stack", col="black")+
  theme_classic()+
  scale_fill_discrete(labels=c('Open', 'Closed'), name="Valve gape")+
  labs(y="Valve gape")+
  ylim(0,1.0)

#separated by treatment and site
ggplot(binary_long_noNAallsites, aes(Treatment, Count, fill=binary))+
  geom_bar(stat="identity", position="fill")+
  theme_classic()+
  scale_fill_discrete(labels=c('Open', 'Closed'), name="Valve gape")+
  labs(y="Valve gape")+
  facet_grid(~Site)

#position=fill for percent stacked, position=dodge for side-by-side, position=stack for stacked. 
#rle- function that tells how long value stays 1 or 0 in dataset. Get list of row index and how long it stays at value. 
#in future- potentially filter out brief open/closed numbers. Run low pass filter over hall percentage values and remove brief closing periods. then can line up with tide height to show they're open when tide is high and closed when tide is low. Can try to correlated temp with gape values too. 

#Use metadata to plot all sensors from oyster treatment (plot hall %, make time series plot). 3 panel plot- seagrass, mud, oyster treatment for De Anza, do for each site. Or all on same plot w/ diff colors for diff treatments.   
#ideally also align with temp data/tide data. Can be in separate datasets just also needs to have POSIXxt to align timestamps. 
#rtide package that generates time series of predicated tides. Use long beach (LA) harbor tide station. 

#can have giant data file with all sensor data. 

#will need to calculate average open for individuals over time. 
```

#ANOVA on valve gape vs treatment
```{r}
# valvetreatment.glm <- binary_long %>% 
#   dplyr::filter(binary==0) %>% 
#   #group_by(Treatment) %>% 
#   #summarise(percentsum=sum(percent)) %>% 
valvetreatment.glm <-  glm(formula = percentclosed ~ Treatment,
                   data = binary_long,
                   family = "binomial")


```

#number of days have data for/other metadata stuff
```{r}
gapedays=allgapemeta %>% 
  group_by(Code) %>% 
  mutate(Date=as.Date(DateTime)) %>%
  select(-c(DateTime, Hall, Temp.C, Battery.V, SN, hallpercent, binary, Alive)) %>% 
  group_by(Code) %>% 
  filter(duplicated(Date) == FALSE) %>% 
  summarise(sumdays = n())

gapedays$Code=as.factor(gapedays$Code)

ggplot(gapedays, aes(x=factor(0),y=sumdays))+
  geom_boxplot()+
  theme_classic()+
  labs(y="Days of data collection", x="")+
  stat_summary(fun.y=mean, geom="point", shape=23, size=4, fill="black")+
  theme(axis.text.x =element_blank() , axis.ticks.x = element_blank(), axis.text = element_text(size = 15), axis.title = element_text(size = 20))

range(gapedays$sumdays)

ggsave(path=plots, "Gape_days_count.png", width=6, height=8)


metasamplesize=allgapemeta %>% 
  mutate(Date=as.Date(DateTime)) %>%
  group_by(Code, Date) %>%
  filter(duplicated(Code) == FALSE) %>% 
  select(-c(DateTime, Hall, Temp.C, Battery.V, SN, hallpercent, binary, Alive)) %>% 
  ungroup() %>% 
  group_by(Date,Site,Treatment) %>% 
  summarise(n = n())

metasamplesize %>% 
  filter(Treatment=="Eelgrass") %>% 
  ggplot(aes(Date, n, col=Site))+
    geom_point()+
    theme_classic()

ggplot(metasamplesize, aes(Date, n, col=Treatment))+
  geom_point()+
  facet_grid(~Site)+
  scale_x_date(date_breaks = "4 months" , date_labels = "%b-%y")+
  theme_bw()

ggsave(path=plots, "samplesize_treatmentsite.png", width=7, height=4)

mean(metasamplesize$n)

totalsamplesize=metasamplesize %>% 
  group_by(Date) %>% 
  mutate(totaln=sum(n)) %>% 
  select(-c(Treatment,Site,n)) %>% 
  filter(duplicated(Date) == FALSE) 
  

ggplot(totalsamplesize, aes(Date, totaln))+
  geom_point()+
  labs(y="Total oysters collecting data")+
 # scale_x_date(date_breaks = "months" , date_labels = "%b-%y")+
  theme_bw()

ggsave(path=plots, "total_sample_size.png", width=6, height=6)

mean(totalsamplesize$totaln)
```



```{r lukeScratch2, eval = FALSE}
require(nlme)
gapecalibfile = paste0(localpath,'Newport_SSINP/calibration/202303_calibrations_ostrea - Sheet1.csv')

gapeCalibs = read.csv(gapecalibfile)
# Reshape to long format
gC = reshape(gapeCalibs, varying = list(c('Reading1','Reading2','Reading3')),
				direction = 'long')
# Remove the saturated sensors P.M.3.1069 and D.M.2.1052
gC = gC[-(which(gC$Code == 'P.M.3.1069')), ]
gC = gC[-(which(gC$Code == 'D.M.2.1052')), ]

gC2 = groupedData(Reading1~Distance_mm|Code/time,data=gC)
		

```

```{r lukeScratch1, eval = FALSE}
# Messing around with the Hall data for growing oysters

# gapedat should hold data for oyster W.O.5.1016
oy="W.O.5.1016"
gapedat=loadComboGape(oy)
gapedat$DateTime = as.POSIXct(gapedat$DateTime, tz='UTC', format="%Y-%m-%d %H:%M:%S")

#plot(gapedat$DateTime, gapedat$Hall, type = 'l')

# It looks like the lower bound starts trending up quicker around Aug 1 2022
d1 = as.POSIXct('2022-08-01 00:00')

gape1 = gapedat[gapedat$DateTime >= d1, c('DateTime','Hall','hallpercent')]

plot(gape1$DateTime, gape1$Hall, type = 'l')


########################
# Less fancy, just grab the min at the start and end of the series, and
# fit a line to those two points to use as the correction
min1 = which.min(gape1$Hall[1:1440])  # 1 day of data at 1min interval = 1440
lastday = gape1[(nrow(gape1)-1440):nrow(gape1),] # subset last day of data
min2 = which.min(lastday$Hall)
bottomline = rbind(gape1[min1,], lastday[min2,])
# Fit a straight line between the 2 minimum values
mod1 = lm(Hall~DateTime,data = bottomline)
# Generate a set of best fit points along the line, for each original time point
newy = predict(mod1, newdata = gape1[c('DateTime','Hall')])
# Subtract off the initial minimum Hall value so that newy2 represents the
# offset below each point in the dataset
newy2 = newy - bottomline$Hall[1]
# Subtract off newy2 (offset) from each original Hall value. The offset will
# be larger for later dates if the minimum data were trending upwards
gape1$Hall2 = gape1$Hall - newy2
# Show the results
plot(gape1$DateTime, gape1$Hall, type = 'l') # Raw Hall data
abline(mod1, col = 1) # the regression line
lines(gape1$DateTime, gape1$Hall2, col = 4) # The adjusted Hall data
abline(h = bottomline$Hall[1], col = 4)

# Comments after playing with this: The simple algorithm does the thing I 
# expect it to do, adjusting the rising Hall values down to a flat baseline. 
# However, what you also see is that the maximum (presumed 100% opening) values
# start to shrink through time after the downward correction. This happens 
# because the actual range of Hall values is getting smaller as the animal 
# grows and the baseline closed value shifts upwards. This is a result of the
# curvelinear response of the hall effect sensor and the fact that when you
# start with the magnet further from the Hall sensor in the closed position,
# any opening of the shell results in a comparatively smaller change in the
# Hall value, because you're starting in a less-sensitive portion of the Hall
# sensor's range. Therefore it may be counterproductive to adjust these Hall
# signals down to have a flat baseline, because it will later result in a
# calculation of a smaller % opening if the total range is shrinking through 
# time. Instead we want to keep the shifting baseline information intact,
# and use it alongside sensor calibration data that come from different starting
# values to adjust the estimated shell opening values into percentages. 



```

### One time use for making allgapemeta file
####list individal gape files and combine into one csv per SN, cut out specific times from meta data that shouldn't be used
```{r get-list-of-files-and-make-big-csv, echo=FALSE}
#use unique() on first column. Save result as "serialnumber"
serialnumber=unique(metafile[,1])
oystercode = unique(metafile[,'Code']) #make list of all oyster codes 
#oystercode=oystercode[c(1:4)] #manually change which codes are combined 
######################################## 

# for(i in (serialnumber)){
for (oy in (oystercode) ){ 
  if(exists('combineddat')) { rm(combineddat)}
  # Handle the cases where an oyster (ID'd by 4-digit value at end of oystercode) moves
  # between positions within the same treatement, by simply looking for the matching
  # 4-digit oyster code and ignoring the site/treatment/position codes
  serialnumber = metafile[which(substr(metafile$Code,7,10) == substr(oy,7,10)),'SN']
  loops = 1
  
  for (i in (serialnumber)){
    # In metadata file, figure out the start and end dates for this serial number
    # when it was associated with this oyster code
    startDate = metafile[which(metafile$SN == i & 
                                 substr(metafile$Code,7,10) == substr(oy,7,10)),'StartIncludeUTC']
    endDate = metafile[which(metafile$SN == i & 
                               substr(metafile$Code,7,10) == substr(oy,7,10)),'EndIncludeUTC']
    gapepath=file.path(paste0(NewportDir,"alldata/",i))
      if(!file.exists(gapepath)) {
       next
      }
    #generate list of file names
    gapecombo=dir(path=gapepath, full.names = TRUE, pattern=paste0(i,"_GAPE.csv"))
      if(length(gapecombo)==0){
        next # if there are no data files found, skip to next serial number
      }
    #Use concatgapefiles to combine
    # gapedat = ConcatGapeFiles(gapecombo, myTimeZone = 'UTC', verbose = FALSE)
    gapedat = ConcatGapeFiles(gapecombo, myTimeZone = 'UTC', code = oy, 
                              startDate = startDate, endDate = endDate,
                              verbose = FALSE)
    
    if (loops == 1){
      combineddat = gapedat
      loops = loops + 1
    } else if (loops > 1) {
      combineddat = rbind(combineddat,gapedat)
    }
  } #end of serial number looping

  if (exists('combineddat')){
    if (nrow(combineddat) > 0){
      gapedat = combineddat # copy back over to this data frame for further use
      gapedat = gapedat[order(gapedat$DateTime),]
      # Figure out if there's a time of death associated with this oyster code. Get all of
      # the rows that match this oyster code and pull the contents of the timeofdeath column
      deathrows = metafile[which(substr(metafile$Code,7,10) == substr(oy,7,10)),'timeofdeath']
      # Check to see if any of the values in deathrows is not an NA
      if (length(which(!is.na(deathrows))) > 0) {
        # If we found a non-NA value in deathrows, grab it and store it in timeofdeath
        timeofdeath = deathrows[which(!is.na(deathrows))]
      } else {
        # There were no non-NA values in deathrows, so set timeofdeath to NA
        timeofdeath = NA
      }
     # Call exciseHall function to generate a master data frame of raw hall effect readings from the oysters that were present in the field.
      gapedat = exciseHall(halldata = gapedat, metadf = metafile, maintenance = maintenance, timeofdeath = timeofdeath)
      write.csv(gapedat, paste0(combogapefile, oy, "_combogape.csv"), row.names = FALSE)
    }
  }

} # end of oystercode for loop 
```

####write over combogape files to add in percent gape and percentiles for
####binary graphing
```{r calcPercentageGape,echo=showcode, eval = evalAll}
#get data from calibration. Generate set of regression coefficients and put them in cRegress. Use cRegress to do % calculation.
oystercode = unique(metafile[,'Code'])
chNum=1
cRegress = cRegressEstimate("SN20", chNum,gapecalibfile)

for(oy in (oystercode)){
#oy="W.O.5.1016"
  gapepath=file.path(paste0(combogapefile, oy, "_combogape.csv"))
    if(!file.exists(gapepath)) {
     next
    }
  gapedat=read.csv(gapepath)
      if(all(is.na(gapedat[,'Hall']))){
      next
    }
	# Convert the Hall effect values into estimated percent gape (0-100%)
  #1440=# min in a day, *5 days for one week of sampling.
	#Every day 5 days calculate percent gape.
	gapePercentage = calcPercentGapeNLMEsubset(gapedat[,"Hall"], cRegress, 
	                                           percentileLim = c(0.01,0.95), maxrow=1440*5, 
	                                           myPercentThreshold = 0.9)
	if (length(gapePercentage) > 0){
	  gapedat[,"hallpercent"] = gapePercentage
	} else {
	  gapedat[,'hallpercent'] = NA
	}
	

	#make empty binary column
	gapedat$binary=NA
	#for hallpercent rows less than or equal to 20, write 0 in binary column
	gapedat[which(gapedat$hallpercent<=5), "binary"]=0
	#for hallpercent rows greater than 20, write 1 in binary column
	gapedat[which(gapedat$hallpercent>5), "binary"]=1
	#save as one csv per SN
	write.csv(gapedat, paste0(combogapefile, oy, "_combogape.csv"), row.names=FALSE)
}
```

###Remove outliers for specific oysters.
```{r}
# Manually removed from combogape files on comp that were all bad
# If not manually removed, code for removing from allgape is below

########## Individually cut off sensor data after death or certain dates
#P.M.3.1069 keep everything after 2022-10-23 21:21:00
gapepath=file.path(paste0(combogapefile, "P.M.3.1069", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime>"2022-10-23 21:21:00")
write.csv(gapedat, paste0(combogapefile, "P.M.3.1069", "_combogape.csv"), row.names=FALSE)

#plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked

#####D.O.1.1100 keep everything after 2023-04-21 00:00:00
gapepath=file.path(paste0(combogapefile, "D.O.1.1100", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime>"2023-04-21 00:00:00")
write.csv(gapedat, paste0(combogapefile, "D.O.1.1100", "_combogape.csv"), row.names=FALSE)
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked

#####D.M.1.1102 keep everything after 2023-05-15 00:00:00
gapepath=file.path(paste0(combogapefile, "D.M.1.1102", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime>"2023-05-15 00:00:00")
write.csv(gapedat, paste0(combogapefile, "D.M.1.1102", "_combogape.csv"), row.names=FALSE)
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked

#####D.M.1.1091 keep everything after 2023-02-15 00:00:00 and before 2023-05-02 20:00:00
gapepath=file.path(paste0(combogapefile, "D.M.1.1091", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime>"2023-02-15 00:00:00") %>% 
  filter(DateTime<"2023-05-01 20:00:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "D.M.1.1091", "_combogape.csv"), row.names=FALSE)

#####D.M.2.1047 keep everything  before 2022-08-15 00:00:00
gapepath=file.path(paste0(combogapefile, "D.M.2.1047", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2022-08-15 00:00:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "D.M.2.1047", "_combogape.csv"), row.names=FALSE)

#####W.E.6.1005 keep everything  before 2023-05-07 00:00:00
gapepath=file.path(paste0(combogapefile, "W.E.6.1005", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2023-05-07 00:00:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "W.E.6.1005", "_combogape.csv"), row.names=FALSE)

#####S.M.1.1092 keep everything >"2023-02-18 00:00:00"
gapepath=file.path(paste0(combogapefile, "S.M.1.1092", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime>"2023-02-18 00:00:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "S.M.1.1092", "_combogape.csv"), row.names=FALSE)

#####W.O.1.1096 keep everything>"2023-05-18 00:00:00"
gapepath=file.path(paste0(combogapefile, "W.O.1.1096", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime>"2023-05-18 00:00:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "W.O.1.1096", "_combogape.csv"), row.names=FALSE)

#####W.M.3.1022 keep everything<"2023-04-12 06:57:00"
gapepath=file.path(paste0(combogapefile, "W.M.3.1022", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2023-04-12 06:57:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "W.M.3.1022", "_combogape.csv"), row.names=FALSE)

#####P.E.1.1078 
gapepath=file.path(paste0(combogapefile, "P.E.1.1078", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime>"2023-01-22 00:00:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "P.E.1.1078", "_combogape.csv"), row.names=FALSE)

###D.M.5.1047
gapepath=file.path(paste0(combogapefile, "D.M.5.1047", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2022-08-16 00:00:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "D.M.5.1047", "_combogape.csv"), row.names=FALSE)

###D.O.5.1038
gapepath=file.path(paste0(combogapefile, "D.O.5.1038", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2022-10-29 20:40:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "D.O.5.1038", "_combogape.csv"), row.names=FALSE)

###W.O.2.1011
gapepath=file.path(paste0(combogapefile, "W.O.2.1011", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2022-08-22 04:40:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "W.O.2.1011", "_combogape.csv"), row.names=FALSE)

###D.E.2.1028
gapepath=file.path(paste0(combogapefile, "D.E.2.1028", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2022-09-23 00:00:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "D.E.2.1028", "_combogape.csv"), row.names=FALSE)

#####D.E.3.1036
gapepath=file.path(paste0(combogapefile, "D.E.3.1036", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime>"2022-12-20 12:00:00")%>% 
  filter(DateTime<"2022-12-25 23:00:00")
  
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "D.E.3.1036", "_combogape.csv"), row.names=FALSE)

#####D.E.4.1033
gapepath=file.path(paste0(combogapefile, "D.E.4.1033", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime>"2023-05-01 00:00:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "D.E.4.1033", "_combogape.csv"), row.names=FALSE)

###S.M.3.1088 died in the field
gapepath=file.path(paste0(combogapefile, "S.M.3.1088", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2023-02-01 00:00:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "S.M.3.1088", "_combogape.csv"), row.names=FALSE)

###P.M.4.1068 #i guess i reattached the gape sensor
gapepath=file.path(paste0(combogapefile, "P.M.4.1068", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2022-09-01 00:00:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "P.M.4.1068", "_combogape.csv"), row.names=FALSE)

###P.M.5.1067
gapepath=file.path(paste0(combogapefile, "P.M.5.1067", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2022-08-09 00:07:00")
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "P.M.5.1067", "_combogape.csv"), row.names=FALSE)

###D.O.2.1037
gapepath=file.path(paste0(combogapefile, "D.O.2.1037", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter((DateTime>"2022-10-01 00:00:00"& DateTime<"2023-01-07 00:00:00")|(DateTime>"2023-02-15 00:00:00" & DateTime<"2023-02-27 00:00:00"))
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "D.O.2.1037", "_combogape.csv"), row.names=FALSE)

###W.M.2.1019
gapepath=file.path(paste0(combogapefile, "W.M.2.1019", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter((DateTime>"2022-07-01 00:00:00"& DateTime<"2022-09-07 00:00:00")|(DateTime>"2023-05-15 00:00:00" & DateTime<"2023-07-27 00:00:00"))
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "W.M.2.1019", "_combogape.csv"), row.names=FALSE)

###P.M.2.1072
gapepath=file.path(paste0(combogapefile, "P.M.2.1072", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2023-02-11 06:00:00")
plot(gapedat$DateTime, gapedat$Hall, type="l") #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "P.M.2.1072", "_combogape.csv"), row.names=FALSE)

###W.E.3.1007
gapepath=file.path(paste0(combogapefile, "W.E.3.1007", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter((DateTime>"2022-07-01 00:00:00"& DateTime<"2022-10-20 00:00:00")|
        (DateTime>"2022-10-25 00:00:00" & DateTime<"2023-05-24 00:00:00")|
        (DateTime>"2023-06-01 00:00:00" & DateTime<"2023-07-27 00:00:00"))
plot(gapedat$DateTime, gapedat$Hall) #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "W.E.3.1007", "_combogape.csv"), row.names=FALSE)

###D.M.2.1053 take off points at end
gapepath=file.path(paste0(combogapefile, "D.M.2.1053", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2022-11-24 00:00:00")
plot(gapedat$DateTime, gapedat$Hall, type="l") #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "D.M.2.1053", "_combogape.csv"), row.names=FALSE)

###W.O.2.1012 cut out middle
gapepath=file.path(paste0(combogapefile, "W.O.2.1012", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter((DateTime>"2022-10-01 00:00:00" & DateTime<"2023-01-01 00:00:00")|
          (DateTime>"2023-04-01 00:00:00" & DateTime<"2023-07-01 00:00:00"))
plot(gapedat$DateTime, gapedat$Hall, type="l") #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "W.O.2.1012", "_combogape.csv"), row.names=FALSE)

###W.M.4.1023 cut out middle
gapepath=file.path(paste0(combogapefile, "W.M.4.1023", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter((DateTime>"2022-05-01 00:00:00" & DateTime<"2022-10-01 00:00:00")|
          (DateTime>"2023-04-01 00:00:00" & DateTime<"2023-07-01 00:00:00"))
plot(gapedat$DateTime, gapedat$Hall, type="l") #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "W.M.4.1023", "_combogape.csv"), row.names=FALSE)

###P.M.2.1070 cut out beginning
gapepath=file.path(paste0(combogapefile, "P.M.2.1070", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime>"2022-08-09 00:00:00")
plot(gapedat$DateTime, gapedat$Hall, type="l") #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "P.M.2.1070", "_combogape.csv"), row.names=FALSE)

###W.M.1.1026 cut out beginning
gapepath=file.path(paste0(combogapefile, "W.M.1.1026", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime>"2022-10-11 00:00:00")
plot(gapedat$DateTime, gapedat$Hall, type="l") #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "W.M.1.1026", "_combogape.csv"), row.names=FALSE)

###D.O.3.1044
gapepath=file.path(paste0(combogapefile, "D.O.3.1044", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2023-04-01 00:00:00")
plot(gapedat$DateTime, gapedat$Hall, type="l") #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "D.O.3.1044", "_combogape.csv"), row.names=FALSE)

###D.E.3.1029
gapepath=file.path(paste0(combogapefile, "D.E.3.1029", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2022-08-15 00:00:00")
plot(gapedat$DateTime, gapedat$Hall, type="l") #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "D.E.3.1029", "_combogape.csv"), row.names=FALSE)

###P.E.5.1076
gapepath=file.path(paste0(combogapefile, "P.E.5.1076", "_combogape.csv"))
gapedat=read.csv(gapepath)
gapedat$DateTime=as.POSIXct(gapedat$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")
gapedat=gapedat %>% 
  filter(DateTime<"2022-08-21 00:00:00")
plot(gapedat$DateTime, gapedat$Hall, type="l") #check to make sure worked
write.csv(gapedat, paste0(combogapefile, "P.E.5.1076", "_combogape.csv"), row.names=FALSE)
```

###gape test plots
```{r testPlotsGape, eval=FALSE}

for (oy in (oystercode)){
  filecheck = file.path(paste0(combogapefile,oy,'_combogape.csv'))
  if(!file.exists(filecheck)) {
   next
  }
  png(file=paste0(plots,"hallpercenttest/",oy,"_hall_test.png"), width=800, height=600)
  testdata=loadComboGape(oy)
    if(all(is.na(testdata[,'Hall']))){
    next
  }
  plot2HallFunction(testdata)
  dev.off()
  remove(testdata)
 }
```

###Create allgapemeta, combine combogape files (w/hallpercent, percentile, binary columns) into one giant dataset. Combine with metadata. Save dataset
```{r combine-all-gape-files-and-add-metadata}
#combine all data frames into 1
allgape <- list.files(path=combogapefile, full.names = T) %>% 
  lapply(read.csv) %>% 
  bind_rows %>% 
  dplyr::filter(!is.na(Hall))

#change datetime to posixct
allgape$DateTime=as.POSIXct(allgape$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")

#combine metafile and allgape by SN to get treatment/site for each sensor
allgapemeta<-merge(allgape,metafile[, c("Site", "Treatment","Alive","Code")], by="Code")

#Remove bad gape oysters
allgapemeta=allgapemeta%>%
  dplyr::filter(
    Code!="D.M.4.1048" & #sensor not good
    Code!="W.M.5.1025" & #sensor not good
    Code!="P.O.2.1061" & #sensor not good
    Code!="S.O.1.1080" & #sensor not good
    Code!="D.O.3.1043" & #sensor saturated
    Code!="P.O.5.1063" & #sensor saturated
    Code!="D.E.1.1034" & #sensor saturated
    Code!="S.O.5.1084" & #sensor saturated
    Code!="S.O.3.1094" & #sensor not good
    Code!="S.O.1.1085" & #sensor not good
    Code!="W.O.3.1099" & #sensor not good
    Code!="P.O.1.1064" & #sensor not good
    Code!="S.O.2.1093" & #sensor not good
    Code!="D.O.1.1042" & #sensor not good
    Code!="P.O.2.1066" & #sensor not good
    Code!="D.E.1.1035" & #sensor not good
    Code!="W.O.3.1014" & #sensor not good
    Code!="D.M.1.1051" & #sensor not good
    Code!="W.O.1.1017" & #sensor saturated
    Code!="P.O.1.1057" & #sensor not good
    Code!="D.O.4.1040" & #sensor not good
    Code!="D.O.4.1039" & #sensor not good
    Code!="W.E.5.1003" & #sensor saturated
    Code!="D.E.3.1030" & #sensor not good
    Code!="D.M.6.1056" & #sensor not good 
    Code!="W.M.5.1024" & #sensor narrow range
    Code!="S.O.2.1081" & #sensor narrow range 
    Code!="S.M.2.1090" & #sensor narrow range 
    Code!="P.M.1.1097" & #sensor narrow range
    Code!="S.M.1.1086" & #probably dead oyster
    Code!="W.E.2.1006" ) #sensor saturated

#change order of treatment
allgapemeta$Treatment <- factor(allgapemeta$Treatment,levels = c("Oyster", "Eelgrass", "Mud"))
#make binary column categorical
allgapemeta$binary=as.factor(allgapemeta$binary)
#change order of sites
allgapemeta$Site <- factor(allgapemeta$Site,levels = c("Westcliff", "PCH", "DeAnza", "Shellmaker"))
#make code factor
allgapemeta$Code<-as.factor(allgapemeta$Code)

remove(allgape)

#calculate length of data collection for each oyster
allgapemetatime<-allgapemeta %>% 
  group_by(Code) %>% 
  summarize(Begin = min(DateTime, na.rm=TRUE),
          End = max(DateTime, na.rm=TRUE)) %>% 
  mutate(Diff=End-Begin)

#oysters <7 days:# S.M.1.1092 # P.M.5.1067 # W.O.3.1095 # P.M.4.1068 # D.E.3.1036 #P.M.3.1069

#remove from dataset:
allgapemeta=allgapemeta%>%
  dplyr::filter(
    Code!="S.M.1.1092" & 
    Code!="P.M.5.1067" & 
    Code!="W.O.3.1095" & 
    Code!="P.M.4.1068" & 
    Code!="P.M.3.1069" & 
    Code!="D.E.3.1036")

allgapemeta=allgapemeta %>% 
  distinct()

write.csv(allgapemeta, paste0(NewportDir,"combinedgapemeta/", "allgapemeta.csv"), row.names=FALSE)

```




#add temp data on SN150
```{r}

#without temp
ggplot(SN150, aes(DateTime, Hall))+
  geom_line() + 
  theme_classic()+
  ylim(0,400)+
  labs(y="Raw Gape Data", x="Date")+
  theme(axis.text = element_text(size = 15), axis.title = element_text(size = 25))

ggsave("SN150_gape.png", width=6.5, height=5)

#with temp
scale=.1
ggplot(SN150, aes(DateTime, Hall))+
  geom_line() + 
  #geom_line(aes(y=Temp.C/scale), color="red")+
  scale_y_continuous(
    # Features of the first axis
    name = "Raw Gape Data",
    # Add a second axis and specify its features
    sec.axis = sec_axis(trans=~.*scale, name="Temperature (C)"))+
  theme_classic()+
  labs(y="Raw Gape Data", x="Date")+
  theme(axis.text = element_text(size = 15), axis.title = element_text(size = 25))
  
ggsave("SN150_with_temp.png", width=6.5, height=5)

```

#look for saturated sensors
```{r}
saturated=allgapemeta[allgapemeta$Hall< 20, ]

unique(saturated$Code)

# D.E.1.1032, D.E.2.1028, D.E.4.1033, D.M.2.1052, D.O.5.1038, P.M.1.1067, P.M.3.1069, P.M.5.1067, P.O.2.1060, P.O.3.1060, W.E.5.1009, W.E.6.1004, W.O.2.1012

### need to toss: D.O.3.1043, P.O.5.1063, S.O.5.1084, D.E.1.1034
# W.M.5.1025 no good
# P.O.2.1061 delete
# P.M.3.1069 keep everything after 2022-10-23 21:21:00
# S.O.1.1080 toss whole thing?

allgapemeta %>%
  dplyr::filter(Code=="W.E.6.1004")%>% 
  #dplyr::filter(DateTime<as.POSIXct("2022-10-17"))%>%
  ggplot(aes(x=DateTime, y=Hall))+
  geom_point()
```

#find oyster field death times
```{r}
# Find oysters that died and cut off the gape after they died- but at what point? Look at raw data--> see it drift up to fully open and then flatline. base plot use identify function, lets you click on point and returns x and y coordinates so you can see DateTime that died.
# W.M.5.1025 no good
# P.O.2.1061 delete
# P.M.3.1069 keep everything after 2022-10-23 21:21:00
# S.O.1.1080 toss whole thing?

fielddeath=allgapemeta %>%
  dplyr::filter(Code=="W.E.5.1002") #%>%
  #dplyr::filter(DateTime<as.POSIXct("2023-02-07 23:26:00"))

plot(fielddeath$DateTime, fielddeath$Hall)

identify(fielddeath$DateTime, fielddeath$Hall)

fielddeath$DateTime[128710]

```

#growth data
```{r}
growth$Date=as.Date(growth$Date)
growth %>% 
  dplyr::filter(Code=="D.E.5.1031") %>% 
  ggplot(growth, aes(Date, Length_mm))+
  labs(title="D.E.2.1028")
  geom_line()
```

#Extra code
```{#r convertHalltobinary attempts}
#make for loop that converts Hall values to 0s if between 0-40, and 1 if between 70-100 percentile. Use allgapemeta

#code for finding percentile
quantile(df$columnname, probs=0.9)

#didn't work- doesn't separate by SN, assigned 0s and 1s based on all Hall percentiles combined
allgapemeta$binary <- with(allgapemeta, +(Hall >= quantile(Hall, probs=0.75)))

#didn't work- only took out 3 Hall numbers per SN that were in 0.25, 0.5, and 0.75 percentile
allgapemetabinary<-allgapemeta %>% 
  group_by(SN) %>% 
  summarise(Hall = quantile(Hall, c(0.25, 0.5, 0.75)), q = c(0.25, 0.5, 0.75))%>%
  mutate(allgapemeta$q)

SN104$binary <- with(SN104, +(hallpercent >= quantile(hallpercent, probs=c(0,0.75))))

#make everything greater than or equal to 0.75 percentile a 1, everything else is 0
SN104$binary <- with(SN104, +(hallpercent >= quantile(hallpercent, 0.75)))

SN104$percentile=transform(SN104, percentile=findInterval(hallpercent, quantile(hallpercent, seq(0,1, by=.1))))

#could separate by SN in large dataset but don't know how to assign 0s and 1s to 0-0.25 and 0.75-1 respectively
allgapemetabinary%>%
  group_by(SN)%>%
  summarize(binary=quantile(hallpercent, 0.75))

#create column quartile with 1,2,3,4 based on hallpercent percentiles 0-0.25, 0.25-0.5, 0.5-0.75, 0.75-1.0
SN104 <- within(SN104, quartile <- as.integer(cut(hallpercent, quantile(hallpercent, probs=0:4/4), include.lowest=TRUE)))

#create binary column that sets 1 for percentile 1, and 2 for percentile 4, percentile 2 and 3 are NAs
SN104=SN104 %>% mutate(binary =
                     case_when(quartile == 1 ~ "1", 
                               quartile == 2&3 ~ "NA",
                               quartile == 4 ~ "2"))
```

\`\`\`{#r Extracode} #concatgapefiles works outside of for loop #gapedat
= ConcatGapeFiles(gapecombo, myTimeZone = 'UTC', verbose = FALSE)

#for(i in (serialnumber)){ \# list.files(dir(path=paste0(NewportDir,
WestcliffEelgrassDir,i), full.names = #TRUE,
pattern=paste0(i,"\_GAPE.csv")))}

#for(i in (serialnumber)){ \# list.files(path=paste0(NewportDir,
#WestcliffEelgrassDir,i),pattern=paste0(i,"\_GAPE.csv"))}

#gapecombo=paste0(ConcatGapeFiles('../data',
myTimeZone="UTC",verbose=FALSE),serialnumber[i],"combogape.csv")

#use "serialnumber" (every row is serial number to be processed) of all
serial numbers in field. in this directory, look for folder "SN107" then
get list of all gape files in that folder and process them. with dir
function can find folder names using something like pattern= "SN107".
#make for loop go through vector. #at end get calculated ranges for
sensors. #paste0(gapeDir(###place where it
saves),serialnumber[i],"combogape.csv") #if there's no folder for serial
number (haven't downloaded data yet) then have it end the loop and go
back to the start and try the next number

```         

```{#r extraconcatGapeFunction}
#' Concatenate multiple daily gape files into one data frame
#' 
#' @param filenames A vector of filenames (including path) to be concatenated. The input files should contain a column named DateTime that will be converted to a POSIXct timestamp class. 
#' @param myTimeZone Specify the time zone of the timestamp data in the imported DateTime column. Default = UTC
#' @param verbose A logical argument specifying whether to show verbose progress output during file ingestion
#'  @return A data frame consisting of the input files concatenated and sorted chronologically. 

# ConcatGapeFiles <- function(filenames, myTimeZone = 'UTC',verbose=FALSE){
#   if(verbose){
#       pb = txtProgressBar(min=0,max = length(filenames), style = 3)
#   }
# # Open the raw data files and concatenate them.
#   for (f in 1:length(filenames)){
#       if(verbose) setTxtProgressBar(pb,f)
#       
#       dattemp = read.csv(filenames[f])
#       ###########################
#       # Columns:
#       # POSIXt: elapsed seconds since 1970-01-01 00:00:00 (unix epoch) in whatever timezone the sensor was set to during deployment. Presumably UTC
#       # DateTime: human-readable character date and time, in whatever timezone the sensor was set to during deployment 
#       # Hallx: Raw analog to digital converter value for the indicated Hall effect channel. There are 16 channels (numbered 0-15) on the GapeTracker
#       # Battery.V:  Supply battery voltage
#       #########################
#       # Convert the DateTime column to a POSIXct object.
#       dattemp$DateTime = as.POSIXct(dattemp$DateTime, tz=myTimeZone)  
#       
#       
#       # Concatenate the data files. 
#       if (f == 1){
#           dat = dattemp
#       } else if (f > 1){
#           dat = rbind(dat,dattemp)
#       }
#   }
#   if(verbose) close(pb)
# # Reorder the concatenated data frame by the DateTime values in case the files
# # were not fed in in chronological order
#   dat = dat[order(dat$DateTime),]
# }
```

\`\`\`{#rextra convert hall to hallpercent } #gapeSN = 'SN21'\
