---
title: "Newport_hr"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r knitroptions, echo=FALSE, cache=FALSE}
options(replace.assign = TRUE, width = 60)
options(digits.secs = 3)  # Needed to show and keep milliseconds values
knitr::opts_chunk$set(tidy = FALSE)
#knitr::opts_chunk$set(fig.path = 'figs/', cache.path = 'cache/', 
#		fig.width = 5, fig.height = 5, dpi = 600,
#		cache = TRUE, par = TRUE)

evalAll = FALSE # Set true to run all the chunks, even the long ones
# Setting this true will re-generate all of the concatenated files from the
# raw data files and re-run the gape sensor calibration and percentage 
# conversion steps, then save new concatenated csv files.
showcode = FALSE # Set true to print out all R code in final document
```

#libraries
```{r loadLibraries, message=FALSE}
library(signal)
library(pracma)
library(forecast)
library(lubridate)
library(car)
library(dplyr)
library(ggplot2)
library(Hmisc)
library(ggpattern)
library(forcats)
library(caTools)
library(ROCR)
library(nlme)
library(ggfortify)
library(betareg) #betareg stats
library(multcomp) #tukey for betareg
library(lme4) #binomial generalized linear mixed model
library(grid) #textGrob
```

#import files
```{r fileLocations, echo=FALSE}
# Directory path on Lauren's machine
setwd("~/Documents/Oyster/Rcode")

# Comment out one of the two localpath lines below depending on what computer you're on
#localpath = '../../data/'  # Luke's path
localpath = '../data/'  # Lauren's path

hrpath = '../data/Newport_SSINP/alldata/'
comboheartpath = paste0(localpath,'Newport_SSINP/combinedheart/')
heartoutputpath = paste0(localpath,'Newport_SSINP/heart_rates/')
NewportDir = paste0(localpath,'Newport_SSINP/')

#meta file with serial numbers
metafilePath = paste0(localpath,'Newport_SSINP/metadata/')

#tides
tides=read.csv("../data/Newport_SSINP/environmental_data/LosAngeles_tides_202205-202307.csv")
tides$DateTime=as.POSIXct(tides$TimeUTC, tz="UTC", format="%Y-%m-%d %H:%M")

#precipitation
precip=read.csv("../data/Newport_SSINP/environmental_data/Weather_data_KSNA_202205-202307.csv")
precip$DateTimeUTC=as.POSIXct(precip$DateTimeUTC, tz="UTC", format="%Y-%m-%d %H:%M")

#plots
plots=paste0(localpath,'Newport_SSINP/plots/')
```

#Functions: loadFieldMetaData, loadFieldMaintData
```{r loadFieldMetaData}
########################################
# loadFieldMetaData function
#' Load a csv data file containing deployment metadata and maintenance data
#' 
#' The deployment metadata file should contain entries for each oyster and 
#' its associated hall effect sensor channel for a given period of time denoted 
#' by the timestamps in 2 columns titled StartIncludeUTC and EndIncludeUTC, which
#' are assumed to have Excel-formatted date and time stamps in the UTC time zone
#' marking the start and end of each known-good deployment period (thus ignoring
#' time periods when the sensors were pulled from the mooring for maintenance or
#' other interruptions). 
#' 
#' @param filename The path and filename of the metadata csv file
#' @param timezone The timezone of the timestamp data in the metadata file. 
#' Default = UTC.
#' @return A data frame containing the same original columns as the metadata 
#' file, but with timestamps formatted as POSIXct values in the appropriate
#' time zone. 

loadFieldMetaData <- function(filename, timezone = 'UTC'){
	metadata = read.csv(file= filename)
	metadata$StartIncludeUTC = as.POSIXct(metadata$StartIncludeUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	metadata$EndIncludeUTC = as.POSIXct(metadata$EndIncludeUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
		metadata$timeofdeath = as.POSIXct(metadata$timeofdeath,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	metadata # return data frame
}

loadFieldMaintData <- function(filename,timezone = 'UTC'){
	maintdata = read.csv(file=filename)
	maintdata$StartMaintUTC = as.POSIXct(maintdata$StartMaintUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	maintdata$EndMaintUTC = as.POSIXct(maintdata$EndMaintUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	maintdata # return data frame
}

```

#themes
```{r}
theme_classicmodify <- function(base_size = 11, base_family = "",
                          base_line_size = base_size / 22,
                          base_rect_size = base_size / 22) {
  theme_bw(
    base_size = base_size,
    base_family = base_family,
    base_line_size = base_line_size,
    base_rect_size = base_rect_size
  ) %+replace%
    theme(
      # no background and no grid
      panel.border     = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text = element_text(size = 15), 
      axis.title = element_text(size = 20),
      # show axes
      axis.line      = element_line(colour = "black", linewidth = rel(1)),

      # match legend key to panel.background
      legend.key       = element_blank(),

      # simple, black and white strips
      strip.background = element_rect(fill = "white", colour = "black", linewidth = rel(2)),
      # NB: size is 1 but clipped, it looks like the 0.5 of the axes

      complete = TRUE
    )
}

theme_classicmodifygrid <- function(base_size = 11, base_family = "",
                          base_line_size = base_size / 22,
                          base_rect_size = base_size / 22) {
  theme_bw(
    base_size = base_size,
    base_family = base_family,
    base_line_size = base_line_size,
    base_rect_size = base_rect_size
  ) %+replace%
    theme(
      # no background and no grid
      panel.border     = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_line(colour="grey"),
      axis.text = element_text(size = 15), 
      axis.title = element_text(size = 20),
      # show axes
      axis.line      = element_line(colour = "black", linewidth = rel(1)),

      # match legend key to panel.background
      legend.key       = element_blank(),

      # simple, black and white strips
      strip.background = element_rect(fill = "white", colour = "black", linewidth = rel(2)),
      # NB: size is 1 but clipped, it looks like the 0.5 of the axes

      complete = TRUE
    )
}
```

#graphing settings
```{r}
sitePalette <- c("#E69F00", "#56B4E9", "#009E73", "#0072B2")
sitePalette2 <- c("#E69F00", "#E69F00","#56B4E9","#56B4E9", "#009E73", "#009E73","#0072B2","#0072B2")
treatment2_palette = c("#C9C9C9", "#999999", "#70DCBE", "#009E73", "#DC9867", "chocolate4")
treatment_palette=c("#999999", "#009E73", "chocolate4")
treatmentidentical_palette=c("#999999","#999999","#009E73","#009E73","chocolate4","chocolate4")

# Function to scale secondary axis
scale_function <- function(x, scale, shift){
  return ((x)*scale - shift)
}

# Function to scale secondary variable values
inv_scale_function <- function(x, scale, shift){
  return ((x + shift)/scale)
}
my_comparisons_season2=list(c("Winter","Summer"),c("Fall","Winter"),
  c("Winter","Spring"))
my_comparisons_season=list(c("Fall","Summer"),c(
  "Winter","Summer"),c("Spring","Summer"),c("Fall","Winter"),c("Fall","Spring"),c("Winter","Spring"))
my_comparisons_treat2=list(c("Oyster","Eelgrass"),c("Oyster","Mud"),c(
  "Eelgrass","Mud"))
my_comparisons=list(c("Winter","Summer"),c("Fall","Winter"),c("Winter","Spring"))
my_comparisons_site=list(c("DeAnza","PCH"),c("DeAnza","Shellmaker"),c(
  "DeAnza","Westcliff"),c("PCH","Shellmaker"),c("PCH","Westcliff"),c("Shellmaker","Westcliff"))
```

#import metadata
```{r}
#import csv metadata_with edits. 
metafile = loadFieldMetaData(filename = (paste0(metafilePath,"field_metadata_forcoding - Sheet1.csv")))

#import csv maintenance
maintenance=loadFieldMaintData(filename = (paste0(metafilePath,"lowtide_metadata - Sheet1.csv")))

oystercode = unique(metafile[,'Code'])

#import growth measurements
growth=read.csv(paste0(metafilePath, "growth_measurements - Sheet1.csv"))

#import allgapemeta
allgapemeta=read.csv(paste0(NewportDir,"combinedgapemeta/", "allgapemeta.csv"))
#change datetime to posixct
allgapemeta$DateTime=as.POSIXct(allgapemeta$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")

#import allhrgapemeta
allhrgapemeta=read.csv(paste0(metafilePath,"/allhrgapemeta.csv"),stringsAsFactors = T)
allhrgapemeta$DateTime=as.POSIXct(allhrgapemeta$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")

#import allhrmeta
allhrmeta=read.csv(paste0(metafilePath,"/allhrmeta.csv"),stringsAsFactors = T)
allhrmeta$DateTime=as.POSIXct(allhrmeta$DateTime, tz="UTC", format="%Y-%m-%d %H:%M:%S")

```

#create combined heart files
```{r importConcatenateRawHeartFiles, echo=showcode, eval=FALSE}
# Loop through all oysters and 
# Ideally you only need to run this once in a while as new data come 
# in from the field. Otherwise, use the output files from this chunk
# in the later analysis steps.
##################################################################
serialnumber=unique(metafile[,1])
oystercode = unique(metafile[,'Code']) #make list of all oyster codes 
#oystercode=oystercode[-c(1:84)] #manually change which codes are combined 
options(digits.secs = 3)


pb = txtProgressBar(min = 0, max = length(oystercode), style = 3)
counter = 0
#oystercode = 'D.E.2.1028'  # test line, for SN150
#oy="W.M.1.1026"
for (oy in (oystercode) ){ 
	counter = counter + 1
	setTxtProgressBar(pb, counter)
	if(exists('combineddat')) { rm(combineddat) }
	
	# Handle the cases where an oyster (ID'd by 4-digit value at end of oystercode) moves
	# between positions within the same treatement, by simply looking for the matching
	# 4-digit oyster code and ignoring the site/treatment/position codes
	serialnumber = metafile[which(substr(metafile$Code,7,10) == substr(oy,7,10)),'SN']
	loops = 1
	
	for (sn in (serialnumber)){
	  if(exists('Heart')) {rm(Heart)} # clear old version of Heart
		# In metadata file, figure out the start and end dates for this serial number
		# when it was associated with this oyster code
		startDate = metafile[which(metafile$SN == sn & 
								substr(metafile$Code,7,10) == substr(oy,7,10)),'StartIncludeUTC']
		endDate = metafile[which(metafile$SN == sn & 
								substr(metafile$Code,7,10) == substr(oy,7,10)),'EndIncludeUTC']
		rawdatapath=file.path(paste0(NewportDir,"alldata/",sn))
		if(!file.exists(rawdatapath)) {
			next
		}
		# generate list of file names
		myfiles = dir(rawdatapath, full.names=TRUE, pattern = '*IR.csv')
#		gapecombo=dir(path=rawdatapath, full.names = TRUE, pattern=paste0(i,"_GAPE.csv"))
		if(length(myfiles)==0){
			next # if there are no data files found, skip to next serial number
		}
		
		# pull HR data and put in Heart
		for (i in 1:length(myfiles)){
			if(exists('temp')) {rm(temp)} # clear old version of temp
			
			hdr  = scan(myfiles[i], nlines = 1, what = character(),sep = ',', quiet = TRUE)
			# Use the try() function to handle cases where the input file
			# might be empty
			try(temp <- read.table(myfiles[i],skip = 1, header = FALSE, 
							sep = ',', na.strings=c("","NA")), silent=TRUE)
			if (!exists('temp')){
				next  # If the data frame temp wasn't created, skip the rest 
					    # of this iteration of the loop
			}
			
			colnames(temp) = hdr
			temp$DateTime  = as.POSIXct(temp$DateTime, tz = 'UTC', format="%Y-%m-%d %H:%M:%S")
			
			# The original data files only had a timestamp every 240 rows
			# at the start and end of a 240-sample period. It might be useful
			# to fill in the intervening time values
			# To preserve this information in csv files, you need to have
			# set R's options to show 3 digits on time values: 
			# options(digits.secs = 3)
			################################################################
			# Get all rows that have a non-NA timestamp value in them
			goodvals = which(!is.na(temp$DateTime))
			# Get the subset where the last digit of the row number is a 1
			# rather than a 0, since these are the rows at the start of 
			# each 240-sample chunk (row 1, row 241, row 481 etc.)
			startRows = goodvals[which((goodvals %% 10)== 1)]
			# Go through the 240 values and add an incrementing milliseconds
			# value onto the timestamp.
			# Add 0.125 seconds to each reading (8Hz sample rate). Very last reading
			# is the last millisecond in 30 seconds. (29.875seconds).
			for (k in 1:length(startRows)){
				nextStart = startRows[k]
				# Generate a set of 240 timestamps incrementing by 0.125sec
				tvals = seq(temp$DateTime[nextStart], by = 0.125, length.out = 240)
				temp$DateTime[nextStart : (nextStart+240-1)] = tvals
			}
			
    	if (!is.null(endDate) & !is.na(endDate)){
    	  temp = temp[temp$DateTime>=startDate & temp$DateTime<=endDate, ]
    	} else if (is.null(endDate) | is.na(endDate)){
    	  # If no endDate was supplied, just trim data before start Date
    	  temp = temp[temp$DateTime>=startDate, ]
    	}
    	
    	if(nrow(temp)==0){
		    next
		  }
			
			
			# if (i == 1) {
			# 	Heart = temp
			# } else if (i > 1){
			# 	Heart = rbind(Heart,temp)
			# }
			
# 		if(length(temp)!=0){
		      if(!exists('Heart')){
    			#if (i == 1) {
    				Heart = temp
    			} else if (exists('Heart')){
    			#} else if (i > 1){
    				Heart = rbind(Heart,temp)
    			}
# 		  } else if(length(temp)==0){
# 		    next
# 		  }
		}
		
		if (loops == 1 & exists('Heart')){
			combineddat = Heart
			loops = loops + 1
		} else if (loops > 1 & exists('Heart')) {
			combineddat = rbind(combineddat,Heart)
		} else if (!exists('Heart')){
		  next
		}
	} #end of serial number looping
	
	if (exists('combineddat')){
		if (nrow(combineddat) > 0){
			Heart = combineddat # copy back over to this data frame for further use
			# Make sure the values are all in chronological order, in case
			# some raw files were imported out of order when an oyster switched
			# serial numbers
			Heart = Heart[order(Heart$DateTime),]
			
    	# Go to the low tide servicing metadata file and get the relevant
    	# start and end times for low tide service periods for the site/treatment
    	# that this oyster is at (contained in the Code column)
    	sitecode = substr(oy,1,1)
    	treatmentcode=substr(oy,3,3)
    	# Go to low tide service metafile and find all rows that match
    	# this sitecode (and treatment code) and get the list of those row numbers
    	# Then cycle through those row numbers, in each case grabbing the start time
    	# and end time of the low tide servicing. Then go to tempoutput and find
    	# any rows that are > start time and < end time, and convert NAs. 
    	services = which( (maintenance$sitecode == sitecode) & (maintenance$treatmentcode == treatmentcode) )
      	for (k in (services) ){
      	  # Find the matching time stamps and write NAs in between them
      	  startTime = maintenance$StartMaintUTC[k]
      	  endTime = maintenance$EndMaintUTC[k]
      	  # Take all rows that are between maintenance times in Heart and make 
      	  # IR values NA. Can go back through later and remove all NAs in "IR"
      	  Heart[which((Heart$DateTime > startTime) & (Heart$DateTime < endTime)),"IR"] = NA
      	}
	    startinclude=which(metafile$Code == oy) #find oy
	    mystarts = metafile$StartIncludeUTC[startinclude] # find first DateTime for oy in the field
	    mystarts = mystarts[order(mystarts)] #order datetimes 
	    Heart=Heart[which(Heart$DateTime>mystarts[1]),] #pull out rows that are after first time
	    # point of mystarts
			# Figure out if there's a time of death associated with this oyster code. Get all of
			# the rows that match this oyster code and pull the contents of the timeofdeath column
			deathrows = metafile[which(substr(metafile$Code,7,10) == substr(oy,7,10)),'timeofdeath']
			# Check to see if any of the values in deathrows is not an NA
			if (length(which(!is.na(deathrows))) > 0) {
				# If we found a non-NA value in deathrows, grab it and store it in timeofdeath
				timeofdeath = deathrows[which(!is.na(deathrows))]
			# If there was a time of death listed, truncate the data set there
				Heart = Heart[which(Heart$DateTime <= timeofdeath),]
			} else {
				# There were no non-NA values in deathrows, so set timeofdeath to NA
				timeofdeath = NA
			}			
			#Keep all rows without NA in IR column
			Heart=Heart[!is.na(Heart$IR),]
			
			write.csv(Heart, paste0(comboheartpath, oy, "_comboheart.csv"), 
					row.names = FALSE)
		}
	}
	
} # end of oystercode for loop 
close(pb)

```

#getBPM function
```{r getBPMfunction,echo=showcode}
# A function to convert raw heartbeat timeseries into estimates of heart rate
# (beats per minute). This function uses the filtfilt routine from the signal
# library and a butterworth filter. If you specify a particular frequency 
# for the lowband (and optionally highband), it will effectively filter
# at half that frequency. For example, if you specify a lowband value of 
# 1/10 (0.1 = 10 Hz), you get a lowpass filter that cuts off signals above around 5Hz.
# If you specify 1/20 (0.05 = 5Hz), the lowpass filter filters anything above
# about 2.5Hz.

#' @param x A data frame with columns DateTime and IR for sensor data
#' @param t1 The starting index of data to be subset from x
#' @param sensor Column name in x declaring the sensor that you want to process
#' @param chunklength Number of samples to process, default = 240 is 30 seconds at 8Hz for BivalveBit loggers
#' @param lowband Frequency for the lower frequency of the bandpass filter, consider values from 1/20 to 1/300
#' @param highband Frequency for the upper frequency of the bandpass filter, consider values from 1/2 to 1/20. Set to NULL to 
#' implement a lowpass filter only
#' @param amplitudeThreshold Minimum peak to peak amplitude of the *filtered* heart signal
#' @param maxBPM Maximum beats per minute estimate, higher values will be flagged in the output
#' @param Fs Sampling interval in seconds (default 0.125 = 8Hz for BivalveBit loggers)
#' @param BPMqualityThreshold A numeric value indicating how close two heart rate estimates must be to be considered in agreement 
#' @param plot Logical value to show a plot or not. Default is FALSE.
#' @param myYlims Set of fixed y-axis limits for plots
#' 
#' 
#' @return A list containing 3 estimates of the beats per minute, along with
#' values indicating how close the 3 estimates agree and whether the signal may
#' be questionable.

getBPM = function(x,t1 = 1, sensor = 'IR',chunklength=240, 
		lowband = 1/100, 
		highband = 1/15, 
		amplitudeThreshold = 60,
		maxBPM = 80,
		Fs = 0.125,
		BPMqualityThreshold = 4,
		plot = FALSE,
		myYlims = NULL)
{
	temp = x[t1:(t1+chunklength-1),]  #Grab the chunk of data
	
	# 
	# Grab a chunk of data that's twice as long as we need
#	temp = x[t1:(t1+(chunklength*2)),]
	# Calculate time difference between each sample (milliseconds), should be 100
#	diffs = diff(temp$startMillis)
	# Identify any gaps in the data where the interval was greater than the sampling interval in milliseconds
#	missedReads = which(diffs > (1000 * Fs))
	
# Check if there are any sampling gaps (missedReads)
#	if (length(missedReads) > 0){
#		# Add on the final row as well
#		missedReads = c(missedReads, (nrow(temp)+1) )
#		# Calculate gap length between any missed reads (and the final read)
#		testgaps = diff(missedReads) 
#		if (missedReads[1] > chunklength){
#			# In this case, just grab the first 300 readings
#			temp = temp[1:chunklength,]
#		} else if (length(which(testgaps>=chunklength)) > 0){
#			# check if any of the testgaps values are >=300 (chunklength)
#			# Get the index in temp that is at the start of the long run
#			tempindx = missedReads[which(testgaps >= chunklength)]+1
#			# If a gap is > 300, grab the sample in that gap
#			temp = temp[tempindx:(tempindx+chunklength-1),]
#		} else if (length( which(testgaps>=chunklength) ) == 0) {
#			# In this case there may be multiple gaps in the time chunk, 
#			# so that there are no good contiguous chunks of 300 readings
#			# Return a data frame with NAs
#			temp[,sensor] = NA 
#			
##			tempindx = missedReads[length(missedReads)]+1
##			temp = temp[tempindx:(tempindx+chunklength-1),]
#			# It's also possible to end up here if you grabbed a chunk that
#			# ran off the end of the 600 samples and returned some NAs. That
#			# will be handled below
#			
#		} 
#	} else if (length(missedReads) == 0) {
#		# Subset down to exactly the chunk length (10Hz sample * 30 secs = 300 samples)
#		temp = temp[1:chunklength,]
#	}
	

	# Add milliseconds onto the timestamps
	temp$DateTimeMS = temp$DateTime
	# create a new set of timestamps that have millisecond values attached
	myvec=vector(mode="numeric", length=chunklength-1)
	myvec[1:length(myvec)]=Fs
	temp$DateTimeMS[2:nrow(temp)] = temp$DateTimeMS[1] + cumsum(myvec)

	
	# Test if there are any NAs in the heart rate values, if there are not
	# then proceed with the filtering and heart rate determination
	if ( length( which( is.na(temp[,sensor]) ) ) == 0){
		
		# Detrend the heartrate readings
		detrendedIR = pracma::detrend(temp[,sensor])	
		# Next look at the detrended values and look for spurious low values
		# For instance, a good heart signal might oscillate between -200 & +300
		# in the detrended data, and a spurious value might suddenly drop to 
		# -6000. 
		# Calculate the standard deviation of the data set, and then find values
		# that are more than 3 SD away from the mean (which should be ~zero in the
		# detrended data)
#		spuriousVals = which(abs(detrendedIR) > (3*sd(detrendedIR)) )

#		if ( length(spuriousVals) > 0) {
#			# Convert to NAs 
#			temp[spuriousVals,sensor] = NA
#			
#			for (i in 1:length(spuriousVals)){
#				indx = spuriousVals[i]  # Get the row index for this spurious value
#				if (indx == 1) {
#					# If the first value is spurious, replace it with a copy 
#					# of the next value
#					temp[indx,sensor] = temp[indx+1,sensor]
#				} else {
#				# Replace the spurious value with the average of the values immediately
#				# before and after the spurious value
##				temp[indx,sensor] = mean(c(temp[indx-1,sensor],temp[indx+1,sensor]))
#				temp[indx,sensor] = mean(temp[(indx-2):(indx+2),sensor], na.rm=TRUE)
#				}
#			}
#		}
#		rm(spuriousVals)

		
		# With the spurious values replaced by interpolated values, re-run the 
		# detrending routine
#		detrendedIR = pracma::detrend(temp[,sensor])
	} else {
		# If there were NAs, just define detrendedIR as NA so that later 
		# operations skip over this chunk of data
		detrendedIR = NA
	}

	

	if (length(which(is.na(detrendedIR))) == 0) {

	# Define a butterworth filter
	# Consider using the bandpass filter rather than just a lowpass filter
	# because of the tendency for the IR heartrate signal to drift up and 
	# down in relation to ambient light, which tends to induce low-frequency
	# shifts that then fool the spectral analysis routines when trying to 
	# identify the dominant frequency
		if (!is.null(highband)) {
			bf = signal::butter(3,W = c(lowband, highband), type = 'pass')	# bandpass filter
		} else if (is.null(highband)){
			bf = butter(3,W = lowband, type = 'low')  #  lowpass filter			
		}

		# Apply the filter to the detrended data chunk
		y = filtfilt(bf, x = detrendedIR)
		# Calculate the spectrum of the filtered data
		myfft2 = spectrum(y, plot = FALSE)
		# Take the peak frequency from the spectrum, divide by sampling
		# rate to convert to cycles per second
		mypeakfreq = myfft2$freq[which.max(myfft2$spec)] / Fs
		# Multiply by 60 seconds to get cycles (beats) per minute
		BPMfft = 60 * mypeakfreq
		# Calculate amplitude of filtered signal to denote weak or noisy signals
		amp = range(y)[2] - range(y)[1]
		# Use function from package 'forecast', returns peak period (not freq)
		forecastPeriod = forecast::findfrequency(y)  
		forecastFreq = 1/forecastPeriod # convert period to frequency
		BPMforecast = forecastFreq * 60 * (1/Fs) # convert frequency to beats per 
		# minute, based on the fact that the sampling period is Fs, and there are
		# 60 seconds in a minute

		# Set a flag for cases where the forecast:findfrequency estimate is 
		# extremely large, which happens when it can't find a clear heart signal
		BPMforecastflag = ifelse(BPMforecast > maxBPM, 'FAIL','OK')
		# Set a flag for cases where the detrended/filtered signal has a very
		# small amplitude, signaling that there may be no good heartbeat signal 
		WeakSignalFlag = ifelse(amp < amplitudeThreshold, 'FAIL','OK')
		# Use pracma package to find peaks. Note that at slower heart rates
		# this function tends to find the sub-peaks (akin to a P or T peak in a
		# human ECG trace) rather than just the main peaks (R peaks on a human). 
		# This happens based on what the bandpass filter lets through. 
		pracPeaks = pracma::findpeaks(x = y, nups = 5, minpeakdistance = 10)
		BPMprac = nrow(pracPeaks)*2
		
		# Calculate the difference between the estimated heart rates from the 
		# spectrum fft routine and the forecast::findfrequency routine. A 
		# small value indicates good agreement
		BPMagreeQuality.fft.forecast = ceiling(abs(BPMfft - BPMforecast))
		# Calculate difference between the forecast and pracma estimates
		BPMagreeQuality.prac.forecast = ceiling(abs(BPMprac - BPMforecast))
		# Calculate difference between fft and pracma estimates
		BPMagreeQuality.prac.fft = ceiling(abs(BPMprac - BPMfft))
		
		if (plot){
			## Plot the raw detrended signal
			if (!is.null(myYlims)){
				# Use the specified y limits
				plot(temp$DateTimeMS, detrendedIR, type = 'l', 
						main = '', xlab = 'Seconds', ylab = '',
						las = 1, ylim = myYlims)	
			} else if (is.null(myYlims)) {
				# No y limits specified, use plot defaults
				plot(temp$DateTimeMS, detrendedIR, type = 'l', 
						main = '', xlab = 'Seconds', ylab = '',
						las = 1)	
			}
			 
			points(temp$DateTimeMS, detrendedIR, col = 1, pch = 20, cex = 1)
			if (amp > amplitudeThreshold){
				lines(temp$DateTimeMS, y, col = 3, lwd = 2) # add the filtered signal as a green line
			} else if (amp <= amplitudeThreshold) {
				lines(temp$DateTimeMS, y, col = 2, lwd = 2) # add the filtered signal as a red line	
				warning('Weak signal')
			}
			points(temp$DateTimeMS[pracPeaks[,2]], y = pracPeaks[,1], col = 4, pch = 19)
			mtext(side = 2, text = 'Detrended IR signal', line = 2.5, cex = 1)
			mtext(side = 1, line = 3, text = paste(sensor, strftime(temp$DateTime[1], tz="PST8PDT")), cex = 0.8, adj = 1)
			mtext(side = 3, line = 3, text = paste0('pracma bpm: ', BPMprac))
			mtext(side = 3, line = 2, text = paste0('fft bpm: ', BPMfft))
			mtext(side = 3, line = 1, text = paste0('forecast filtered bpm: ', round(BPMforecast,1)))
			if (!is.null(highband)){
				mtext(side = 3, line = 0.01, 
						text = paste0('Bandpass - Lower pass: ', round(lowband,digits =3), ', upper pass: ', round(highband,digits = 3)),
						cex = 0.8)	
			} else if (is.null(highband)) {
				mtext(side = 3, line = 0.01, 
						text = paste0('Lowpass - Lower limit: ', round(lowband,digits =3)),cex = 0.8)	
			}
			
			
		}

		resultsList = list(DateTime = temp$DateTime[1],
				Sensor = sensor,
				BPMfft = round(BPMfft,1),
				BPMforecast = round(BPMforecast,1),
				BPMpeaks = BPMprac,
				BPMagreeQuality.fft.forecast = BPMagreeQuality.fft.forecast,
				BPMagreeQuality.prac.forecast = BPMagreeQuality.prac.forecast,
				BPMagreeQuality.prac.fft = BPMagreeQuality.prac.fft,
				BPMforecastflag = BPMforecastflag,
				WeakSignalFlag = WeakSignalFlag,
				FilteredAmplitude = round(amp,1),
				finalBPM = NA,
				finalfilter = 1/highband,
				QA = FALSE)
		# If there is reasonable agreement between the 3 beats per minute estimates,
		# record the mean of the 3 values as the finalBPM
		if (resultsList$BPMagreeQuality.fft.forecast <= BPMqualityThreshold & 
				resultsList$BPMagreeQuality.prac.forecast <= BPMqualityThreshold &
				resultsList$BPMagreeQuality.prac.fft <= BPMqualityThreshold
				& resultsList$WeakSignalFlag == 'OK') {

			resultsList$finalBPM = round(mean(resultsList$BPMfft,
							resultsList$BPMforecast,
							resultsList$BPMpeaks), digits = 1)
		}
	} else if (length( which( is.na(temp[,sensor]) ) ) > 0) {
		# Handle the case where there are NAs in the data chunk that prevent
		# the filtering and fft routines
		resultsList = list(DateTime = temp$DateTime[1],
				Sensor = sensor,
				BPMfft = NA,
				BPMforecast = NA,
				BPMpeaks = NA,
				BPMagreeQuality.fft.forecast = NA,
				BPMagreeQuality.prac.forecast = NA,
				BPMagreeQuality.prac.fft = NA,
				BPMforecastflag = 'FAIL',
				WeakSignalFlag = 'FAIL',
				FilteredAmplitude = NA,
				finalBPM = NA,
				finalfilter = NA,
				QA = FALSE
				)
	}
	

	return(resultsList)
}

```

#extractRawIR function
```{r cleanupRawHeartForPlotting, echo=showcode}
# A function for grabbing a chunk of 300 samples at a given time stamp and 
# returning the detrended raw IR data, for plotting purposes primarily. 

#' @param tstamp POSIX time stamp in Pacific Standard Time zone
#' @param rawHeartdf Data frame of raw heart rate data from multiple sensors
#' @param mysensor Character string matching the name of the sensor column you want to extract 

#rawHeartdf = surfHeart

extractRawIRchunk = function(tstamp, rawHeartdf, mysensor, chunklength = 300)
{
	# In the "raw" data frame rawHeartdf, find the closest time stamp
	t1 = which.min(abs(tstamp - rawHeartdf$DateTimePST))
	# Grab the chunk of raw data. We'll assume that the timestamp should have
	# us starting at a section of 300 contiguous reads
	temp = rawHeartdf[t1:(t1 + (chunklength*2)),c(mysensor,'DateTimePST','startMillis')]
	diffs = diff(temp$startMillis)
	missedReads = which(diffs > 100)
# Check if there are any sampling gaps (missedReads)
	if (length(missedReads) > 0){
		# Add on the final row as well
		missedReads = c(missedReads, (nrow(temp)+1) )
		# Calculate gap length between any missed reads (and the final read)
		testgaps = diff(missedReads) 
		if (missedReads[1] > chunklength){
			# In this case, just grab the first 300 readings
			temp = temp[1:chunklength,]
		} else if (length(which(testgaps>= chunklength)) > 0){
			# check if any of the testgaps values are >= chunklength
			# Get the index in temp that is at the start of the long run
			tempindx = missedReads[which(testgaps >= chunklength)] + 1
			# If a gap is > 300, grab the sample in that gap
			temp = temp[tempindx:(tempindx+chunklength-1),]
		} else if (length(which(testgaps>=chunklength)) == 0) {
			# In this case there may be multiple gaps in the time chunk, 
			# so that there are no good contiguous chunks of 300 readings
			# Return a data frame with NAs
			temp[,sensor] = NA 			
		}
	} else if (length(missedReads) == 0) {
		# Subset down to exactly the chunk length (10Hz sample * 30 secs = 300 samples)
		temp = temp[1:chunklength,]
	}
	
# Add milliseconds onto the timestamps
	temp$DateTimeMS = temp$DateTimePST
	
	diffs = diff(temp$startMillis) / 1000
	temp$DateTimeMS[2:nrow(temp)] = temp$DateTimeMS[1] + cumsum(diffs)
	
# Handle cases where single very low values appear due to spurious values
# being recorded in the dataset. Replace with a linear interpolation of
# of the two neighboring values. First detrend the raw values
	detrendedIR = pracma::detrend(temp[,mysensor])
# Next look at the detrended values and look for spurious low values
# For instance, a good heart signal might oscillate between -200 & +300
# in the detrended data, and a spurious value might suddenly drop to 
# -6000. 
# Calculate the standard deviation of the data set, and then find values
# that are more than 3 SD away from the mean (which should be ~zero in the
# detrended data)
	spuriousVals = which(abs(detrendedIR) > (3*sd(detrendedIR)) )
	
	if ( length(spuriousVals) > 0) {
		# Convert to NAs 
		temp[spuriousVals,mysensor] = NA
		
		for (i in 1:length(spuriousVals)){
			indx = spuriousVals[i]  # Get the row index for this spurious value
			# Replace the spurious value with the average of the values immediately
			# before and after the spurious value
#				temp[indx,sensor] = mean(c(temp[indx-1,sensor],temp[indx+1,sensor]))
			temp[indx,mysensor] = mean(temp[(indx-2):(indx+2),mysensor], na.rm=TRUE)
		}
	}
	rm(spuriousVals)
# With the spurious values replaced by interpolated values, re-run the 
# detrending routine
	detrendedIR = pracma::detrend(temp[,mysensor])
	# Return just the vector of detrended IR data
	detrendedIR
}

```

#interactive plot function
```{r interactivePlotFunctions}
####################################################################################
# Creating basic interactive graphing routines that allow a user to press a
# particular key to create some response

# 2 functions to allow user interaction with a graph
readkeygraph <- function(prompt)
{
	getGraphicsEvent(prompt = prompt, 
			onMouseDown = NULL, onMouseMove = NULL,
			onMouseUp = NULL, onKeybd = onKeybd,
			consolePrompt = "Enter a key:\npress n = next (enters NAs) \nk = keep current values \nr = refilter \nl = less filtering \np = use pracma bpm \nf = use forecast bpm \nb = step back to previous time \nq to quit")
	Sys.sleep(0.01)
	return(keyPressed)
}

onKeybd <- function(key)
{
	keyPressed <<- key
}
# End of function definitions

```

## Filter heart rates from data logger
```{r AutoProcessHeartrates, echo=showcode, eval=evalAll}
# Go through the data files for a particular sensor and try to automatically
# estimate the heart rate
 
# Step through 1 minute at a time
#mytime = as.POSIXct('2020-12-22 12:00',tz='etc/GMT+8')

# TODO wrap the code below in a control structure that will let you specify
# an oyster code and then process that oyster's raw IR data
oystercode = unique(metafile[,'Code']) #make list of all oyster codes 
oystercode=oystercode[-c(1:23)] #manually change which codes are combined 
#skipped D.M.1.1102 bc error in resultlist$DPMagreeQuality.fft.fo argument is of length zero
oy="W.O.5.1016"
for (oy in (oystercode) ){ 
  if(exists('rawheartpath')) { rm(rawheartpath) }
  options(digits.secs = 3) # used to preserve milliseconds values in time stamps
  rawheartpath=file.path(paste0(comboheartpath,oy,'_comboheart.csv'))
  if(!file.exists(rawheartpath)) {
			next
		}
  Heart = read.csv(rawheartpath)

  Heart$DateTime = as.POSIXct(Heart$DateTime, tz = 'UTC', format="%Y-%m-%d %H:%M:%S")
  
  
  # Figure out where the start of each 30-second sampling bout should be. 
  # The sampling bouts were typically separated by 5 minutes, so we'll just
  # look for any time gap that's bigger than 60 seconds. 
  steps = diff(Heart$DateTime)
  bigsteps = which(steps > 60)
  bigsteps = bigsteps+1
  bigsteps = c(1,bigsteps) # A set of row indices in Heart for each new sample period
  
  pb = txtProgressBar(min = 0, max = length(bigsteps), style = 3)
  
  # mylowband- filters low freq signals out, gets rid of slow signals
  mylowband = 1/100  # filtering - ran some at 1/100 as a starting point.
  # 1/100 filters out more low freq stuff.Can go as low as 1/20, try 1/50 next
  #myhighband-get rid of high freq noise so you just get normal peaks. 
  # lower (1/20-->1/15) low band if have low long dips but actually have 
  # quick peaks in there. Bc the smaller it gets, then only lets 
  # through small range of freq
  myhighband = 1/10  # filtering - changing this has the largest effect, start around 1/15
  #1/5 letting more high freq peaks through. may also catch every little peak.
  myThreshold = 4 # Minimum agreement between BPM estimates
  
  for (i in 1:length(bigsteps)){
  	setTxtProgressBar(pb,i)
  #	t1 = which.min(abs(Heart$DateTime - bigsteps[i]))
  	# Test that we've hit the chosen minute (not just the closest time)
  #	if (difftime(bigsteps[i],Heart$DateTime[t1], units = 'secs') == 0){
  		
  		res1 = getBPM(Heart, t1 = bigsteps[i], sensor = 'IR', chunklength = 240,
  				lowband = mylowband, highband = myhighband, amplitudeThreshold = 20,
  				BPMqualityThreshold = myThreshold,
  				maxBPM = 80, Fs = 0.125, plot = FALSE)
  		if (i == 1) {			
  			SensorResult = as.data.frame(res1)
  			SensorResult[2:length(bigsteps),] = NA
  		} else {
  			SensorResult[i,] = as.data.frame(res1)
  		}
  		
  
  #	} else {
  #		SensorResult$DateTime[i] = timesteps[i]
  #
  #	}
  }
  close(pb)
  # Add a column with the oyster code
  SensorResult$OysterCode = oy 
  
  
  # Save an output file for faster loading in next chunks
  write.csv(SensorResult,
  		file = paste0(heartoutputpath,oy,'_bpm.csv'),  
  		row.names=FALSE)
}
```

```{r QAvisualInspectionSetup, echo=showcode, eval = evalAll}
# Go through the results lists and visually inspect timepoints where the
# heart rate was flagged as questionable. This may be a one-time thing. Run the
# code in this chunk to open the BPM data and extract the data for your 
# desired sensor channel ('mysensor'). Then run the code in the next chunk
# to do the manual inspection. You may save your progress at the end of the
# next chunk (manually) so that it will be available here the next time you
# reopen the data file. 

mysensor = 'IR'

oy = "D.O.1.1042"

# Open up the concatenated heart data for this oyster
Heart = read.csv(paste0(comboheartpath,oy,'_comboheart.csv'))
Heart$DateTime = as.POSIXct(Heart$DateTime, tz = 'UTC', format="%Y-%m-%d %H:%M:%S")

# Open up the output file from the heart rate auto-processing routine
if (length(dir(path = heartoutputpath, 
				pattern = paste0(oy,'[[:alnum:][:punct:]]*QA.csv'))) == 1){
	# If the QA'd version exists already, load that up
	Heartbpm = read.csv(paste0(heartoutputpath, oy,'_bpm_QA.csv'))
} else {
	# Reopen the original filtered results file, which hasn't had QA started yet
	Heartbpm = read.csv(paste0(heartoutputpath,oy,'_bpm.csv'))	
}


# Convert DateTime to POSIX
Heartbpm$DateTime = as.POSIXct(Heartbpm$DateTime, tz = 'UTC', format="%Y-%m-%d %H:%M:%S")
# The heart beat estimates and quality flags are stored in Heartbpm


# Define a BPM quality threshold. The BPM estimates from the various methods should 
# be less than this value, meaning they are in close agreement. 
BPMqualityThreshold = 4

# Figure out rows where there's disagreement among the 3 methods and also values haven't
# previously been QA'd. If the row already has been QA'd, this will ignore that row. 
# This will pull out any row where the disagreement
# between 2 of the methods is greater than the BPMqualityThreshold
# and the sensor hasn't been QA'd. This will catch rows with 
# weak signals though, which may be too many.
checkTheseRows = which( (Heartbpm$BPMagreeQuality.fft.forecast > BPMqualityThreshold |
			Heartbpm$BPMagreeQuality.prac.forecast > BPMqualityThreshold |
		Heartbpm$BPMagreeQuality.prac.fft > BPMqualityThreshold ) &
	Heartbpm$QA == 'FALSE')
# give every row where it's not all NAs, include good HR estimates and ones that
# need to be QAd
# checkTheseRows = which( (!is.na(Heartbpm$BPMfft)) & (!is.na(Heartbpm$BPMforecast)))

# This version requires the WeakSignalFlag field to be OK instead
# of FAIL. Useful in cases where large numbers (thousands) of 
# rows would be caught here, but most of them have such a weak signal
# that they're not even worth looking at. 
#checkTheseRows = which( (sensor$BPMagreeQuality.fft.forecast > BPMqualityThreshold | 
#				sensor$BPMagreeQuality.prac.forecast > BPMqualityThreshold | 
#				sensor$BPMagreeQuality.prac.fft > BPMqualityThreshold ) & 
#			sensor$QA == 'FALSE' &
#			  sensor$WeakSignalFlag == 'OK')
# Also get the rows with NAs
NArows = which(is.na(Heartbpm$BPMfft) & is.na(Heartbpm$BPMforecast))
# Combine the two sets of bad row indices
badRows = c(checkTheseRows,NArows)

counter = 1 # Start at the first entry in checkTheseRows

if (exists('ManualStart')){
	# This won't run automatically, but you can use it to find a starting point in the middle 
	# of the dataset to start at, if you don't want to start at the first entry
	myTime = as.POSIXct('2022-08-22 10:52:30', tz = 'etc/GMT+8')
	counter = which.min(abs(myTime - Heartbpm$DateTime[checkTheseRows]))
}

```

```{r QAmanualVisualInspection,echo=showcode,eval=evalAll}
# On a Mac in Rstudio, before you run this chunk, run the following on the command line:
X11(type='Xlib')
# On a Windows machine in Rstudio, you may instead need to run this line:
# dev.new()
# The goal is to open a plotting window outside of Rstudio that can accept user input

# The value 'counter' should be defined in the previous chunk

quitFlag = FALSE
skipFlag = FALSE
ylims = c(-100,100)
ylims=NULL
amplitudeThresh = 10
chunkLen = 240
myFs = 0.125

# You should have 
sensor = Heartbpm  # make a copy of the original data to work on


while (!quitFlag){
	# Get the row index of the next questionable row
	thisrow = checkTheseRows[counter]
	
	# Get the time stamp of the current row
	tstamp = sensor$DateTime[thisrow]
	currentBPM=sensor$finalBPM[thisrow]
	QAd=sensor$QA[thisrow]
	t1 = which.min(abs(tstamp - Heart$DateTime))
	# Also get the rows for the sample periods before and after the target minute
	t0 = which.min(abs( (tstamp-300) - Heart$DateTime))
	t2 = which.min(abs( (tstamp+240) - Heart$DateTime))
	
	HighBandDenom = 10  # Initial value should be 15 to make a 1/15 highband cutoff
	#Was 25
	
	# Make the first version of the plot
	op = par()
	par(mfcol = c(1,3), mar = c(5,5,5,1))
	# Plot the prior minute's data for context
	junk = getBPM(Heart,t1 = t0, sensor = mysensor, chunklength = chunkLen,
			lowband = 1/200, highband = 1/10, amplitudeThreshold = amplitudeThresh,
			maxBPM = 50, Fs = myFs, plot = TRUE, myYlims = ylims)
	if (is.na(junk$BPMfft) & is.na(junk$BPMforecast) & is.na(junk$BPMpeaks)){
		# If all of these were NA, there's nothing to plot
		plot.new()
		box()
		text(x=0.5,y=0.5, labels = 'No good data')
	} 
	# Plot the one we're interested in
	res1 = getBPM(Heart,t1 = t1, sensor = mysensor, chunklength = chunkLen,
			lowband = 1/100, highband = 1/HighBandDenom, amplitudeThreshold = amplitudeThresh,
			maxBPM = 50, Fs = myFs, plot = TRUE, myYlims = ylims)
	#Show currently recorded final BPM (may be NA)
	mtext(side=3, line=-2, text=paste("Recorded BPM: ", currentBPM), adj=0)
	#show if previous chunk was QAd 
	mtext(side=3, line=-3, text=paste("Previous QA: ", ifelse(QAd==TRUE, "TRUE", "FALSE")), adj=0)
	# Sanity check, if nothing gets plotted then we need to skip
	if (is.na(res1$BPMfft) & is.na(res1$BPMforecast) & is.na(res1$BPMpeaks)){
		# If all of these were NA, there's nothing to plot
		plot.new()
		box()
		text(x=0.5,y=0.5, labels = 'No good data')
		skipFlag = TRUE
	} 
	mtext(side = 1, text = paste0(counter,'/',length(checkTheseRows)), adj = 0, line = 2.5)
	# Plot the next minute's data as well, just for context
	junk = getBPM(Heart,t1 = t2, sensor = mysensor, chunklength = chunkLen,
			lowband = 1/100, highband = 1/15, amplitudeThreshold = amplitudeThresh,
			maxBPM = 50, Fs = myFs, plot = TRUE, myYlims = ylims)
	if (is.na(junk$BPMfft) & is.na(junk$BPMforecast) & is.na(junk$BPMpeaks)){
		# If all of these were NA, there's nothing to plot
		plot.new()
		box()
		text(x=0.5,y=0.5, labels = 'No good data')
	} 
	
	keyPressed = '' # initialize this so that it exists
	while (keyPressed != 'q'){
		# First, if the data for the desired time had too many missing values
	  # and skipFlag is true, handle that case and skip over this time point
		if (skipFlag){
			skipFlag = FALSE
			next
		}
		
		# Use the readkeygraph() function to prompt the user for input on the graph
		keyPressed = readkeygraph(prompt = "Enter a key: n = next (enters NAs), k = keep [average] 3 BPM values, r = refilter, l = less filtering, p = use pracma bpm, f = use fft bpm, b = step back to previous time, 0 = flatline signal, s = save and quit, q = quit without saving")
		keyPressed # print the resulting key press, this will be a character value
		# At this point interpret the resulting key press and cause some
		# decision to be made 
		if (keyPressed == 'n'){
			# User typed n, set the BPM values and flags to NA, this chunk isn't usable
			sensor[thisrow,3:8] = NA
			sensor$finalBPM[thisrow] = NA
			sensor$QA[thisrow] = TRUE
			keyPressed = 'q' # break out of the while loop
		} else if (keyPressed == 'k') {
			# User typed k, keep the current values and calculate a final BPM value
			sensor$finalBPM[thisrow] = round(mean(res1$BPMfft, res1$BPMforecast,res1$BPMpeaks),1)
			sensor$QA[thisrow] = TRUE
			# Store the final highband filter value used as well
			sensor$finalfilter[thisrow] = 1/HighBandDenom
			keyPressed = 'q'  # Set to q to quit this round of the while loop
		} else if (keyPressed == 'r') {
			# Refilter more aggressively
			HighBandDenom = HighBandDenom + 5 
			junk = getBPM(Heart,t1 = t0, sensor = mysensor, chunklength = chunkLen,
					lowband = 1/100, highband = 1/15, amplitudeThreshold = amplitudeThresh,
					maxBPM = 80, Fs = myFs, plot = TRUE, myYlims = ylims)
			res1 = getBPM(Heart,t1 = t1, sensor = mysensor, chunklength = chunkLen,
					lowband = 1/100, highband = 1/HighBandDenom, amplitudeThreshold = amplitudeThresh,
					maxBPM = 80, Fs = myFs, plot = TRUE, myYlims = ylims)
			junk = getBPM(Heart,t1 = t2, sensor = mysensor, chunklength = chunkLen,
					lowband = 1/100, highband = 1/15, amplitudeThreshold = amplitudeThresh,
					maxBPM = 80, Fs = myFs, plot = TRUE, myYlims = ylims)
			keypressed = ''
			# now we return to the top of the while loop
		} else if (keyPressed == 'l') {
			# Filter less aggressively
		  if (HighBandDenom > 5){
		    	HighBandDenom = HighBandDenom - 5
		  } else if (HighBandDenom <= 5 & HighBandDenom > 3) {
		     HighBandDenom = HighBandDenom - 1
		  } else if (HighBandDenom <= 3){
		    HighBandDenom = 3
		  }

			junk = getBPM(Heart,t1 = t0, sensor = mysensor, chunklength = chunkLen,
					lowband = 1/100, highband = 1/15, amplitudeThreshold = amplitudeThresh,
					maxBPM = 50, Fs = myFs, plot = TRUE, myYlims = ylims)
			res1 = getBPM(Heart,t1 = t1, sensor = mysensor, chunklength = chunkLen,
					lowband = 1/100, highband = 1/HighBandDenom, amplitudeThreshold = amplitudeThresh,
					maxBPM = 50, Fs = myFs, plot = TRUE, myYlims = ylims)
			junk = getBPM(Heart,t1 = t2, sensor = mysensor, chunklength = chunkLen,
					lowband = 1/100, highband = 1/15, amplitudeThreshold = amplitudeThresh,
					maxBPM = 50, Fs = myFs, plot = TRUE, myYlims = ylims)
			keypressed = ''
			# Now we return to the top of the while loop
		} else if (keyPressed == 'p') {
			# User wants to assign the pracma-derived BPM value as the finalBPM
			sensor$finalBPM[thisrow] = res1$BPMpeaks
			# Store the final highband filter value used as well
			sensor$finalfilter[thisrow] = 1/HighBandDenom
			sensor$QA[thisrow] = TRUE
			keyPressed = 'q' # set this to move on to next time point
		} else if (keyPressed == 'f') {
			# User wants to use the fft-derived BPM value as the finalBPM
			sensor$finalBPM[thisrow] = res1$BPMfft
			# Store the final highband filter value used as well
			sensor$finalfilter[thisrow] = 1/HighBandDenom	
			sensor$QA[thisrow] = TRUE
			keyPressed = 'q' # set this to move to next time point
		} else if (keyPressed == 'b') {
			# User wants to go back to the previous plot, maybe they want to do that
			# one over again.
			counter = counter - 2 # Decrement by 2, because the end of the loop will increment by 1 again
			keyPressed = 'q' # Set to q to exit while loop			
		} else if (keyPressed == 'q'){
			quitFlag = TRUE # Set TRUE to exit outer while loop
		} else if (keyPressed == 's') {
			# Write the updated data back into the main data frame
			Heartbpm[which(Heartbpm$Sensor == mysensor),] = sensor
			# Save the data frame to a csv file
			write.csv(Heartbpm, file = paste0(heartoutputpath,oy,'_bpm_QA.csv'), 
					row.names=FALSE)
			# User wants to save progress and quit
			keyPressed = 'q' # Set to q to exit while loop
			quitFlag = TRUE # Set TRUE to exit outer while loop
		} else if (keyPressed == '0'){
		  sensor$finalBPM[thisrow] = 0
		  sensor$finalfilter[thisrow] = 1/HighBandDenom
		  sensor$QA = TRUE
		  keyPressed = 'q' # Set to q to exit while loop
		} else {
			# Key pressed wasn't one of the choices above, so do some default action
			mtext(side = 3, line = -2, text = "No sensible choice made")
		}
	}
	keyPressed = '' # reset the value
	counter = counter+1 # increment the counter
	if(counter > length(checkTheseRows)){
	  quitFlag = TRUE
	}
	par(op)
	
	
}

print(paste('Last row checked: ', checkTheseRows[i], ' time: ', sensor$DateTime[thisrow]))

# At the end of the loop, the 'sensor' data frame should have the 'finalBPM' values
# assigned in most rows (besides rows with bad data)
	

# userInput = readline(prompt = "Save progress? y or n: \n")
userInput = askYesNo(msg = "Save progress? y or n: \n")

if(userInput){
	# Write the updated data back into the main data frame
	Heartbpm = sensor
	# Save the data frame to a csv file
	write.csv(Heartbpm, file = paste0  (heartoutputpath,oy,'_bpm_QA.csv'), 
			row.names=FALSE)
} else {
  print("Updates not saved.")
}

```



#look for oysters with "good" heart rates super messy code, but it works ¯\_(ツ)_/¯ 
```{r}
#look at finalBPM. NA unless there was actually enough agreement and a strong enough signal. Otherwise look at the BPMagreeQuality.prac.fft column and see if values are close (like <4), since pracma and fft methods are usually the closest to agreeing even while the forecast method is showing 300+.
df = data.frame()
oystercode = unique(metafile[,'Code']) #make list of all oyster codes 
for (oy in (oystercode)){
  if(exists('bpmheartpath')) { rm(bpmheartpath) }
  options(digits.secs = 3) # used to preserve milliseconds values in time stamps
  bpmheartpath=file.path(paste0(heartoutputpath,oy,'_bpm.csv'))
    if(!file.exists(bpmheartpath)) {
  			next
  		}
  bpm = read.csv(bpmheartpath)
  bpm=table(bpm$finalBPM, useNA = "ifany") #get frequency of BPM values for each oyster
  bpmprop=prop.table(bpm) #calculate proportion of BPM values
  bpmpropdata=data.frame(bpmprop)
  output <- bpmpropdata$Freq[is.na(bpmpropdata$Var1)]#pull out Freq value for the NA row
  
  df=rbind(df, output)    
  newoy <- rep(oy, ncol(df))              # Create new row
  df[nrow(df) + 1, ] <- newoy              # Append new row
  
  write.csv(df,
  		file = paste0(heartoutputpath,'bpmprop_v2.csv'),  
  		row.names=FALSE)
}

df=read.csv(paste0(heartoutputpath,'bpmprop_v2.csv'))

df2 <- data.frame(
  "col1" = df[seq(1,length(df[,1]),2), "X0.797646211120452"], 
  "col2" = df[seq(2,length(df[,1]),2), "X0.797646211120452"])

write.csv(df2, 
          file=paste0(heartoutputpath, 'NAprop_v2.csv'),
          row.names=F)
```

#grab QAd HR data and put in one big csv
```{r}
#go through QAd HR files and pull out non NAs
dfheart = data.frame()
oystercode = unique(metafile[,'Code']) #make list of all oyster codes 
#oystercode=oystercode[-c(1:23)] #manually change which codes are combined 

for (oy in (oystercode) ){ 
  if(exists('rawQApath')) { rm(rawQApath) }
  options(digits.secs = 3) # used to preserve milliseconds values in time stamps
  rawQApath=file.path(paste0(heartoutputpath,oy,'_bpm_QA.csv'))
  if(!file.exists(rawQApath)) {
			rawQApath=file.path(paste0(heartoutputpath,oy,'_bpm.csv'))
			if(!file.exists(rawQApath)){
			  next
        } 
		}
  Heart = read.csv(rawQApath)
  HeartnotNA= Heart[complete.cases(Heart[ , c('finalBPM')]), ] 
  HeartnotNA$DateTime = as.POSIXct(HeartnotNA$DateTime, tz = 'UTC', format="%Y-%m-%d %H:%M:%S")
 
  dfheart=rbind(dfheart, HeartnotNA) 
    
  write.csv(dfheart,
  		file = paste0(heartoutputpath,'bpmall.csv'),  
  		row.names=FALSE)
}
```

#combine QAd HRs with gape data
```{r}
bpmQA <- read.csv(paste0(heartoutputpath,'bpmQA.csv'))
bpmall<-read.csv(paste0(heartoutputpath,'bpmall.csv'))
bpmQA$DateTime = as.POSIXct(bpmQA$DateTime, tz = 'UTC', format="%Y-%m-%d %H:%M:%S")
bpmQA$Code=bpmQA$OysterCode
bpmall$DateTime = as.POSIXct(bpmall$DateTime, tz = 'UTC', format="%Y-%m-%d %H:%M:%S")
bpmall$Code=bpmall$OysterCode

bpmQA2 <- subset(bpmQA,finalBPM > 2)
bpmall2 <- subset(bpmall,finalBPM > 2)

allhrmeta<-merge(bpmall2,metafile[, c("Site", "Treatment","Alive","Code")], by="Code")

allhrgapemeta=merge(allgapemeta[,c("Code","DateTime","Temp.C","hallpercent","binary")],
                    allhrmeta[,c("Code","DateTime","finalBPM",
                                 "Site","Treatment")],
                    by=c("DateTime","Code"))

allhrgapemeta=allhrgapemeta %>% distinct()
allhrmeta=allhrmeta %>% distinct()

#change order of treatment
allhrmeta$Treatment <- factor(allhrmeta$Treatment,levels = c("Oyster", "Eelgrass", "Mud"))
#change order of sites
allhrmeta$Site <- factor(allhrmeta$Site,levels = c("Westcliff", "PCH", "DeAnza", "Shellmaker"))
write.csv(allhrmeta, paste0(metafilePath,"/allhrmeta.csv"))
write.csv(allhrgapemeta, paste0(metafilePath,"/allhrgapemeta.csv"))
```

#average HR boxplots
```{r}
#NAs when datetime is 00:00:00 because DateTime column has NA for those timestamps
fourseasonshr=allhrmeta %>%
      mutate(Season = case_when( #new variable
      DateTime >= "2022-07-15 00:00:00" & DateTime <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
      DateTime >="2022-10-01 00:00:00" & DateTime <= "2022-12-31 23:59:00" ~ "Fall",
      DateTime >= "2023-01-01 00:00:00" & DateTime <= "2023-03-31 23:59:00" ~ "Winter",
      DateTime >= "2023-04-01 00:00:00" & DateTime <= "2023-06-19 23:59:00" ~ "Spring",
      TRUE ~ NA)) %>% 
  filter(!is.na(Season)) %>% #display error if a value is not assigned to one of the previous groups
  mutate_if(is.character, as.factor) %>% 
  mutate(Season = fct_relevel(Season,c("Summer","Fall", "Winter", "Spring")),
         Treatment= fct_relevel(Treatment,c("Mud","Oyster", "Eelgrass"))) %>% 
  distinct()

test2=fourseasonshr %>%
  group_by(lubridate::floor_date(DateTime, "day"),Treatment,Site,Season,Code) %>%
  mutate(rangebpm=max(finalBPM, na.rm=TRUE)-min(finalBPM, na.rm=TRUE))

seasonspercentopencodehr=fourseasonshr %>%
  group_by(lubridate::floor_date(DateTime, "day"),Treatment,Site,Season,Code) %>%
  mutate(rangebpm=max(finalBPM, na.rm=TRUE)-min(finalBPM, na.rm=TRUE)) %>% 
  ungroup() %>% 
  group_by(Treatment,Site,Season,Code) %>% 
  summarise(avgbpm=mean(finalBPM),rangebpm=mean(rangebpm))

seasonsavgsitehr=seasonspercentopencodehr %>% 
  group_by(Treatment,Season,Site) %>% 
  dplyr::select(-c("Code")) %>% 
  mutate(ntreats = n()) %>% 
  mutate(avgbpm=mean(avgbpm),rangebpm=mean(rangebpm)) %>% 
  distinct()

#group by season
ggplot(seasonsavgsitehr,aes(Season,avgbpm,fill=Season))+
  geom_boxplot()+
  stat_summary(fun=mean, geom="point", shape=23, size=5, color="black", fill="red")+
  theme_classicmodify()+
  labs(y="Average heart rate (bpm)")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")+
  stat_n_text(size=5,y.pos=15) + 
  ylim(0,16)

ggsave(path=plots, "hr_fourseasons_box_stats.png",width=6, height=6)

# Compute the analysis of variance
res.aov <- aov(avgbpm ~ Season, data=seasonsavgsitehr)
# Summary of the analysis
summary(res.aov)
#pvalue=0.685

#tukey pairwise
TukeyHSD(res.aov)
#None are sig

#test for homogeneity
leveneTest(avgbpm ~ Season, data=seasonsavgsitehr)
#equal variances p=0.4675
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
#pvalue=0.006502 not normal

kruskal.test(avgbpm ~ Season, data=seasonsavgsitehr)
#pvalue=0.803

#####################
#SITE
# Compute the analysis of variance
res.aov <- aov(avgbpm ~ Site, data=seasonsavgsitehr)
# Summary of the analysis
summary(res.aov)
#pvalue=0.271

#tukey pairwise
TukeyHSD(res.aov)
#All comparisons not sig

#or pairwise
pairwise.t.test(seasonsavgsitehr$avgbpm,seasonsavgsitehr$Site,
                 p.adjust.method = "BH")
#All comparisons not sig

#test for homogeneity
leveneTest(avgbpm ~ Site, data=seasonsavgsitehr)
#equal variances p=0.4461
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.005302 so not normal

kruskal.test(avgbpm ~ Site, data=seasonsavgsitehr)
#pvalue=0.1885

ggboxplot(seasonsavgsitehr, x = "Site", y = "avgbpm",
          color = "black", palette = sitePalette,fill="Site")+
  stat_summary(fun=mean, geom="point", shape=23, size=5, color="black", fill="red")+
  labs(y="Average heart rate (bpm)")+
  stat_n_text(size=5,y.pos=15) + 
  ylim(0,16)+
  theme(axis.text=element_text(size=15),
        axis.title=element_text(size=18),legend.text=element_text(size=12),legend.title=element_text(size=15))

ggsave("hr_season_stats_site.png",path=plots,width=6, height=5)

############
#TREATMENT

# Compute the analysis of variance
res.aov <- aov(avgbpm ~ Treatment, data=seasonsavgsitehr)
# Summary of the analysis
summary(res.aov)
#pvalue=0.926

#tukey pairwise
TukeyHSD(res.aov)
#All comparisons not sig

#or pairwise
pairwise.t.test(seasonsavgsitehr$avgbpm,seasonsavgsitehr$Treatment,
                 p.adjust.method = "BH")
#All comparisons not sig

#test for homogeneity
leveneTest(avgbpm ~ Treatment, data=seasonsavgsitehr)
#equal variances p=0.8861
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.00306 so not normal

kruskal.test(avgbpm ~ Treatment, data=seasonsavgsitehr)
#pvalue=0.8687

ggboxplot(seasonsavgsitehr, x = "Treatment", y = "avgbpm",
          color = "black", palette = c("chocolate4","#999999","#009E73"),fill="Treatment")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  labs(y="Average heart rate (bpm)")+
  #compare_means(avgpercentopen ~ Site,  data = test)+
  #stat_compare_means(method = "anova")+
  #stat_compare_means(label = "p.signif", method = "t.test",
  #                   ref.group = "Winter") +
  #stat_compare_means(comparisons = my_comparisons_site,label.y = c(1, 1.05, 1.13),aes(label = after_stat(p.signif)))+
  stat_n_text(size=5,y.pos=15) + 
  ylim(0,16)+
  theme(axis.text=element_text(size=15),
        axis.title=element_text(size=18),legend.text=element_text(size=12),legend.title=element_text(size=15))

ggsave("hr_season_stats_treatment.png",path=plots,width=6, height=5)

ggplot(data = seasonsavgsitehr,
       aes(x = Season, y = avgbpm, fill = Treatment))+
  geom_boxplot(col="black",position=position_dodge(.9))+
  stat_summary(position=position_dodge(.9),fun="mean", geom="point", shape=18, size=3, color="black")+
  theme_classicmodify()+
  labs(y="Average heart rate (bpm)")+
  ylim(0,16)+
  scale_fill_manual(values=c( "chocolate4","#999999", "#009E73"))+
  theme(legend.title=element_text(size=12),legend.text=element_text(size=12))

ggsave("hr_season_boxplot_treat.png",path=plots,width=6, height=5)

ggplot(seasonsavgsitehr,aes(x = Season, y = avgbpm, fill = Site))+
  geom_boxplot(col="black",position=position_dodge(.9))+
  stat_summary(position=position_dodge(.9),fun="mean", geom="point", shape=18, size=3, color="black")+
  theme_classicmodify()+
  labs(y="Average heart rate (bpm)")+
  ylim(0,16)+
  scale_fill_manual(values=sitePalette)+
  theme(legend.title=element_text(size=12),legend.text=element_text(size=12))

ggsave("hr_season_boxplot_site.png",path=plots,width=6, height=5)

####################
#combine avg hr season, site, treat

#group by season
seasonhrplot=ggplot(seasonsavgsitehr,aes(Season,avgbpm,fill=Season))+
  geom_boxplot()+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  theme_classicmodify()+
  labs(y="Average daily heart rate (bpm)",tag = "A")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")+
  stat_n_text(size=5,y.pos=15) + 
  ylim(0,16)+
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=18),legend.position="none")

sitehrplot=ggboxplot(seasonsavgsitehr, x = "Site", y = "avgbpm",
          color = "black", palette = sitePalette,fill="Site")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  labs(y="Average daily heart rate (bpm)",tag = "C")+
  stat_n_text(size=5,y.pos=15) + 
  ylim(0,16)+
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=18),legend.position="none")

treathrplot=ggboxplot(seasonsavgsitehr, x = "Treatment", y = "avgbpm",
          color = "black", palette = c("chocolate4","#999999","#009E73"),fill="Treatment")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  labs(y="Average daily heart rate (bpm)",tag = "B")+
  #compare_means(avgpercentopen ~ Site,  data = test)+
  #stat_compare_means(method = "anova")+
  #stat_compare_means(label = "p.signif", method = "t.test",
  #                   ref.group = "Winter") +
  #stat_compare_means(comparisons = my_comparisons_site,label.y = c(1, 1.05, 1.13),aes(label = after_stat(p.signif)))+
  stat_n_text(size=5,y.pos=15) + 
  ylim(0,16)+
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=18),legend.position="none")

figure=ggarrange(seasonhrplot+
                   theme(
                  axis.title.y = element_blank() ), 
           treathrplot+ 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank() ), 
          sitehrplot+ 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank() ) , nrow=1, ncol=3)

figure=annotate_figure(figure, left = textGrob("Average daily heart rate (bpm)", rot = 90, vjust = 1, gp = gpar(cex = 1.25)))
figure

ggsave(plot=figure,"avghr_seatreatsite.png",path=plots,height=4, width=9)


```

#range HR boxplots
```{r}
seasonsavgsitehr=seasonspercentopencodehr %>% 
  group_by(Treatment,Season,Site) %>% 
  dplyr::select(-c("Code")) %>% 
  mutate(ntreats = n()) %>% 
  mutate(avgbpm=mean(avgbpm),rangebpm=mean(rangebpm)) %>% 
  distinct()

#group by season
ggplot(seasonsavgsitehr,aes(Season,rangebpm,fill=Season))+
  geom_boxplot()+
  stat_summary(fun=mean, geom="point", shape=23, size=5, color="black", fill="red")+
  theme_classicmodify()+
  labs(y="Average daily heart rate range (bpm)")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")+
  stat_n_text(size=5,y.pos=15) + 
  ylim(0,16)

ggsave(path=plots, "rangehr_fourseasons_box_stats.png",width=6, height=6)

# Compute the analysis of variance
res.aov <- aov(rangebpm ~ Season, data=seasonsavgsitehr)
# Summary of the analysis
summary(res.aov)
#pvalue=0.33

#tukey pairwise
TukeyHSD(res.aov)
#None are sig

#test for homogeneity
leveneTest(rangebpm ~ Season, data=seasonsavgsitehr)
#not equal variances p=0.03096 
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals )
#pvalue=0.8223 normal

kruskal.test(rangebpm ~ Season, data=seasonsavgsitehr)
#pvalue=0.3582

#####################
#SITE
# Compute the analysis of variance
res.aov <- aov(rangebpm ~ Site, data=seasonsavgsitehr)
# Summary of the analysis
summary(res.aov)
#pvalue=0.67

#tukey pairwise
TukeyHSD(res.aov)
#All comparisons not sig

#test for homogeneity
leveneTest(rangebpm ~ Site, data=seasonsavgsitehr)
#not equal variances p=0.02936
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.8646 so normal

kruskal.test(rangebpm ~ Site, data=seasonsavgsitehr)
#pvalue=0.7885

ggboxplot(seasonsavgsitehr, x = "Site", y = "rangebpm",
          color = "black", palette = sitePalette,fill="Site")+
  stat_summary(fun=mean, geom="point", shape=23, size=5, color="black", fill="red")+
  labs(y="Average daily heart rate range (bpm)")+
  stat_n_text(size=5,y.pos=15) + 
  ylim(0,16)+
  theme(axis.text=element_text(size=15),
        axis.title=element_text(size=18),legend.text=element_text(size=12),legend.title=element_text(size=15))

ggsave("rangehr_season_stats_site.png",path=plots,width=6, height=5)

############
#TREATMENT

# Compute the analysis of variance
res.aov <- aov(rangebpm ~ Treatment, data=seasonsavgsitehr)
# Summary of the analysis
summary(res.aov)
#pvalue= 0.0195 Significant

#tukey pairwise bc passes assumptions
TukeyHSD(res.aov)
#Eelgrass sig diff from oyster

#or pairwise
pairwise.t.test(seasonsavgsitehr$rangebpm,seasonsavgsitehr$Treatment,
                 p.adjust.method = "BH")
#All comparisons not sig

#test for homogeneity
leveneTest(rangebpm ~ Treatment, data=seasonsavgsitehr)
#equal variances p=0.2516
plot(res.aov, 1)

#test for normality
plot(res.aov, 2)
#test with Shapiro-Wilk
# Extract the residuals
aov_residuals <- residuals(object = res.aov )
# Run Shapiro-Wilk test
shapiro.test(x = aov_residuals)
#pvalue=0.8951 so normal

my_comparisons_treat=list(c("Mud","Oyster"),c("Mud","Eelgrass"),c("Oyster","Eelgrass"))

ggboxplot(seasonsavgsitehr, x = "Treatment", y = "rangebpm",
          color = "black", palette = c("chocolate4","#999999","#009E73"),fill="Treatment")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  labs(y="Average daily heart rate range (bpm)")+
  #compare_means(avgpercentopen ~ Site,  data = test)+
  #stat_compare_means(method = "anova")+
  #stat_compare_means(label = "p.signif", method = "t.test",
  #                   ref.group = "Winter") +
  stat_compare_means(comparisons = my_comparisons_treat,label.y = c(11.2, 12.5, 14),aes(label = after_stat(p.signif)))+
  stat_n_text(size=5,y.pos=16.3) + 
  ylim(0,16.5)+
  theme(axis.text=element_text(size=15),
        axis.title=element_text(size=18),legend.text=element_text(size=12),legend.title=element_text(size=15))

ggsave("rangehr_stats_treatment.png",path=plots,width=6, height=5)

ggplot(data = seasonsavgsitehr,
       aes(x = Season, y = rangebpm, fill = Treatment))+
  geom_boxplot(col="black",position=position_dodge(.9))+
  stat_summary(position=position_dodge(.9),fun="mean", geom="point", shape=18, size=3, color="black")+
  theme_classicmodify()+
  labs(y="Average daily heart rate range (bpm)")+
  ylim(0,16)+
  scale_fill_manual(values=c( "chocolate4","#999999", "#009E73"))+
  theme(legend.title=element_text(size=12),legend.text=element_text(size=12))

ggsave("rangehr_season_boxplot_treat.png",path=plots,width=6, height=5)

ggplot(seasonsavgsitehr,aes(x = Season, y = rangebpm, fill = Site))+
  geom_boxplot(col="black",position=position_dodge(.9))+
  stat_summary(position=position_dodge(.9),fun="mean", geom="point", shape=18, size=3, color="black")+
  theme_classicmodify()+
  labs(y="Average daily heart rate range (bpm)")+
  ylim(0,16)+
  scale_fill_manual(values=sitePalette)+
  theme(legend.title=element_text(size=12),legend.text=element_text(size=12))

ggsave("rangehr_season_boxplot_site.png",path=plots,width=6, height=5)

##############
#range hr with site, treat, season
seasonrangeplot=ggplot(seasonsavgsitehr,aes(Season,rangebpm,fill=Season))+
  geom_boxplot()+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  theme_classicmodify()+
  labs(y="Average daily heart rate range (bpm)",tag = "A")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")+
  stat_n_text(size=5,y.pos=15) + 
  ylim(0,16)+
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=18),legend.position="none")

siterangeplot=ggboxplot(seasonsavgsitehr, x = "Site", y = "rangebpm",
          color = "black", palette = sitePalette,fill="Site")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  labs(y="Average daily heart rate range (bpm)",tag = "C")+
  stat_n_text(size=5,y.pos=15) + 
  ylim(0,16)+
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=18),legend.position="none")

treatrangeplot=ggboxplot(seasonsavgsitehr, x = "Treatment", y = "rangebpm",
          color = "black", palette = c("chocolate4","#999999","#009E73"),fill="Treatment")+
  stat_summary(fun=mean, geom="point", shape=23, size=4, color="black", fill="red")+
  labs(y="Average daily heart rate range (bpm)",tag = "B")+
  #compare_means(avgpercentopen ~ Site,  data = test)+
  #stat_compare_means(method = "anova")+
  #stat_compare_means(label = "p.signif", method = "t.test",
  #                   ref.group = "Winter") +
  #stat_compare_means(comparisons = my_comparisons_treat2,label.y = c(11.2, 12.5, 14),aes(label = after_stat(p.signif)))+
  geom_signif(comparisons = my_comparisons_treat2,map_signif_level = TRUE,y_position = c(11.2, 12.5, 14),annotation = c("*", "ns","ns"))+
  stat_n_text(size=5,y.pos=16.3) + 
  ylim(0,16.5)+
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=18),legend.position="none")

figure=ggarrange(seasonrangeplot+
                   theme(
                  axis.title.y = element_blank() ), 
           treatrangeplot+ 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank() ), 
          siterangeplot+ 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank() ) , nrow=1, ncol=3)

figure=annotate_figure(figure, left = textGrob("Average daily heart rate range (bpm)", rot = 90, vjust = 1, gp = gpar(cex = 1.25)))
figure
library(ggpubr)
ggsave(plot=figure,"rangehr_seatreatsite.png",path=plots,height=4.3, width=9)
```


#graph allhrgapemeta 
```{r}
ggplot(allhrgapemeta, aes(Treatment, Temp.C))+
  geom_boxplot()

#ANCOVA graph
ggplot(allhrgapemeta, aes(Temp.C,finalBPM, col=Code))+
  geom_point()+
  theme_classic()+
  geom_smooth()

ggsave(path=plots, "temp_hr_allcode.png",width=6, height=6)

ggplot(allhrgapemeta, aes(Site,finalBPM))+
  geom_boxplot(col="black",fill=sitePalette)+
  labs(y="Heart rate (bpm)")+
  theme_classicmodify()

ggsave(path=plots, "site_hr_all.png",width=6, height=6)

ggplot(allhrgapemeta, aes(Treatment,finalBPM))+
  geom_boxplot(col="black",fill=treatment_palette)+
  theme_classicmodify()+
  labs(y="Heart rate (bpm)")+
  theme()

ggsave(path=plots, "treat_hr_all.png",width=6, height=6)

```

#Raw hr vs gape logistic regression 
```{r}
#logistic regression plot
ggplot(allhrgapemeta, aes(finalBPM, binary)) + 
  geom_point() +
	stat_smooth(method="glm", 
				method.args = list(family=binomial))+
  theme_classicmodify()+
  labs(x="Heart rate (bpm)", y="Gape binary (0=closed, 1=open)")
  

ggsave(path=plots, "hr_gapebinary.png", width=6, height=6)

ggplot(allhrgapemeta, aes(finalBPM, binary,col=Treatment)) + 
  geom_point() +
	stat_smooth(method="glm", 
				method.args = list(family=binomial))+
  theme_classicmodify()+
  scale_color_manual(values=treatment_palette)+
  labs(x="Heart rate (bpm)", y="Gape binary (0=closed, 1=open)")
  

ggsave(path=plots, "hr_gapebinary_treatment.png", width=6, height=6)

ggplot(allhrgapemeta, aes(finalBPM, binary,col=Site)) + 
  geom_point() +
	stat_smooth(method="glm", 
				method.args = list(family=binomial))+
  theme_classicmodify()+
  scale_color_manual(values=sitePalette)+
  labs(x="Heart rate (bpm)", y="Gape binary (0=closed, 1=open)")
  

ggsave(path=plots, "hr_gapebinary_site.png", width=6, height=6)


```

#stats
```{r}
#ANCOVA for temp and hr 
#test assumption 1 that temp and treatment are independent 
model <- aov(Temp.C ~ Treatment, data = allhrgapemeta)
summary(model)
#for some reason pvalue 9s <2e-16? But graph shows a ton of boxplot overlap in temp across treatments. Maybe too much data?

#test assumption 2 levene's test. Make sure variances among groups are equal
leveneTest(finalBPM~Treatment, data = allhrgapemeta)
#pvalue=<2.2e-16 variances not equal, transform finalBPM
allhrgapemeta <- mutate(allhrgapemeta, logfinalBPM = log10(finalBPM))

#try ANCOVA
ancova_model <- aov(logfinalBPM ~ Temp.C + Treatment, data = allhrgapemeta)
Anova(ancova_model, type="III")
#says treatment and temp are highly sig. 

plot(ancova_model, which = 2)
#residual line isn't super linear

```

#average weighted HRs 
```{r}
#group by code, take weighted mean and variance with HMISC package-wtd.var, stats package-weighted.mean


allhrgapeweight=allhrgapemeta %>% 
  group_by(Code) %>% 
  mutate(Count = n(),avghr=mean(finalBPM), varhr=var(finalBPM)) %>%
  ungroup() %>% 
  group_by(Treatment,Site) %>%
  summarise(avg_hr_weight = weighted.mean(finalBPM, Count), var_hr_weight=wtd.var(finalBPM, Count))

hrgapebinary=allhrgapemeta %>% 
  group_by(Code) %>% 
  mutate(Count = n(),avghr=mean(finalBPM), varhr=var(finalBPM)) %>%
  ungroup() %>% 
  group_by(Treatment,Site,binary) %>%
  na.omit() %>% 
  summarise(avg_hr_weight = weighted.mean(finalBPM, Count),
            var_hr_weight=wtd.var(finalBPM, Count))

ggplot(allhrgapeweight, aes(Treatment, avg_hr_weight))+
  geom_bar(position="dodge", stat="summary", fun="mean",fill=treatment_palette,col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  labs(y="Average heart rate (bpm)")

ggsave(path=plots, "avghrweighted_treatment.png", width=6, height=6)

ggplot(allhrgapeweight, aes(Site, avg_hr_weight))+
  geom_bar(position="dodge", stat="summary", fun="mean",fill=sitePalette,col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  labs(y="Average heart rate (bpm)")

ggsave(path=plots, "avghrweighted_site.png", width=6, height=6)

ggplot(allhrgapeweight, aes(Treatment, var_hr_weight))+
  geom_bar(position="dodge", stat="summary", fun="mean",fill=treatment_palette,col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  labs(y="Daily range in heart rate (bpm)")

ggsave(path=plots, "varhrweighted_treatment.png", width=6, height=6)

ggplot(allhrgapeweight, aes(Site, var_hr_weight))+
  geom_bar(position="dodge", stat="summary", fun="mean",fill=sitePalette,col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  labs(y="Variability heart rate (bpm)")

ggsave(path=plots, "varhrweighted_site.png", width=6, height=6)

ggplot(hrgapebinary, aes(Treatment, avg_hr_weight, fill=interaction(binary,Treatment)))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  scale_fill_manual(values=treatment2_palette)+
  guides(pattern = guide_legend(override.aes = list(fill = "white"),title="Open/Closed",size=20),
         fill = guide_legend(override.aes = list(pattern = "none")))+
  stat_summary(position=position_dodge(0.9), fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  #guides(fill = "none")+
  labs(y="Average heart rate (bpm)")

ggsave(path=plots, "avghrweighted_treat_binary.png", width=6, height=6)

ggplot(hrgapebinary, aes(Treatment, var_hr_weight, fill=interaction(binary,Treatment)))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  scale_fill_manual(values=treatment2_palette)+
  guides(pattern = guide_legend(override.aes = list(fill = "white"),title="Open/Closed",size=20),
         fill = guide_legend(override.aes = list(pattern = "none")))+
  stat_summary(position=position_dodge(0.9), fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  #guides(fill = "none")+
  labs(y="Daily range in heart rate (bpm)")

ggsave(path=plots, "varhrweighted_treat_binary.png", width=6, height=6)

ggplot(hrgapebinary, aes(Site, var_hr_weight, fill=interaction(binary,Site)))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  scale_fill_manual(values=sitePalette2)+
  guides(pattern = guide_legend(override.aes = list(fill = "white"),title="Open/Closed",size=20),
         fill = guide_legend(override.aes = list(pattern = "none")))+
  stat_summary(position=position_dodge(0.9), fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  #guides(fill = "none")+
  labs(y="Variability heart rate (bpm)")

ggsave(path=plots, "varhrweighted_site_binary.png", width=6, height=6)

ggplot(hrgapebinary, aes(Site, avg_hr_weight, fill=interaction(binary,Site)))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  scale_fill_manual(values=sitePalette2)+
  guides(pattern = guide_legend(override.aes = list(fill = "white"),title="Open/Closed",size=20),
         fill = guide_legend(override.aes = list(pattern = "none")))+
  stat_summary(position=position_dodge(0.9), fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  #guides(fill = "none")+
  labs(y="Average heart rate (bpm)")

ggsave(path=plots, "avghrweighted_site_binary.png", width=6, height=6)

ggplot(hrgapebinary, aes(binary, avg_hr_weight))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  stat_summary(position=position_dodge(0.9), fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  labs(y="Average heart rate (bpm)")

ggsave(path=plots, "avghrweighted_binary.png",width=6, height=6)

ggplot(hrgapebinary, aes(binary, var_hr_weight))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  stat_summary(position=position_dodge(0.9), fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  labs(y="Variability heart rate (bpm)")

ggsave(path=plots, "varhrweighted_binary.png",width=6, height=6)

```

#HR through time graphs
```{r}
ggplot(allhrgapemeta, aes(DateTime, finalBPM,col=Treatment))+
  geom_point()+
  theme_classicmodify()+
  labs(y="Average heart rate (bpm)")+
  geom_smooth()

ggsave(path=plots, "hr_time_treatment.png", width=8, height=6)

ggplot(allhrgapemeta, aes(DateTime, finalBPM,col=Site))+
  geom_point()+
  theme_classicmodify()+
  labs(y="Average heart rate (bpm)")+
  geom_smooth()

ggsave(path=plots, "hr_time_site.png", width=8, height=6)

ggplot(allhrgapemeta, aes(DateTime, finalBPM))+
  geom_point()+
  theme_classicmodify()+
  labs(y="Average heart rate (bpm)")+
  geom_smooth()

ggsave(path=plots, "hr_time.png", width=8, height=6)
```

#range per day
```{r}
hrvarday=allhrmeta %>% 
  group_by(day = floor_date(DateTime, unit = "day")) %>%
  summarise(hrday_mean=mean(finalBPM, na.rm=T),
            hrday_var=max(finalBPM, na.rm=TRUE)-min(finalBPM, na.rm=TRUE))
  
ggplot(hrvarday, aes(day, hrday_var))+
  geom_point()+
  theme_classicmodify()+
  labs(x="Date",y="Variability heart rate (bpm)")

ggsave(path=plots, "hrvar_day.png",height=6, width=6)

#separate by treatment/code/site
hrvarday_grouped=allhrmeta %>% 
  group_by(day = floor_date(DateTime, unit = "day"),Treatment,Site,Code) %>%
  summarise(hrday_mean=mean(finalBPM, na.rm=T),
            hrday_var=max(finalBPM, na.rm=TRUE)-min(finalBPM, na.rm=TRUE))

#separate by treatment/code/site
hrvarday_grouped_code=allhrgapemeta %>% 
  group_by(day = floor_date(DateTime, unit = "day"),Code) %>%
  mutate(hrday_mean=mean(finalBPM, na.rm=T),
            hrday_var=max(finalBPM, na.rm=TRUE)-min(finalBPM, na.rm=TRUE))

ggplot(hrvarday_grouped_code, aes(day,hrday_var,col=Code))+
  geom_point()+
 # geom_smooth()+
  theme_classicmodify()+
  #scale_color_manual(values=treatment_palette)+
  labs(x="Date",y="Variability heart rate (bpm)")

ggsave(path=plots, "hrvar_day_treat.png",height=6, width=6)

ggplot(allhrmeta, aes(Code,finalBPM,col=Code))+
  geom_boxplot()+
 # geom_smooth()+
  theme_classicmodify()+
  #scale_color_manual(values=treatment_palette)+
  labs(x="Date",y="Variability heart rate (bpm)")

ggsave(path=plots, "hrvar_day_code.png",height=6, width=6)

ggplot(hrvarday_grouped, aes(day,hrday_var,col=Treatment))+
  geom_point()+
  geom_smooth()+
  theme_classicmodify()+
  scale_color_manual(values=c( "#009E73","#999999","chocolate4"))+
  labs(x="Date",y="Daily average heart rate range (bpm)")

ggsave(path=plots, "hrvar_day_treat.png",height=6, width=6)

ggplot(hrvarday_grouped, aes(day,hrday_var,col=Site))+
  geom_point()+
  geom_smooth()+
  theme_classicmodify()+
  scale_color_manual(values=sitePalette)+
  labs(x="Date",y="Variability heart rate (bpm)")

ggsave(path=plots, "hrvar_day_site.png",height=6, width=6)

ggplot(hrvarday_grouped, aes(day,hrday_var,col=Code))+
  geom_line()+
  #geom_smooth()+
  theme_classicmodify()+
  labs(x="Date",y="Variability heart rate (bpm)")

ggsave(path=plots, "hrvar_day_code.png",height=6, width=10)

#max HR by code
allhrmeta %>% 
  group_by(Code) %>% 
  summarise(maxhr=max(finalBPM)) %>% 
  ggplot(aes(Code, maxhr))+
  geom_point()+
  ylim(0,10)

hrvartreatment=fourseasonshr %>%
  #filter(Code!="D.M.6.1054") %>% #remove outliers
  #filter(Code!="W.O.5.1016") %>% 
  group_by(lubridate::floor_date(DateTime, "day"),Treatment,Site,Season,Code) %>%
  mutate(rangebpm=max(finalBPM, na.rm=TRUE)-min(finalBPM, na.rm=TRUE)) %>% 
  ungroup() %>% 
  group_by(Treatment,day=lubridate::floor_date(DateTime, "day")) %>% 
  summarise(avgbpm=mean(finalBPM),rangebpm=mean(rangebpm))

testhr=fourseasonshr %>%
  #filter(Code!="D.M.6.1054") %>% #remove outliers
  #filter(Code!="W.O.5.1016") %>% 
  group_by(lubridate::floor_date(DateTime, "day"),Treatment,Site,Season,Code) %>%
  summarise(rangebpm=max(finalBPM, na.rm=TRUE)-min(finalBPM, na.rm=TRUE)) 

ggplot(testhr, aes(`lubridate::floor_date(DateTime, "day")`,rangebpm,col=Treatment))+
  geom_point()+
  geom_smooth()+
  #ylim(0,16)+
  theme_classicmodify()+
  scale_color_manual(values=c("chocolate4", "#999999","#009E73"))+
  labs(x="Date",y="Daily average heart rate range (bpm)")

ggsave("hrrange_treat_line.png",path=plots, width=8, height=6)
```

#HR per season
```{r}
hrwet <- hrvarday_grouped %>%
    filter((DateTime > '2022-12-31 00:00:00') &
        (DateTime<"2023-04-01 00:00:00"))

hrdry <- hrvarday_grouped %>%
    filter((DateTime>"2022-05-01 00:00:00"& DateTime<"2022-12-31 00:00:00")|(DateTime>"2023-04-01 00:00:00"))

hrseason<- bind_rows(hrwet, hrdry, .id = 'Season')

hrseason=hrseason %>% 
  mutate(Season = recode(Season, '1' = 'Wet', '2' = 'Dry')) 

#take avg and var hr for each oyster each day and average by site, treatment, and season
hrseasongrouped=hrseason %>% 
  group_by(Treatment,Site,Season) %>%
  summarise(hr_mean=mean(hrday_mean),
         avghr_var=mean(hrday_var),
         Count = n(),
         hrse=sd(hrday_mean)/sqrt(Count))

ggplot(hrseasongrouped, aes(Treatment, hr_mean, fill=interaction(Season, Treatment)))+
    geom_bar_pattern(stat="summary", fun="mean",
        aes(pattern = Season),
        colour = "black",
        pattern_fill = "black",
        pattern_angle = 45,
        pattern_density = 0.05,
        pattern_spacing = 0.02,
        pattern_key_scale_factor = 0.6,
        position = position_dodge(width = .9)) +
    stat_summary(position=position_dodge(0.9), fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  #geom_errorbar(aes(ymin=hr_mean-hrse, ymax=hr_mean+hrse),position=position_dodge(width = .9), width=0.2,size = 0.75)+
  scale_pattern_manual(values = c(Dry="stripe", Wet="none")) +
  scale_fill_manual(values=treatmentidentical_palette)+
  guides(pattern = guide_legend(override.aes = list(fill = "white"),title="Season",size=20),
         fill = guide_legend(override.aes = list(pattern = "none")))+
  theme_classicmodify()+
  labs(y="Average heart rate (bpm)")+
  theme(axis.text = element_text(size = 20), axis.title = element_text(size = 25),legend.title = element_text(size=25), legend.text = element_text(size=15))+
  guides(fill = "none")

ggsave(path=plots, "avghr_treat_season.png",width=6, height=6)

ggplot(hrseasongrouped, aes(Treatment, avghr_var, fill=interaction(Season, Treatment)))+
    geom_bar_pattern(stat="summary", fun="mean",
        aes(pattern = Season),
        colour = "black",
        pattern_fill = "black",
        pattern_angle = 45,
        pattern_density = 0.05,
        pattern_spacing = 0.02,
        pattern_key_scale_factor = 0.6,
        position = position_dodge(width = .9)) +
    stat_summary(position=position_dodge(0.9), fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  scale_pattern_manual(values = c(Dry="stripe", Wet="none")) +
  scale_fill_manual(values=treatmentidentical_palette)+
  guides(pattern = guide_legend(override.aes = list(fill = "white"),title="Season",size=20),
         fill = guide_legend(override.aes = list(pattern = "none")))+
  theme_classicmodify()+
  labs(y="Average daily range in heart rate (bpm)")+
  theme(axis.text = element_text(size = 20), axis.title = element_text(size = 25),legend.title = element_text(size=25), legend.text = element_text(size=15))+
  guides(fill = "none")

ggsave(path=plots, "avgvar_treat_season.png",width=8, height=8)

ggplot(hrseasongrouped, aes(Site, avghr_var, fill=interaction(Season, Site)))+
    geom_bar_pattern(stat="summary", fun="mean",
        aes(pattern = Season),
        colour = "black",
        pattern_fill = "black",
        pattern_angle = 45,
        pattern_density = 0.05,
        pattern_spacing = 0.02,
        pattern_key_scale_factor = 0.6,
        position = position_dodge(width = .9)) +
    stat_summary(position=position_dodge(0.9), fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  scale_pattern_manual(values = c(Dry="stripe", Wet="none")) +
  scale_fill_manual(values=sitePalette2)+
  guides(pattern = guide_legend(override.aes = list(fill = "white"),title="Season",size=20),
         fill = guide_legend(override.aes = list(pattern = "none")))+
  theme_classicmodify()+
  labs(y="Average daily range in heart rate (bpm)")+
  theme(axis.text = element_text(size = 20), axis.title = element_text(size = 25),legend.title = element_text(size=25), legend.text = element_text(size=15))+
  guides(fill = "none")

ggsave(path=plots, "avgvar_site_season.png",width=8, height=8)

ggplot(hrseasongrouped, aes(Site, hr_mean, fill=interaction(Season, Site)))+
    geom_bar_pattern(stat="summary", fun="mean",
        aes(pattern = Season),
        colour = "black",
        pattern_fill = "black",
        pattern_angle = 45,
        pattern_density = 0.05,
        pattern_spacing = 0.02,
        pattern_key_scale_factor = 0.6,
        position = position_dodge(width = .9)) +
    stat_summary(position=position_dodge(0.9), fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  scale_pattern_manual(values = c(Dry="stripe", Wet="none")) +
  scale_fill_manual(values=sitePalette2)+
  guides(pattern = guide_legend(override.aes = list(fill = "white"),title="Season",size=20),
         fill = guide_legend(override.aes = list(pattern = "none")))+
  theme_classicmodify()+
  labs(y="Average heart rate (bpm)")+
  theme(axis.text = element_text(size = 20), axis.title = element_text(size = 25),legend.title = element_text(size=25), legend.text = element_text(size=15))+
  guides(fill = "none")

ggsave(path=plots, "avghr_site_season.png",width=8, height=8)

```

#HR per 4 seasons
```{r}
#hrvarday_grouped from range per day
seasonhr=hrvarday_grouped %>%
    ungroup() %>% 
    mutate(Season = case_when( #new variable
          DateTime >= "2022-07-15 00:00:00" & DateTime <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
          DateTime >="2022-10-01 00:00:00" & DateTime <= "2022-12-31 23:59:00" ~ "Fall",
          DateTime >= "2023-01-01 00:00:00" & DateTime <= "2023-03-31 23:59:00" ~ "Winter",
          DateTime >= "2023-04-01 00:00:00" & DateTime <= "2023-06-19 23:59:00" ~ "Spring",
          TRUE ~ NA)) %>% #display error if a value is not assigned to one of the previous groups
  filter(!is.na(Season)) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(Season = fct_relevel(Season,c("Summer","Fall", "Winter", "Spring")),
          Treatment= fct_relevel(Treatment,c("Mud","Oyster", "Eelgrass"))) %>% 
  select(-c("DateTime","Temp.C","hallpercent","binary","finalBPM")) %>% 
  distinct()

seasonhrsite=seasonhr %>% 
  group_by(Treatment,Site,Code,Season) %>% 
  mutate(ntreats = n()) %>% 
  mutate(avghr=mean(hrday_mean),avgse=sd(hrday_mean)/sqrt(ntreats),
         avgrange=mean(hrday_var)) 

ggplot(seasonhrsite,aes(Season,avghr,fill=Season))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  #geom_errorbar(aes(ymin=avghr-avgse, ymax=avghr+avgse), width=.2,
               #  position=position_dodge(.9),stat="summary")+ #same thing
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  labs(y="Average daily heart rate (bpm)")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")

ggsave(path=plots, "avghr_fourseasons.png",width=6, height=6)

ggplot(seasonhr,aes(Season,hrday_mean,fill=Season))+
  geom_boxplot()+
  stat_summary(fun=mean, geom="point", shape=18, size=5, color="black", fill="red")+
  theme_classicmodify()+
  labs(y="Average daily heart rate (bpm)")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")

ggsave(path=plots, "avghr_fourseasons_box.png",width=6, height=6)

ggplot(seasonhrsite,aes(Season,avghr,fill=Season))+
  geom_boxplot()+
  stat_summary(fun=mean, geom="point", shape=18, size=5, color="black", fill="red")+
  theme_classicmodify()+
  labs(y="Average daily heart rate (bpm)")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")

ggsave(path=plots, "avghr_fourseasons_box.png",width=6, height=6)

ggplot(seasonhrsite,aes(Season,avgrange,fill=Season))+
  geom_boxplot()+
  stat_summary(fun=mean, geom="point", shape=18, size=5, color="black", fill="red")+
  theme_classicmodify()+
  labs(y="Average daily range in heart rate (bpm)")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")

ggsave(path=plots, "avgrange_fourseasons_box.png",width=6, height=6)

seasonhrstats=seasonhrsite %>% 
  group_by(Code,Season) %>% 
  select(c("Code","Season","avghr","avgrange")) %>% 
  distinct()

#not normal 
######### Do 4 separate times for each season. If at least 1 not normal then KW. If all normal then ANOVA
shapiro.test(seasonhrstats$avghr)
qqPlot(seasonhrsite$avghr)

#test for equal variances

#kruskal wallice test test
kruskal.test(avghr ~ Season, data=seasonhrstats)
#p=0.4475, daily average HR doesn't differ across seasons, pooling by site and treatment 

#kruskal wallice test test
kruskal.test(avgrange ~ Season, data=seasonhrstats)
#p=0.1039, daily average HR doesn't differ across seasons, pooling by site and treatment 


ggplot(seasonhrsite,aes(Season,avghr,fill=Treatment))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  #geom_errorbar(aes(ymin=avghr-avgse, ymax=avghr+avgse), width=.2,
               #  position=position_dodge(.9),stat="summary")+ #same thing
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75,position=position_dodge(.9))+
  theme_classicmodify()+
  labs(y="Average daily heart rate (bpm)")+
  scale_fill_manual(values=c("#009E73","chocolate4","#999999"))

ggsave(path=plots, "avghr_treatment_fourseasons.png",width=6, height=6)

ggplot(seasonhrsite,aes(Treatment,hrday_mean,fill=Treatment))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  #geom_errorbar(aes(ymin=avghr-avgse, ymax=avghr+avgse), width=.2,
               #  position=position_dodge(.9),stat="summary")+ #same thing
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75,position=position_dodge(.9))+
  theme_classicmodify()+
  labs(y="Average daily heart rate (bpm)")+
  scale_fill_manual(values=c("#009E73","chocolate4","#999999"))
#different than other treatment graph 

ggplot(seasonhrsite,aes(Season,avghr,fill=Site))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  #geom_errorbar(aes(ymin=avghr-avgse, ymax=avghr+avgse), width=.2,
               #  position=position_dodge(.9),stat="summary")+ #same thing
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75,position=position_dodge(.9))+
  theme_classicmodify()+
  labs(y="Average daily heart rate (bpm)")+
  scale_fill_manual(values=sitePalette)

ggsave(path=plots, "avghr_site_fourseasons.png",width=6, height=6)

###############
#HR range
ggplot(seasonhrsite,aes(Season,avgrange,fill=Season))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75)+
  theme_classicmodify()+
  labs(y="Average daily range in heart rate (bpm)")+
  scale_fill_manual(values=season_palette)+
  theme(legend.position="none")

ggsave(path=plots, "avgrange_fourseasons.png",width=6, height=6)

ggplot(seasonhrsite,aes(Season,avgrange,fill=Treatment))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  #geom_errorbar(aes(ymin=avghr-avgse, ymax=avghr+avgse), width=.2,
               #  position=position_dodge(.9),stat="summary")+ #same thing
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75,position=position_dodge(.9))+
  theme_classicmodify()+
  labs(y="Average daily range in heart rate (bpm)")+
  scale_fill_manual(values=c("#009E73","chocolate4","#999999"))

ggsave(path=plots, "rangehr_treatment_fourseasons.png",width=6, height=6)

ggplot(seasonhrsite,aes(Season,avgrange,fill=Site))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  #geom_errorbar(aes(ymin=avghr-avgse, ymax=avghr+avgse), width=.2,
               #  position=position_dodge(.9),stat="summary")+ #same thing
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75,position=position_dodge(.9))+
  theme_classicmodify()+
  labs(y="Average daily range in heart rate (bpm)")+
  scale_fill_manual(values=sitePalette)

ggsave(path=plots, "avgrange_site_fourseasons.png",width=6, height=6)
```

#precip
```{r}
#get culm rainfall per day
precip[is.na(precip)] <- 0

#only pull out 00:53:00 numbers from precip
precip53 <- subset(precip, format(precip$DateTimeUTC, "%M") %in% c('53'))

#average of precip per day
precipday=precip53 %>%
    group_by(day = floor_date(DateTimeUTC, unit = "day")) %>%
  summarise(day_mean = mean(Precip1hr.mm, na.rm=T),
            sd = sd(Precip1hr.mm, na.rm=T),
            n = n())

#get sum of precip per day
precipsumday=precip53 %>%
  summarise_by_time(
    .date_var=DateTimeUTC,
    .by= "day", 
    precipday = sum(Precip1hr.mm, na.rm=T))

hrvarday$DateTimeUTC=hrvarday$day
hrprecip=merge(hrvarday,precipsumday,by="DateTimeUTC")
hrprecip=merge(hrprecip,temphallseasonprecip, by="DateTimeUTC")

precipplot=ggplot(hrprecip, aes(x=DateTimeUTC,y=precipday)) +
  geom_bar(stat="summary",position="dodge",fun="sum", size=1, color="black") + 
  theme_classicmodify()+
    theme(
    axis.title.y = element_text(color = "black"),
    plot.margin = margin(10, 30, 10, 10),
    #axis.text.x=element_text(size=15, angle=20,hjust=0,vjust=0.2),
    axis.title.y.right = element_text(color = "black"))+
  labs(x="Date",y="Precipitation (mm)",tag = "D")+
  scale_x_datetime(breaks = seq(as.POSIXct("2022-07-01"), as.POSIXct("2023-07-01"), by="90 days"), date_labels = "%b %Y")

hrplot=ggplot(hrprecip, aes(x=DateTimeUTC,y=hrday_mean)) +
  geom_line(size=1,col="red") + 
  theme_classicmodify()+
    theme(
    axis.title.y = element_text(color = "black"),
    plot.margin = margin(10, 30, 10, 10))+
  labs(x="Date",y="Average daily heart rate (bpm)",tag = "A")+
  scale_x_datetime(breaks = seq(as.POSIXct("2022-07-01"), as.POSIXct("2023-07-01"), by="90 days"), date_labels = "%b %Y")

gapeplot=ggplot(hrprecip, aes(x=DateTimeUTC,y=meanpercentopen)) +
  geom_line(size=1,col="blue") + 
  theme_classicmodify()+
    theme(
    axis.title.y = element_text(color = "black"),
    plot.margin = margin(10, 30, 10, 10))+
  labs(x="Date",y="Proportion of open valves",tag = "C")+
  scale_x_datetime(breaks = seq(as.POSIXct("2022-07-01"), as.POSIXct("2023-07-01"), by="90 days"), date_labels = "%b %Y")

rangeplot=ggplot(hrprecip, aes(x=DateTimeUTC,y=hrday_var)) +
  geom_line(size=1,col="purple") + 
  theme_classicmodify()+
    theme(
    axis.title.y = element_text(color = "black"),
    plot.margin = margin(10, 30, 10, 10))+
  labs(x="Date",y="Average daily heart rate range (bpm)",tag = "B")+
  scale_x_datetime(breaks = seq(as.POSIXct("2022-07-01"), as.POSIXct("2023-07-01"), by="90 days"), date_labels = "%b %Y")

figure=ggarrange( 
           hrplot+ 
            theme(axis.text.x = element_blank(),
                  #axis.title.y = element_text(angle = 45, vjust = 1, hjust=1),
                  axis.ticks.x = element_blank(),
                  axis.title.x = element_blank(),
                  axis.text=element_text(size=12),
                  axis.title=element_text(size=11)),
           rangeplot+
             theme(axis.text.x = element_blank(),
                    #axis.title.y = element_text(angle = 45, vjust = 1, hjust=1),
                  axis.ticks.x = element_blank(),
                  axis.title.x = element_blank(),
                  axis.text=element_text(size=12),
                  axis.title=element_text(size=10)),
          gapeplot+ 
            theme(axis.text.x = element_blank(),
                  # axis.title.y = element_text(angle = 45, vjust = 1, hjust=1),
                  axis.ticks.x = element_blank(),
                  axis.title.x = element_blank(),
                  axis.text=element_text(size=8),
                  axis.title=element_text(size=12)),
          precipplot+
            theme(axis.text=element_text(size=13),
                  # axis.title.y = element_text(angle = 45, vjust = 1, hjust=1),
                  axis.title=element_text(size=13)), nrow=4, ncol=1)

ggsave(plot=figure,"preciphrgapetest.png",path=plots,height=9, width=6)



# Generate 4 different sets of outputs
y <- list(hrprecip$precipday,hrprecip$hrday_mean,hrprecip$meanpercentopen, hrprecip$hrday_var)

# Colors for y[[2]], y[[3]], y[[4]] points and axes
colors = c("red", "blue", "green")

labels=c("Average daily heart rate (bpm)","Proportion of open valves","Average daily heart rate range (bpm)")

# The side for the axes.  The next one will go on
# the left, the following two on the right side
sides <- list(4, 4)

# The number of "lines" into the margin the axes will be
lines <- list(NA, 2)

#graph 
png(paste0(plots,"precip_hr_gape.png"), width = 8, height = 6, units = 'in', res = 300)

# Set the margins of the plot wider
par(oma = c(0, 0, 0, 6))
#bottom, left, top, right
  plot(hrprecip$DateTimeUTC, y[[1]], yaxt = "n", xlab = "Date",type="n",cex.lab=1.25, cex.axis=1,ylab="Precipitation (mm)")
# We use the "pretty" function go generate nice axes
axis(at = pretty(y[[1]]), side = 2)

  par(new = TRUE)
  plot(hrprecip$DateTimeUTC, hrprecip$hrday_mean, axes = FALSE, col = "red", xlab = "", ylab = "",type="n")
  axis(side = 4, line = NA,col = "red",at = pretty(hrprecip$hrday_mean))
  mtext("Average daily heart rate\n (bpm)", side=4, line=3, cex.lab=1,las=3, col="red")
  lines(hrprecip$DateTimeUTC, hrprecip$hrday_mean, col = adjustcolor("red", alpha = 0.4),lwd=1.5)
  
  par(new = TRUE)
  plot(hrprecip$DateTimeUTC, hrprecip$meanpercentopen, axes = FALSE, col = "blue", xlab = "", ylab = "",type="n")
  axis(side = 4, line = 5,col = "blue",at = pretty(hrprecip$meanpercentopen))
  mtext("Proportion of open valves", side=4, line=7, cex.lab=1,las=3, col="blue")
  lines(hrprecip$DateTimeUTC, hrprecip$meanpercentopen, col = adjustcolor("blue", alpha = 0.4),lwd=1.5)
  
  par(new=TRUE)
  plot(hrprecip$DateTimeUTC,hrprecip$precipday, axes=FALSE, xlab = "Date",type="n",cex.lab=1.25, cex.axis=1,ylab="Precipitation (mm)")
lines(hrprecip$DateTimeUTC, hrprecip$precipday,lwd=1.5, col=adjustcolor("black", alpha = 0.4))

dev.off()


coeff=1/3

precipplot=ggplot(hrprecip, aes(x=DateTimeUTC)) +
  geom_bar(aes(y=precipday), stat="summary",position="dodge",fun="sum", size=1, color="black", alpha=.4) + 
  labs(tag = "A")+
  geom_line(aes(y=hrday_mean/coeff), size=1,alpha=0.6,col="red") +
  scale_y_continuous(
    # Features of the first axis
    name = "Precipitation (mm)",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~.*coeff, name="Average heart rate per day (bpm)"))+
  #theme_classicmodify()+
  theme_classicmodifygrid()+
    theme(
    axis.title.y = element_text(color = "black"),
    axis.title.y.right = element_text(color = "red")) 

ggsave("precip_hr.png",path=plots,dpi = 500, width = 9, height = 4, units = "in")

coeff=3

ggplot(hrprecip, aes(x=DateTimeUTC)) +
  geom_line(aes(y=hrday_mean), size=1,alpha=0.6,col="red") +
  geom_bar(aes(y=precipday/coeff), stat="summary",position="dodge",fun="sum", size=1, color="blue", alpha=.4) + 
  scale_y_continuous(
    # Features of the first axis
    name = "Average heart rate per day (bpm)",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~.*coeff, name="Precipitation (mm)"))+
  theme_classicmodifygrid()+
    theme(
    axis.title.y = element_text(color = "red"),
    axis.title.y.right = element_text(color = "blue")) +
  scale_x_datetime(limits = as.POSIXct(strptime(c("2023-01-01 00:00:00", "2023-04-01 00:00:00"), 
                   format = "%Y-%m-%d %H:%M")))

ggsave("precip_hr_zoom.png",path=plots, height=6, width=6)

coeff2=1/55

gapeplot=ggplot(hrprecip, aes(x=DateTimeUTC)) +
  geom_bar(aes(y=precipday), stat="summary",position="dodge",fun="sum", size=1, color="black", alpha=.4) +
  labs(x="Date",tag = "B")+
  geom_line(aes(y=meanpercentopen/coeff2), size=1,alpha=0.6,col="blue") +
  scale_y_continuous(
    # Features of the first axis
    name = "Precipitation (mm)",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~.*coeff2, name="Proportion of open valves"))+
  theme_classicmodifygrid()+
    theme(
    axis.title.y = element_text(color = "black"),
    axis.title.y.right = element_text(color = "blue")) 

ggarrange( 
           precipplot+ 
            theme(axis.text.x = element_blank(),
                  #axis.title.y = element_text(angle = 45, vjust = 1, hjust=1),
                  axis.ticks.x = element_blank(),
                  axis.title.x = element_blank(),
                  #axis.text=element_text(size=12),
                  axis.title=element_text(size=15)),
           gapeplot+
             theme(#axis.text.x = element_blank(),
                    #axis.title.y = element_text(angle = 45, vjust = 1, hjust=1),
                  #axis.ticks.x = element_blank(),
                  #axis.title.x = element_blank(),
                  #axis.text=element_text(size=12),
                  axis.title=element_text(size=15)), nrow=2, ncol=1)

ggsave("preciphrgapehrfacet.png",path=plots,height=9, width=6)
```

#HR vs gapepercent
```{r}
ggplot(allhrgapemeta, aes(hallpercent, finalBPM))+
  geom_smooth()+
  theme_classicmodify()+
  labs(x="Percent gape (%)",y="Heart rate (bpm)")+
  xlim(0,100)+
  ylim(0,16)

ggscatter(allhrgapemeta, x = "hallpercent", y = "finalBPM", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson")


#check normality
shapiro.test(allhrgapemeta$hallpercent)


ggsave("hr_percentgape.png",path=plots, height=6, width=6)

ggplot(allhrgapemeta, aes(hallpercent, finalBPM,col=Treatment))+
  geom_smooth()+
  theme_classicmodify()+
  labs(x="Percent gape (%)",y="Heart rate (bpm)")+
  xlim(0,100)+
  scale_color_manual(values=c("#009E73","#999999","chocolate4"))+
  ylim(0,16)

ggsave("hr_percentgape_treat.png",path=plots, height=6, width=6)

ggplot(allhrgapemeta, aes(hallpercent, finalBPM,col=Site))+
  geom_smooth()+
  theme_classicmodify()+
  labs(x="Percent gape (%)",y="Heart rate (bpm)")+
  xlim(0,100)+
  scale_color_manual(values=sitePalette)+
  ylim(0,16)

ggsave("hr_percentgape_site.png",path=plots, height=6, width=6)

allhrgapemeta %>%
  mutate(Season = case_when( #new variable
      DateTime >= "2022-07-15 00:00:00" & DateTime <= "2022-09-30 23:59:00" ~ "Summer", #define condition for factor levels
      DateTime >="2022-10-01 00:00:00" & DateTime <= "2022-12-31 23:59:00" ~ "Fall",
      DateTime >= "2023-01-01 00:00:00" & DateTime <= "2023-03-31 23:59:00" ~ "Winter",
      DateTime >= "2023-04-01 00:00:00" & DateTime <= "2023-06-19 23:59:00" ~ "Spring",
      TRUE ~ NA)) %>% #display error if a value is not assigned to one of the previous groups
    filter(!is.na(Season)) %>% 
    mutate_if(is.character, as.factor) %>% 
    mutate(Season = fct_relevel(Season,c("Fall", "Winter", "Spring", "Summer"))) %>% 
  ggplot(aes(hallpercent, finalBPM,col=Season))+
  geom_smooth()+
  theme_classicmodify()+
  labs(x="Percent gape (%)",y="Heart rate (bpm)")+
  xlim(0,100)+
  scale_color_manual(values=season_palette)+
  ylim(0,16)

ggsave("hr_percentgape_season.png",path=plots, height=6, width=6)

#mixed effects logistic regression 
# estimate the model and store results in m
#tried for a really long time and never went through
m <- glmer(binary ~ finalBPM + Site+Treatment+ (1 | Code), data = allhrgapemeta, family = binomial, control = glmerControl(optimizer = "bobyqa"), nAGQ = 10)

# print the mod results without correlations among fixed effects
print(m, corr = FALSE)

model<-lme(binary~ finalBPM + Site+Treatment, random = ~1|Code, data = allhrgapemeta, na.action = na.omit)

#residuals
plot(model)
qqnorm(resid(model,type="normalized"))
#doesn't look normal

model=lme(log(binary+1)~ finalBPM + Site+Treatment, random = ~1|Code, data = allhrgapemeta, na.action = na.omit)
#doesn't help

options(contrasts = c("contr.sum","contr.poly")) 

model<-lme (binary~ finalBPM + Site+Treatment, random = ~1|Code, data = allhrgapemeta,method="ML", na.action = na.omit)

Anova(model,type="3")

options(contrasts = c("contr.treatment","contr.treatment"))

ggplot(data = allhrgapemeta, 
       aes(x = finalBPM, y = binary)) +
  geom_line(size = 1)+
  #labs(x="Water temperature (°C)", y="Gape binary (0=closed, 1=open)")+
  theme_classicmodify()+
  ylim(0,1)


```

#HR vs code
```{r}
allhrgapemeta$Code=as.factor(allhrgapemeta$Code)

ggplot(allhrgapemeta, aes(Code, finalBPM,fill=Code))+
  geom_bar(position="dodge", stat="summary", fun="mean",col="black")+
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.2, size = 0.75,position=position_dodge(.9))+
  theme_classicmodify()+
  labs(x="Code",y="Heart rate (bpm)")

ggsave("finalbpm_code.png",path=plots, width=10, height=6)

```

#ANOVA attempts HR vs gape binary vs precip
```{r}

#get number of gape obvs per oyster per day and get the number of open gapes
temphallseasonday2=fourseasons %>% 
  dplyr::select(-c(Alive)) %>% 
  distinct() %>% 
  group_by(day = floor_date(DateTime, unit = "day")) %>% #make month column 
  group_by(Treatment,Site,Season,day,Code) %>% 
  mutate(data_obvs = n(),
         sum_open=sum(binary),
         tempavg=mean(Temp.C)) %>% 
  filter(data_obvs>1260) %>%  #at least full day of observations (including maintenance periods)
  dplyr::select(-c(DateTime,Hall,Temp.C,Battery.V,SN,hallpercent,binary)) %>% 
    distinct() %>% 
  filter_at(vars(sum_open, data_obvs), all_vars(!is.na(.)))
#data_obvs=total observeration, sum_binary= total successes (when oysters are open)
  #count(binary) %>% 
  #mutate(percentopen=n/sum(n))
  #filter(binary=="1") %>% 
  #select(-"binary") %>% 
  #ungroup()

#seasonhr = subset(seasonhr, select = -X )

#merge percent open and hr together
hrgapeprecip=merge(temphallseasonday2, seasonhr, by=c("day","Code","Treatment","Site","Season"))

#make precip data have yes and no for rained or did not rain each day. Make so that Yes if rained previous day
precipanova=precipsumday %>%
  arrange(DateTimeUTC) %>% 
   mutate(
     rain = 
       dplyr::case_when(
        precipday > 0 |lag(precipday) > 0 ~ "Yes",
        lag(precipday)<=0 |precipday <= 0 ~ "No")) %>%
  rename(day=DateTimeUTC) %>% 
  distinct()

precipanova$rain=as.factor(precipanova$rain)
    
#merge with precip data
hrgapeprecip2=merge(hrgapeprecip, precipanova, by=c("day"))
hrgapeprecip2=hrgapeprecip2 %>% distinct()
################
#try betareg? for proportion data

#try changing percentopen 1s to 0.99999
#hrgapeprecip9=hrgapeprecip2 %>% 
#  mutate(percentopen=replace(percentopen,percentopen==1, 0.9999999)) 

#betareg
# mod <- betareg(I(sum_open/data_obvs) ~ rain + Treatment + Site, data = hrgapeprecip2, link = "logit",na.action=na.omit)
# plot(mod)
# summary(mod)

#try with glmer not betareg
# mod=glmer(sum_open~rain + Treatment + Site + (1|Code), 
#            data=hrgapeprecip2,weight=data_obvs,family="poisson")
# 
# mod=glmer(I(sum_open/data_obvs)~rain + Treatment + Site + (1|Code), 
#            data=hrgapeprecip2,family="binomial")

##################################################
######## gape model 
mod=glmer(cbind(sum_open, (data_obvs-sum_open)) ~ rain + Treatment + Site + tempavg +(1|Code), data=hrgapeprecip2,family="binomial")

plot(mod)
summary(mod)
Anova(mod,type="III")

library("DHARMa")

testDispersion(mod) #0.76 not overdispersed

simulationOutput <- simulateResiduals(fittedModel = mod, plot = F)
plot(simulationOutput) #warning to test outliers with bootstrapping

#test outliers with bootstrap
testOutliers(simulationOutput, type = c("bootstrap"), nBoot = 100, plot = T)
#p=0.08 so fail to reject the null hyp that the model is correct. Not an excess nor lack of outliers. Not overdispersed nor underdispersed. 

############################################
### Heart rate model
#negative binomial glmms to fix overdispersion
modhr=glmer.nb(hrday_mean~rain + Treatment + Site + tempavg + (1|Code), 
           data=hrgapeprecip2)

summary(modhr)
Anova(modhr, type="III")
plot(modhr)
testDispersion(modhr) #no longer overdispersed 
simulationOutput <- simulateResiduals(fittedModel = modhr, plot = F)
plot(simulationOutput) #not normal

testOutliers(simulationOutput, type = c("bootstrap"), nBoot = 100, plot = T)
testDispersion(modhr, type = "DHARMa")
testDispersion(modhr, type = "PearsonChisq")


modhrvar=glmer.nb(hrday_var~Treatment + rain+Site + (1|Code), 
           data=hrgapeprecip2)

summary(modhrvar)
Anova(modhrvar, type="III")
plot(modhrvar)
testDispersion(modhrvar) #no longer overdispersed 
simulationOutput <- simulateResiduals(fittedModel = modhrvar, plot = F)
plot(simulationOutput) #not normal

testOutliers(simulationOutput, type = c("bootstrap"), nBoot = 100, plot = T)


#######################################
############# 
#hr plus gape model. Use gape binary data. 
temphallseasonday3=fourseasons %>% 
  dplyr::select(-c(Alive)) %>% 
  distinct() %>% 
  group_by(day = floor_date(DateTime, unit = "day")) %>% #make month column 
  group_by(Treatment,Site,Season,day,Code) %>% 
  mutate(data_obvs = n()) %>% 
  filter(data_obvs>1260) %>%  #at least full day of observations (including maintenance periods)
  dplyr::select(-c(Hall,Temp.C,Battery.V,SN,hallpercent,data_obvs)) %>% 
  distinct()

seasonhr=seasonhr %>% distinct()

#merge percent open and hr together
hrgapeprecip3=merge(temphallseasonday3, seasonhr, by=c("day","Code","Treatment","Site","Season"))

precipanova=precipanova %>% distinct()
    
#merge with precip data
hrgapeprecip4=merge(hrgapeprecip3, precipanova, by=c("day"))
hrgapeprecip4=hrgapeprecip4 %>% distinct()

modhragpe=glmer(hrday_mean ~ binary + rain + Treatment + Site + (1|Code), data=hrgapeprecip4,family="poisson")

#modhragpe=glmer(hrday_mean ~ binary + (1|Code), data=hrgapeprecip4,family="poisson")

#negative binomial bc binary data
#modhrgape=glmer(finalBPM ~ binary + (1|Code),data=allhrgapemeta, family="inverse.gaussian")

plot(modhrgape)
summary(modhrgape)
Anova(modhrgape,type="III")

testDispersion(modhrgape) #0.76 not overdispersed

simulationOutput <- simulateResiduals(fittedModel = mod, plot = F)
plot(simulationOutput) #warning to test outliers with bootstrapping

#test outliers with bootstrap
testOutliers(simulationOutput, type = c("bootstrap"), nBoot = 100, plot = T)
#p=0.08 so fail to reject the null hyp that the model is correct. Not an excess nor lack of outliers. Not overdispersed nor underdispersed. 


#modhr=glmer(hrday_mean~rain + Treatment + Site + (1|Code), 
  #         data=hrgapeprecip2,family="inverse.gaussian")
#can't do quasipoisson, gamma, or inverse.gaussian with glmer

#If doesn't look good then maybe switch to glmer and family=poisson.  hrday_mean ~ Treatment + rain + Site + (1| Code), data = hrgapeprecip2, family = poisson or quasipoisson or Gamma or inverse.gaussian. 

#When considering other factors (rain, treatment) there is a difference among sites. Oysters are less open when it rains than when it doesn't rain (Estimate is negative. No rain is ref. When it does rain, estimate drops compared to no rain). Oysters more open in oyster/eelgrass than mud. oysters are less open in PCH than DA. 

Anova(mod,type="III") #put in paper. Fit logit link betaregression to percent open vs rain, site, treatment. Ran Anova on rain, treatment, site and were significant. Then look at post-hoc linear comparisons emmeans on treatments and rain conditions to see how they were different. Site is supposed to be replicates, relatively close, site shouldn't be driving factor so didn't include in post-hoc. Need to run mixed effects model with betareg bc need mixed effects model for repeated measures. Using betareg becaues looking at proportion data. 
    
Post_Hoc <- emmeans (mod, spec = 'Treatment','rain', type = "response")
summary(Post_Hoc)
contrast(Post_Hoc, method='pairwise')
    #Mud different from Eelgrass and Oyster regardless of rain. Estimate is neg so oyster has higher value so mud has lower percent open. Oysters less open in mud regardless of rain. 
    

#binomial generalized linear mixed model. Now code is random factor. 
# glmmod=glmer(percentopen ~ Treatment + rain + Site + (1| Code), data = hrgapeprecip9, family = binomial)
#issue with non-integer in a binomial glm

# summary(glmmod)

#################################
# anovagape<- aov(hrday_mean ~ percentopen + rain + Treatment + Site, data = hrgapeprecip2) 
# autoplot(anovagape) #not normal, all look bad
# 
# anovagape<- aov(log(hrday_mean+1)  ~ percentopen + rain + Treatment + Site, data = hrgapeprecip2) 
# autoplot(anovagape) #not normal
# summary(anovagape)

######glm? 
glm <- glm(hrday_mean ~ rain + Treatment + Site, data = hrgapeprecip2) 
autoplot(glm) #not normal, all look bad

#try poisson 
pois.glm <- glm(hrday_mean ~ rain + Treatment + Site, data = hrgapeprecip2, family = poisson) 
summary(pois.glm) #results make the most sense. But residual deviance is not 1:1 ratio with df

autoplot(pois.glm,which=1:2)
#doesn't look like a good fit?
#quasipoisson makes it worse

pois.glm <- glm(log(percentopen+1) ~ rain + Treatment + Site, data = hrgapeprecip2, family = poisson) 
summary(pois.glm) #only intercept is sig now. plots still look like not good fit

autoplot(pois.glm,which=1:2)

#try Type 3?? Type3 is the best test since we know the data is unbalanced and there is an interaction between the variables after switching the variables in the quasipoisson model??
options(contrasts=c("contr.sum", "contr.poly"))
model3<-lm(percentopen ~ rain + Treatment + Site, data = hrgapeprecip2)
Anova(model3, type=3)
#all sig so def not right
options(contrasts=c("contr.treatment", "contr.poly"))

#########
#Try repeated measures method
#+time-> time factor is fixed. random is thing that got measured multiple times (grouping), need to be clear about na's with lme()
repeatedmodel<-lme(hrday_mean~rain*Treatment*Site+day, data=hrgapeprecip2, na.action=na.omit) #error

plot(repeatedmodel) #heteroscedastic (non constant variance). Switch to poisson glm with counts but can't do here because not counts (densities).

#don't try and log bc numbers only go up to 16 max 
repeatedmodel2<-lme(hrday_mean~Treatment*Site+percentopen, random = ~1|Code, data=hrgapeprecip2, na.action=na.omit)

plot(repeatedmodel2) #looks better. If doesn't look good then maybe switch to glmer and family=poisson.  hrday_mean ~ Treatment + rain + Site + (1| Code), data = hrgapeprecip2, family = poisson or quasipoisson or Gamma or inverse.gaussian. 

qqnorm(resid(repeatedmodel2,type="normalized"))#looks better

summary(repeatedmodel2) 
Anova(repeatedmodel2) #try Anova or anova to see if treatment/site/etc are sig predictors

#post-hoc Tukey
summary(glht(mod,linfct = c("SitePCH = 0","SiteShellmaker = 0","SiteWestcliff = 0","SiteShellmaker = SiteWestcliff")))

##################
#try repeated measures way with gape as response. 
#original attempt

options(contrasts=c("contr.sum", "contr.poly")) #set contrasts
modellog<-lm(hrday_mean~percentopen*Treatment*Site,data=hrgapeprecip2) #linear model of transformed data because continuous
summary(modellog)
#check residuals
autoplot(modellog)

#type 3 ancova for unbalanced dataset and continuous independent variable
Anova(modellog,type=3)
options(contrasts=c("contr.treatment", "contr.poly")) #set contrasts back
summary(modellog)

###############
#binomial distribution with percentopen 

# binomial.glm <- glm(percentopen ~ rain + Treatment + Site, data = hrgapeprecip2, family = binomial) 
# summary(beta.glm) #results make the most sense. But residual deviance is not 1:1 ratio with df
# 
# autoplot(beta.glm,which=1:2)
# #doesn't look like a good fit?
# #quasipoisson makes it worse

#tukey type post hoc for betareg?
# summary(glht(mod,linfct = c("SitePCH = 0","SiteShellmaker = 0","SiteWestcliff = 0","SiteShellmaker = SiteWestcliff")))

#other post hoc
# linearHypothesis(mod, c('SiteShellmaker = SitePCH')) #p=0.4646
# linearHypothesis(mod, c("SiteShellmaker = SiteWestcliff")) #p=0.4707
# linearHypothesis(mod, c("SiteShellmaker = (Intercept)")) #<2.2e-16
# linearHypothesis(mod, c("SitePCH = SiteWestcliff")) #0.0001236
# linearHypothesis(mod, c("SitePCH = (Intercept)")) #2.2e-16
# linearHypothesis(mod, c("SiteWestcliff = (Intercept)")) #<2.2e-16

#try emmeans post hoc
    # Post_Hoc <- emmeans (mod, spec = 'Site','Treatment', type = "response")
    # summary(Post_Hoc)
    # contrast(Post_Hoc, method='pairwise')
    # 
    # Post_Hoc <- emmeans (mod, spec = c('Site','Treatment'),'rain', type = "response")
    # summary(Post_Hoc)
    # contrast(Post_Hoc, method='pairwise')

# #######another way 
# #Build General Linear Model
# mymodel <- lm(hrday_mean ~ rain + Treatment + Site, data = hrgapeprecip2)
# 
# #Confirm assumptions
# qqnorm(mymodel$residuals)
# qqline(mymodel$residuals,col='red')
# 
# plot(mymodel$fitted.values,mymodel$residuals,xlab='Fitted Values',ylab='Residuals')
# abline(h=0,col='red')
# 
# #View model results
# summary(mymodel)
# 
# #log doesn't work
# mymodel <- lm(log(hrday_mean+1) ~ rain + Treatment + Site, data = hrgapeprecip2)
```

#temp correlation
```{r}
hrtemp=allhrgapemeta %>%
  group_by(DateTime= floor_date(DateTime, unit = "day"),Treatment,Site,Code) %>% 
  mutate(rangebpm=max(finalBPM, na.rm=TRUE)-min(finalBPM, na.rm=TRUE)) %>% 
  mutate(avg_hr=mean(finalBPM),
         avg_range=mean(rangebpm),
         temp_avg=mean(Temp.C))%>% 
  mutate_if(is.character, as.factor) %>% 
  dplyr::select(-c(X, hallpercent,finalBPM,rangebpm,binary,Temp.C)) %>% 
  distinct()



gapetemptest=allgapemeta %>% 
  group_by(DateTime2= floor_date(DateTime, unit = "day"),Treatment,Site,Code) %>%
  mutate(n = n()) %>% 
  filter(n>1080) %>% 
  ungroup()

hightidetempgape=merge(gapetemptest,tides, by="DateTime")

hightidetempgape=hightidetempgape %>% 
  subset(TideHT.m>0.5) %>%  #only use rows where tide height is >0.5
  group_by(DateTime2= floor_date(DateTime, unit = "day"),Treatment,Site,Code) %>%
  count(binary) %>% 
  mutate(prop_open=n/sum(n)) %>% 
  filter(binary=="1") %>% 
  ungroup() %>% 
  dplyr::select(-c(binary,n)) %>% 
  distinct()

hightidetempgape$DateTime=hightidetempgape$DateTime2

hr_gape_temp_rain=merge(hightidetempgape,precipsumday,by=c("DateTime"))

hr_gape_temp=merge(hrtemp,gapetemp,by=c("Treatment", "Site", "Code","DateTime"))

hr_gape_temp=hr_gape_temp %>% 
  group_by(DateTime= floor_date(DateTime, unit = "day"),Treatment,Site,Code) %>% 
  summarise(avg_hr=mean(avg_hr),
         avg_range=mean(avg_range),
         temp_avg=mean(temp_avg),
         prop_open=mean(prop_open))

#precipsumday$DateTime=precipsumday$DateTimeUTC

hr_gape_temp_rain=merge(hr_gape_temp,precipsumday,by=c("DateTime"))

#see if temp and gape correlate
shapiro_test(hr_gape_temp$prop_open)
shapiro_test(hr_gape_temp$temp_avg)
shapiro_test(hr_gape_temp$avg_hr)
shapiro_test(hr_gape_temp$avg_range)
#none normal

cor.test(hr_gape_temp$prop_open, hr_gape_temp$temp_avg, method = "kendall")
#p=<2.2e-16

cor.test(hr_gape_temp$avg_hr, hr_gape_temp$temp_avg, method = "kendall")
#p=0.1244

cor.test(hr_gape_temp$avg_range, hr_gape_temp$temp_avg, method = "kendall")
#p=0.07781

ggplot(hr_gape_temp, aes(temp_avg, prop_open))+
  geom_point()+
  geom_smooth()+
  labs(x="Daily average temperature (°C)",y="Proportion of open valves")+
  theme_classicmodify()

ggplot(hr_gape_temp, aes(temp_avg, avg_range))+
  geom_point()+
  geom_smooth()+
  labs(x="Daily average temperature (°C)",y="Daily average heart rate range (bpm)")+
  theme_classicmodify()

##############################
#temp and rainfall correlation


shapiro_test(hr_gape_temp_rain$temp_avg)
shapiro_test(hr_gape_temp_rain$precipday)
#none normal

cor.test(hr_gape_temp_rain$temp_avg,hr_gape_temp_rain$precipday, method = "kendall")
#p=<2.2e-16

ggplot(hr_gape_temp_rain, aes(temp_avg, precipday))+
  geom_point()+
  geom_smooth()+
  labs(x="Daily average temperature (°C)",y="Daily average rainfall (mm)")+
  theme_classicmodify()

ggplot(hr_gape_temp_rain, aes(precipday, prop_open))+
  geom_point()+
  geom_smooth()+
  labs(x="Daily cumulative rainfall (mm)",y="Daily average proportion of open valves")+
  theme_classicmodify()
#oysters with data from at least 75% of the day, at high tide. 

cor.test(hr_gape_temp_rain$precipday,hr_gape_temp_rain$prop_open, method = "kendall")
#p=2.2e-16

ggsave("prop_open_rain.png", path=plots, width=8, height=6)
```

#quickly see temp differences per oyster
```{r}
temprange=allgapemeta %>% 
  subset(Temp.C>0) %>% #remove the bad data
  group_by(Code) %>% 
  summarise(temprange=(max(Temp.C)-min(Temp.C)))

temprange=allgapemeta %>% 
  subset(Temp.C>0) %>% #remove the bad data
  group_by(Code,DateTime= floor_date(DateTime, unit = "day")) %>% 
  summarise(maxtemp=max(Temp.C),
            mintemp=min(Temp.C),
            temprange=(max(Temp.C)-min(Temp.C)))
```

```{r testheartratefilter, echo=showcode , eval=evalAll}
# Some manual test code used to develop the 
# beats-per-minute calculations and filtering settings

Fs = 0.125 # 8Hz sampling rate, sampling interval = 0.125 seconds
# Step through 1 minute at a time
mytime = as.POSIXct('2022-07-25 10:50:00',tz='UTC', format="%Y-%m-%d %H:%M:%S")
mysensor = 'IR'
t1 = which.min(abs(Heart$DateTime - mytime)) #compares all times to "my time" to find "my time". T1 gives index for "smallest difference". 
chunklength = 240 # number of samples to use
# Test that we've hit the chosen minute (not just the closest time)
if (difftime(mytime,Heart$DateTime[t1], units = 'secs') %in% c(-2,-1,0,1,2)){
	# Extract a chunk of samples slightly longer than target length
	temp = Heart[t1:(t1+(chunklength-1)),]
  }

	# Add milliseconds onto the timestamps. So there is 
  # datetime for each reading down to the millisecond. 
  # Add 0.125 seconds to each reading. Very last reading
  # is the last millisecond in 30 seconds. (30.875seconds). 

	temp$DateTimeMS = temp$DateTime
	
	diffs = diff(temp$startMillis) / 1000
	temp$DateTimeMS[2:nrow(temp)] = temp$DateTimeMS[1] + cumsum(diffs)
	
# Make loop from row 1:240 that takes previous row and 
#	adds on 0.125. 
# Go through first 240 rows, then after hit 240-> 
# reset time using DateTime and start adding again. 
# Then after another 240, reset time and start again. 
# If jumped 240 rows and didn't see POSIXct time value 
# at the end, then stop b/c didn't take 240 measurements. 
for(j in 2:240){
  temp$DateTimeMS[j]=temp$DateTimeMS[j-1]+0.125
}

# go through Heart and plot each 240 chunk to 
# see if can see HR. Make function that fills in 
# timestamps to each 240 chunk so don't have 
# to pull out chunks first. 
plot(x=temp$DateTimeMS, y=temp$IR, type="l", main="De Anza Mud SN142 2022-07-28 00:20:00")

	# Handle cases where single very low values appear due to spurious values
	# being recorded in the dataset. Replace with a linear interpolation of
	# of the two neighboring values. First detrend the raw values.

# detrend fits regression through points, figures out
# slope, takes out slope from all values to get rid 
# of slow trend down or up in IR rates for the plots. 

# 	detrendedIR = pracma::detrend(temp[,mysensor])
# 	# Next look at the detrended values and look for spurious low values
# 	# For instance, a good heart signal might oscillate between -200 & +300
# 	# in the detrended data, and a spurious value might suddenly drop to 
# 	# -6000. 
# 	# Calculate the standard deviation of the data set, and then find values
# 	# that are more than 3 SD away from the mean (which should be ~zero in the
# 	# detrended data)
#   # take out outliers essentially. Take out data points >3x SD
# 	spuriousVals = which(abs(detrendedIR) > (3*sd(detrendedIR)) )
# 
# 
# 
# 	if ( length(spuriousVals) > 0) {
# 		# Convert to NAs 
# 		temp[spuriousVals,mysensor] = NA
# 		
# 		for (i in 1:length(spuriousVals)){
# 		  indx = spuriousVals[i] 
# 		  #if spuriousVals=first row then truncate out value from temp
# 		  if(indx==1){
# 		    temp=temp[2:nrow(temp),]
# 		  } else if(indx==2){
# 		    temp=temp[2:nrow(temp),]
# 		  } else if(indx>2){
# 		    # Get the row index for this spurious value
# 			# Replace the spurious value with the average 
        # of the values immediately
# 			# before and after the spurious value
#				temp[indx,sensor] = mean(c(temp[indx-1,sensor],temp[indx+1,sensor]))
# 			temp[indx,mysensor] = mean(temp[(indx-2):(indx+2),mysensor], na.rm=TRUE)
# 		  }
# 			 
# 		}
# 	}
# 	rm(spuriousVals)
	# With the spurious values replaced by interpolated values, re-run the 
	# detrending routine
	########################################
	detrendedIR = pracma::detrend(temp[,"IR"])
	
	# Define a butterworth filter- generate smooth
  # version of noisy graph so can see smooth peaks.
	# Consider using the bandpass filter rather than just a lowpass filter
	# because of the tendency for the IR heartrate signal to drift up and 
	# down in relation to ambient light, which tends to induce low-frequency
	# shifts that then fool the spectral analysis routines when trying to 
	# identify the dominant frequency
	bf = butter(3,W = c(1/60, 1/10), type = 'pass')  # bandpass filter
	#bf = butter(3,W = c(1/120, 1/10), type = 'pass')  # bandpass filter
	#bf = butter(3,W = 0.1, type = 'low')  # 10Hz lowpass filter
	# Apply the filter to the detrended data chunk
	y = filtfilt(bf, x = detrendedIR)
#plot to test smooth version
plot(detrendedIR,col="red", type="l")
lines(y) #add smooth version in black on top of graph
#	y2 = filtfilt(bf2, x = detrendedIR)
	myfft2 = spectrum(y, plot = FALSE)
	# Take the peak frequency from the spectrum, divide by sampling
	# rate to convert to cycles per second
	mypeakfreq = myfft2$freq[which.max(myfft2$spec)] / Fs
	# Calculate amplitude of filtered signal to avoid weak or noisy signals
	#amp = range(y)[2] - range(y)[1]
	# Multiply by 60 seconds to get cycles (beats) per minute
	bpm = 60 * mypeakfreq
	
	# Also look at spectrum of detrendedIR (unfiltered data)
#	op = par()
#	par(mfrow = c(2,1))
#	myfftraw = spectrum(detrendedIR)
#	myfft2 = spectrum(y)
#	par(op)
	
	
	# Plot the raw detrended signal
	par(mar = c(4.5,5,5,1))
	plot(temp$DateTimeMS, detrendedIR, type = 'l', 
			#main = paste0("Amplitude = ",round(amp,1),
			#		', BPM = ', bpm), 
			las = 1,
			xlab = 'Time, seconds',
			ylab = 'Detrended IR signal') 
	points(temp$DateTimeMS, detrendedIR, col = 1, pch = 20, cex = 0.5)
	#if (amp > 60){
		lines(temp$DateTimeMS,y, col = 3, lwd = 2) # add the filtered signal. See how many peaks are in this smooth signal. 
#		lines(temp$DateTimeMS,y2, col = 4, lwd = 2)
#	} else if (amp <= 60) {
#		lines(temp$DateTimeMS,y, col = 2, lwd = 2) # add the filtered signal	
#		warning('Garbage signal')
#	}
	# Use function from package 'forecast', returns peak period (not freq)
	forecastPeriod = forecast::findfrequency(y)  #look for frequency of peaks in green "smooth" line
	forecastFreq = 1/forecastPeriod # convert period to frequency
	forecastBPM = forecastFreq * 60 * 8 # convert frequency to beats per minute, based on the fact that the sampling rate is 8Hz, and there are 60 seconds in a minute. Get # for how many peaks it thinks there are in the sample. Double to get beats/min. 
	#forecast gets diff BPM than spectrum BPM calculation. 
	mtext(side = 3, text = paste0('Forecast BPM: ',round(forecastBPM,1)))
	# Use pracma package to find peaks. Note that at slower heart rates
	# this function tends to find the sub-peaks (akin to a P or T peak in a
	# human ECG trace) rather than just the main peaks (R peaks on a human). 
	# This happens based on what the bandpass filter lets through. 
	res = pracma::findpeaks(x = y, nups = 5, minpeakdistance = 10) #n-ups=make sure peak is "5" up from previous valley. 
	points(temp$DateTimeMS[res[,2]], y = res[,1], col = 4, pch = 19)
	# Print the estimated bpm from the pracma::findpeaks routine, but 
	# you need to double the number of peaks to get bpm if your time series
	# is only 30 seconds long
	mtext(side = 3, line = 2, text = paste0('pracma bpm: ', nrow(res)*2))
	mtext(side = 3, line = 3, text = paste(mysensor, mytime))
	mtext(side = 3, line = 1, text = paste('Spectrum BPM:' ,round(bpm,2)))
#} 
#ideally see similar BPM for forecast, pracma, and spectrum calculations. Go through other samples from other times/oysters and get a sense for what "good"/"bad" signals look like. See which routine gives the most accurate BPM. 
#Pracma individually picks out each peak (can plot results). Can't plot results from forecast because doesn't call out individual peaks, just gives summary number. 

``` 

```{r exampleHeartPlot, echo=showcode,dev='png',fig.width=5,fig.height=5,res=300}

mytime = as.POSIXct('2022-07-28 00:20:00',tz='UTC', format="%Y-%m-%d %H:%M:%S")
mysensor='IR'
t1 = which.min(abs(Heart$DateTimePST - mytime))
chunklength = 300
temp = Heart[t1:(t1+(chunklength*2)),]
Fs = 0.125

diffs = diff(temp$startMillis)
missedReads = which(diffs > 100)
# Check if there are any sampling gaps (missedReads)
if (length(missedReads) > 0){
	# Add on the final row as well
	missedReads = c(missedReads, (nrow(temp)+1) )
	# Calculate gap length between any missed reads (and the final read)
	testgaps = diff(missedReads) 
	if (missedReads[1] > chunklength){
		# In this case, just grab the first 300 readings
		temp = temp[1:chunklength,]
	} else if (length(which(testgaps>= chunklength)) > 0){
		# check if any of the testgaps values are >= chunklength
		# Get the index in temp that is at the start of the long run
		tempindx = missedReads[which(testgaps >= chunklength)] + 1
		# If a gap is > 300, grab the sample in that gap
		temp = temp[tempindx:(tempindx+chunklength-1),]
	} else if (length(which(testgaps>=chunklength)) == 0) {
		# In this case there may be multiple gaps in the time chunk, 
		# so that there are no good contiguous chunks of 300 readings
		# Return a data frame with NAs
		temp[,sensor] = NA 			
	}
} else if (length(missedReads) == 0) {
	# Subset down to exactly the chunk length (10Hz sample * 30 secs = 300 samples)
	temp = temp[1:chunklength,]
}

# Add milliseconds onto the timestamps
temp$DateTimeMS = temp$DateTimePST

diffs = diff(temp$startMillis) / 1000
temp$DateTimeMS[2:nrow(temp)] = temp$DateTimeMS[1] + cumsum(diffs)

# Handle cases where single very low values appear due to spurious values
# being recorded in the dataset. Replace with a linear interpolation of
# of the two neighboring values. First detrend the raw values
detrendedIR = pracma::detrend(temp[,mysensor])
# Next look at the detrended values and look for spurious low values
# For instance, a good heart signal might oscillate between -200 & +300
# in the detrended data, and a spurious value might suddenly drop to 
# -6000. 
# Calculate the standard deviation of the data set, and then find values
# that are more than 3 SD away from the mean (which should be ~zero in the
# detrended data)
spuriousVals = which(abs(detrendedIR) > (3*sd(detrendedIR)) )

if ( length(spuriousVals) > 0) {
	# Convert to NAs 
	temp[spuriousVals,mysensor] = NA
	
	for (i in 1:length(spuriousVals)){
		indx = spuriousVals[i]  # Get the row index for this spurious value
		# Replace the spurious value with the average of the values immediately
		# before and after the spurious value
#				temp[indx,sensor] = mean(c(temp[indx-1,sensor],temp[indx+1,sensor]))
		temp[indx,mysensor] = mean(temp[(indx-2):(indx+2),mysensor], na.rm=TRUE)
	}
}
rm(spuriousVals)
# With the spurious values replaced by interpolated values, re-run the 
# detrending routine
detrendedIR = pracma::detrend(temp[,mysensor])

# Define a butterworth filter
# Consider using the bandpass filter rather than just a lowpass filter
# because of the tendency for the IR heartrate signal to drift up and 
# down in relation to ambient light, which tends to induce low-frequency
# shifts that then fool the spectral analysis routines when trying to 
# identify the dominant frequency
bf = butter(3,W = c(1/60, 1/10), type = 'pass')  # bandpass filter
#	bf2 = butter(3,W = c(1/120, 1/10), type = 'pass')  # bandpass filter
#	bf = butter(3,W = 0.1, type = 'low')  # 10Hz lowpass filter
# Apply the filter to the detrended data chunk
y = filtfilt(bf, x = detrendedIR)
#	y2 = filtfilt(bf2, x = detrendedIR)
myfft2 = spectrum(y, plot = FALSE)
# Take the peak frequency from the spectrum, divide by sampling
# rate to convert to cycles per second
mypeakfreq = myfft2$freq[which.max(myfft2$spec)] / Fs
# Calculate amplitude of filtered signal to avoid weak or noisy signals
amp = range(y)[2] - range(y)[1]
# Multiply by 60 seconds to get cycles (beats) per minute
bpm = 60 * mypeakfreq

# Also look at spectrum of detrendedIR (unfiltered data)
#	op = par()
#	par(mfrow = c(2,1))
#	myfftraw = spectrum(detrendedIR)
#	myfft2 = spectrum(y)
#	par(op)

# Plot the raw detrended signal
par(mar = c(4.5,5.5,5,1))
mycex = 1.5
plot(temp$DateTimeMS, detrendedIR, type = 'l', 
#		main = paste0("Amplitude = ",round(amp,1),', BPM = ', bpm), 
		las = 1,
		xlab = 'Time, seconds',
#		ylab = 'Detrended IR signal',
		ylab = '',
		cex.axis = mycex,
		cex.lab = mycex,
		col = 2,
		lwd = 2) 
points(temp$DateTimeMS, detrendedIR, col = 1, pch = 20, cex = 1)
#if (amp > 60){
#	lines(temp$DateTimeMS,y, col = 3, lwd = 2) # add the filtered signal
##		lines(temp$DateTimeMS,y2, col = 4, lwd = 2)
#} else if (amp <= 60) {
#	lines(temp$DateTimeMS,y, col = 2, lwd = 2) # add the filtered signal	
#	warning('Garbage signal')
#}
# Use function from package 'forecast', returns peak period (not freq)
forecastPeriod = forecast::findfrequency(y)  
forecastFreq = 1/forecastPeriod # convert period to frequency
forecastBPM = forecastFreq * 60 * 10 # convert frequency to beats per 
# minute, based on the fact that the sampling rate is 10Hz, and there are
# 60 seconds in a minute
#mtext(side = 3, text = paste0('Forecast BPM: ',round(forecastBPM,1)))
# Use pracma package to find peaks. Note that at slower heart rates
# this function tends to find the sub-peaks (akin to a P or T peak in a
# human ECG trace) rather than just the main peaks (R peaks on a human). 
# This happens based on what the bandpass filter lets through. 
res = pracma::findpeaks(x = y, nups = 5, minpeakdistance = 10)
#points(temp$DateTimeMS[res[,2]], y = res[,1], col = 4, pch = 19)
# Print the estimated bpm from the pracma::findpeaks routine, but 
# you need to double the number of peaks to get bpm if your time series
# is only 30 seconds long
#mtext(side = 3, line = 3, text = paste0('pracma bpm: ', nrow(res)*2))
mtext(side = 3, line = 1, text = paste(mysensor, mytime), cex = mycex)
mtext(side = 2, text = 'Detrended IR signal', line = 4, cex = mycex)

```

```{r openHeartBPM,echo=showcode}
# Open the re-processed beats per minute estimates from surface board
bpm = read.csv(paste0(heartoutputpath,'W.O.3.1099_bpm.csv'))
surfHeartbpm$DateTimePST = as.POSIXct(surfHeartbpm$DateTimePST, tz = 'etc/GMT+8', format="%Y-%m-%d %H:%M:%S")
surfHeartbpm$Sensor = factor(surfHeartbpm$Sensor)
surfHeartbpm$forecastBPMflag = factor(surfHeartbpm$forecastBPMflag)
surfHeartbpm$WeakSignalFlag = factor(surfHeartbpm$WeakSignalFlag)
surfHeartbpm$Species = factor(surfHeartbpm$Species)
surfHeartbpm$Treatment = factor(surfHeartbpm$Treatment)

botHeartbpm = read.csv(paste0(outputDirCSVs,'BottomHeart_SN05_bpm.csv'))
botHeartbpm$DateTimePST = as.POSIXct(botHeartbpm$DateTimePST, tz = 'etc/GMT+8', format="%Y-%m-%d %H:%M:%S")
botHeartbpm$Sensor = factor(botHeartbpm$Sensor)
botHeartbpm$forecastBPMflag = factor(botHeartbpm$forecastBPMflag)
botHeartbpm$WeakSignalFlag = factor(botHeartbpm$WeakSignalFlag)
botHeartbpm$Species = factor(botHeartbpm$Species)
botHeartbpm$Treatment = factor(botHeartbpm$Treatment)
```

```{r subsetHeartBPM,echo=showcode}

# Remove questionable bpm estimates based on the flag values
removeFailedBPM = function(x){
	x[which(x$WeakSignalFlag == 'FAIL'), c('BPMfft','forecastBPM')] = NA
	x[which(x$forecastBPMflag == 'FAIL'), c('forecastBPM')] = NA
	return(x)
}

surfHeartS1 = surfHeartbpm[surfHeartbpm$Sensor == 'Sensor1IR',]	# 
surfHeartS2 = surfHeartbpm[surfHeartbpm$Sensor == 'Sensor2IR',]	# 
surfHeartS3 = surfHeartbpm[surfHeartbpm$Sensor == 'Sensor3IR',]	# 
surfHeartS4 = surfHeartbpm[surfHeartbpm$Sensor == 'Sensor4IR',]	# 
surfHeartS5 = surfHeartbpm[surfHeartbpm$Sensor == 'Sensor5IR',]	# 
surfHeartS6 = surfHeartbpm[surfHeartbpm$Sensor == 'Sensor6IR',]	# 
surfHeartS7 = surfHeartbpm[surfHeartbpm$Sensor == 'Sensor7IR',]	# 
#surfHeartS8 = surfHeartbpm[surfHeartbpm$Sensor == 'Sensor8IR',]  	# 
surfHeartS1 = removeFailedBPM(surfHeartS1)
surfHeartS2 = removeFailedBPM(surfHeartS2)
surfHeartS3 = removeFailedBPM(surfHeartS3)
surfHeartS4 = removeFailedBPM(surfHeartS4)
surfHeartS5 = removeFailedBPM(surfHeartS5)
surfHeartS6 = removeFailedBPM(surfHeartS6)
surfHeartS7 = removeFailedBPM(surfHeartS7)


botHeartS1 = botHeartbpm[botHeartbpm$Sensor == 'Sensor1IR',]	# 
botHeartS2 = botHeartbpm[botHeartbpm$Sensor == 'Sensor2IR',]	# 
botHeartS3 = botHeartbpm[botHeartbpm$Sensor == 'Sensor3IR',]	# 
botHeartS4 = botHeartbpm[botHeartbpm$Sensor == 'Sensor4IR',]	# 
botHeartS5 = botHeartbpm[botHeartbpm$Sensor == 'Sensor5IR',]	# 
botHeartS6 = botHeartbpm[botHeartbpm$Sensor == 'Sensor6IR',]	# 
botHeartS7 = botHeartbpm[botHeartbpm$Sensor == 'Sensor7IR',]	# 

botHeartS1 = removeFailedBPM(botHeartS1)
botHeartS2 = removeFailedBPM(botHeartS2)
botHeartS3 = removeFailedBPM(botHeartS3)
botHeartS4 = removeFailedBPM(botHeartS4)
botHeartS5 = removeFailedBPM(botHeartS5)
botHeartS6 = removeFailedBPM(botHeartS6)
botHeartS7 = removeFailedBPM(botHeartS7)

```

```{r PlotFunctions, echo=showcode}
# Function to plot a heartrate dataset, plotting both the actual points
# and a loess smoother
loessFunc = function(x,y, col = 1, lwd = 2, pch = '.', cex = 1, span = 0.75){
	points(x, y, col = col, pch = pch, cex = cex)
	lo = loess(y~as.numeric(x), na.action = 'na.omit', span = span)
	predy = predict(lo, newdata = as.numeric(x))
	lines(x, predy, col = col, lwd = lwd)
}

#TideBoxes = function(Exposure,DateTime, cols = c('lightblue','white')){
#	runs = rle(as.numeric(Exposure))
#	runs$lengths = c(1,runs$lengths)
#	
#	indsright = numeric(0)
#	for (i in 1:length(runs$lengths)){
#		indsright = c(indsright,sum(runs$lengths[1:i])) 
#	}
#	indsleft = c(1,indsright[1:(length(indsright)-1)])
#	
#	plotdims = par()$usr
#	
#	rect(xleft = DateTime[indsleft], 
#			ybottom = rep(plotdims[3],length(indsleft)),
#			xright = DateTime[indsright],
#			ytop = rep(plotdims[4], length(indsleft)),
#			col = cols, border = NA)
#	box()
#}

```


```{r getBPMfunctionOldVersion}
getBPMold = function(x,t1 = 1, sensor = 'Sensor1IR',chunklength=300, 
		lowband = 1/60, 
		highband = 1/10, 
		#amplitudeThreshold = 60,
		Fs = 0.125)
{
	# Test if there are any NAs in the heart rate values, if there are not
	# then proceed with the filtering and heart rate determination
	if ( length( which( is.na(temp[,sensor]) ) ) == 0){
		
		# Detrend the heartrate readings
		detrendedIR = pracma::detrend(temp[,sensor])	
		# Next look at the detrended values and look for spurious low values
		# For instance, a good heart signal might oscillate between -200 & +300
		# in the detrended data, and a spurious value might suddenly drop to 
		# -6000. 
		# Calculate the standard deviation of the data set, and then find values
		# that are more than 3 SD away from the mean (which should be ~zero in the
		# detrended data)
		spuriousVals = which(abs(detrendedIR) > (3*sd(detrendedIR)) )
		if ( length(spuriousVals) > 0) {
			# Convert to NAs 
			temp[spuriousVals,mysensor] = NA
			
			for (i in 1:length(spuriousVals)){
				indx = spuriousVals[i]  # Get the row index for this spurious value
				if (indx == 1) {
		 temp=temp[2:nrow(temp),]
		  } else if(indx==2){
		    temp=temp[2:nrow(temp),]
		  } else if(indx>2){
		    # Get the row index for this spurious value
			# Replace the spurious value with the average of the values immediately
			# before and after the spurious value
      #temp[indx,sensor] = mean(c(temp[indx-1,sensor],temp[indx+1,sensor]))
			temp[indx,mysensor] = mean(temp[(indx-2):(indx+2),mysensor], na.rm=TRUE)
		  }
			}
		}
		rm(spuriousVals)

		
		# With the spurious values replaced by interpolated values, re-run the 
		# detrending routine
		detrendedIR = pracma::detrend(temp[,sensor])
	} else {
		# If there were NAs, just define detrendedIR as NA so that later 
		# operations skip over this chunk of data
		detrendedIR = NA
	}

	

	if (length(which(is.na(detrendedIR))) == 0) {

		# Define a butterworth filter
	# Consider using the bandpass filter rather than just a lowpass filter
	# because of the tendency for the IR heartrate signal to drift up and 
	# down in relation to ambient light, which tends to induce low-frequency
	# shifts that then fool the spectral analysis routines when trying to 
	# identify the dominant frequency
		bf = signal::butter(3,W = c(lowband, highband), type = 'pass')
		
		# Apply the filter to the detrended data chunk
		y = filtfilt(bf, x = detrendedIR)
		# Calculate the spectrum of the filtered data
		myfft2 = spectrum(y, plot = FALSE)
		# Take the peak frequency from the spectrum, divide by sampling
		# rate to convert to cycles per second
		mypeakfreq = myfft2$freq[which.max(myfft2$spec)] / Fs
		# Multiply by 60 seconds to get cycles (beats) per minute
		BPMfft = 60 * mypeakfreq
		# Calculate amplitude of filtered signal to denote weak or noisy signals
		#amp = range(y)[2] - range(y)[1]
		
		## Plot the raw detrended signal
#	plot(temp$DateTimeMS, detrendedIR, type = 'l', 
#			main = paste0("Amplitude = ",round(amp,1),
#					', BPM = ', bpm), 
#			las = 1) 
#	if (amp > 60){
#		lines(temp$DateTimeMS,y, col = 3, lwd = 2) # add the filtered signal
#	} else if (amp <= 60) {
#		lines(temp$DateTimeMS,y, col = 2, lwd = 2) # add the filtered signal	
#		warning('Garbage signal')
#	}
		# Use function from package 'forecast', returns peak period (not freq)
		forecastPeriod = forecast::findfrequency(y)  
		forecastFreq = 1/forecastPeriod # convert period to frequency
		forecastBPM = forecastFreq * 60 * 8 # convert frequency to beats per 
		# minute, based on the fact that the sampling rate is 10Hz, and there are
		# 60 seconds in a minute
		#	mtext(side = 3, text = paste0('Forecast BPM: ',round(forecastBPM,1)))
		# Use pracma package to find peaks. Note that at slower heart rates
		# this function tends to find the sub-peaks (akin to a P or T peak in a
		# human ECG trace) rather than just the main peaks (R peaks on a human). 
		# This happens based on what the bandpass filter lets through. 
#	res = pracma::findpeaks(x = y, nups = 5, minpeakdistance = 10)
#	points(temp$DateTimeMS[res[,2]], y = res[,1], col = 4, pch = 19)
		# Print the estimated bpm from the pracma::findpeaks routine, but 
		# you need to double the number of peaks to get bpm if your time series
		# is only 30 seconds long
#	mtext(side = 3, line = 3, text = paste0('pracma bpm: ', nrow(res)*2))
		
		# Calculate the difference between the estimated heart rates from the 
		# spectrum fft routine and the forecast::findfrequency routine. A 
		# small value indicates good agreement
		BPMagreeQuality = ceiling(abs(BPMfft - forecastBPM))
		# Set a flag for cases where the forecast:findfrequency estimate is 
		# extremely large, which happens when it can't find a clear heart signal
		forecastBPMflag = ifelse(forecastBPM > amplitudeThreshold, 'FAIL','OK')
		# Set a flag for cases where the detrended/filtered signal has a very
		# small amplitude, signaling that there may be no good heartbeat signal 
		WeakSignalFlag = ifelse(amp < amplitudeThreshold, 'FAIL','OK')
		
		
		resultsList = list(DateTimePST = temp$DateTimePST[1],
				Sensor = sensor,
				BPMfft = round(BPMfft,1),
				forecastBPM = round(forecastBPM,1),
				BPMagreeQuality = BPMagreeQuality,
				forecastBPMflag = forecastBPMflag,
				WeakSignalFlag = WeakSignalFlag)
				#FilteredAmplitude = round(amp,1))
	} else if (length( which( is.na(temp[,sensor]) ) ) > 0) {
		# Handle the case where there are NAs in the data chunk that prevent
		# the filtering and fft routines
		resultsList = list(DateTimePST = temp$DateTimePST[1],
				Sensor = sensor,
				BPMfft = NA,
				forecastBPM = NA,
				BPMagreeQuality = NA,
				forecastBPMflag = 'FAIL',
				WeakSignalFlag = 'FAIL')
				#FilteredAmplitude = NA)
	}
	

	return(resultsList)
}

```
